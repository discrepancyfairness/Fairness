{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries necessary for this project\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "from time import time\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Import 'GridSearchCV', 'make_scorer', and any other necessary libraries\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Import the three supervised learning models from sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA    \n",
    "\n",
    "# Pretty display for notebooks\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/subham/anaconda3/envs/tensorflow/lib/python3.6/site-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['shuffle', 'choice', 'triangular', 'sample', 'randint', 'uniform', 'seed', 'random', 'e', 'time']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "#SVM \n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline\n",
    "from random import *\n",
    "from subprocess import check_output\n",
    "def Bank_svm(X,Y):\n",
    "    #Split data into training and test datasets (training will be based on 70% of data)\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0,shuffle=True) \n",
    "    #test_size: if integer, number of examples into test dataset; if between 0.0 and 1.0, means proportion\n",
    "    print('There are {} samples in the training set and {} samples in the test set'.format(X_train.shape[0], X_test.shape[0]))\n",
    "\n",
    "    \n",
    "    #Scaling data\n",
    "    #from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    from sklearn.model_selection import cross_val_score\n",
    "\n",
    "    #sc = StandardScaler(with_mean=False)\n",
    "    \n",
    "    \n",
    "    #sc.fit(X_train)\n",
    "    #X_train_std = sc.transform(X_train)\n",
    "    #X_test_std = sc.transform(X_test)\n",
    "\n",
    "    #X_train_std and X_test_std are the scaled datasets to be used in algorithms\n",
    "\n",
    "    #Applying SVC (Support Vector Classification)\n",
    "    from sklearn.svm import SVC\n",
    "    svm = SVC(kernel='rbf', random_state=0, gamma=.001, C=1.0,probability=True)\n",
    "    svm.fit(X_train, Y_train)\n",
    "    print('The accuracy of the SVM classifier on training data is {:.2f}'.format(svm.score(X_train, Y_train)))\n",
    "    print('The accuracy of the SVM classifier on test data is {:.2f}'.format(svm.score(X_test, Y_test)))\n",
    "    print('####Train prediction Label###############################################')\n",
    "    Y_train_pred=svm.predict(X_train)\n",
    "    #print(y_1)\n",
    "    Y_test_pred=svm.predict(X_test)\n",
    "\n",
    "    print('####Actual Train Label###############################################')\n",
    "    e=svm.predict_proba(X_test)\n",
    "\n",
    "    print(e)\n",
    "    print(Y_test_pred)\n",
    "    print('####Change to colors###############################################')\n",
    "        \n",
    "    \n",
    "    return X_test,Y_test_pred,Y_test,e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#without accuracy ---> 2\n",
    "def main(datax, y_test, y_test_pred,e): \n",
    "        \n",
    "    n=datax.shape[1]\n",
    "    s=datax.shape[0]    \n",
    "    data = np.zeros((s, n), dtype = int)\n",
    "    \n",
    "    r = np.zeros(n, dtype = int) \n",
    "    \n",
    "    for i in range(n):\n",
    "        if int(y_test.iloc[i])==1 :\n",
    "            r[i]=1\n",
    "        else :\n",
    "            r[i]= -1  \n",
    "    \n",
    "    r2 = np.zeros(n, dtype = int) \n",
    "    for i in range(n):\n",
    "        if int(y_test_pred[i])==1 :\n",
    "            r2[i]=1\n",
    "        else :\n",
    "            r2[i]= -1          \n",
    "    ar=[]\n",
    "    \n",
    "    for j in range(s):\n",
    "        print(\"sensitive attribute \",(j+1)) \n",
    "        a=0\n",
    "        b=0\n",
    "        acc1=0\n",
    "        acc2=0\n",
    "        for i in range(n):\n",
    "                data[j][i]= datax.iloc[j,i]\n",
    "                if data[j][i]== 1 :\n",
    "                    a=a+1\n",
    "                    if r[i]==1:\n",
    "                         acc1=acc1+1 \n",
    "\n",
    "        print(\"ACTUAL----------total ,accepted, aceeptance rate:\")             \n",
    "        a1=float(acc1/a)\n",
    "        print(a)\n",
    "        \n",
    "        print(acc1)\n",
    "        print(a1)\n",
    "        ar.append(a1)\n",
    "        \n",
    "    maxi= max(ar)\n",
    "    mini= min(ar)\n",
    "    DP=float(maxi-mini)\n",
    "    print(\"data acceptance rates\")\n",
    "    print(ar)\n",
    "    print(\"data DP\")\n",
    "    print(DP)\n",
    "    \n",
    "    ar=[]\n",
    "    \n",
    "    for j in range(s):\n",
    "        print(\"sensitive attribute \",(j+1)) \n",
    "        a=0\n",
    "        b=0\n",
    "        acc1=0\n",
    "        acc2=0\n",
    "        prec=0\n",
    "        reca=0\n",
    "        accur=0\n",
    "        FP=0\n",
    "        FN=0\n",
    "        TP=0\n",
    "        TN=0\n",
    "        for i in range(n):\n",
    "             if data[j][i]== 1 :\n",
    "                    a=a+1\n",
    "                    if r2[i]==1:\n",
    "                        acc1=acc1+1 \n",
    "                        if r[i]==1:\n",
    "                            TP=TP+1\n",
    "                        else:\n",
    "                             FP=FP+1                \n",
    "                    else:\n",
    "                        if r[i]==1:\n",
    "                            FN=FN+1\n",
    "                        else:\n",
    "                            TN=TN+1    \n",
    "        \n",
    "        print(\"prec reca accuracy for each sens\") \n",
    "        prec= float(TP/(TP+FP))\n",
    "        reca= float(TP/(TP+FN))\n",
    "        accur= float((TP+TN)/a)\n",
    "        print(prec,reca,accur)\n",
    "        \n",
    "        \n",
    "        print(\"SVM----------total , accepted, aceeptance rate:\")             \n",
    "        \n",
    "        a1=float(acc1/a)\n",
    "        print(a)\n",
    "        \n",
    "        print(acc1)\n",
    "        print(a1)\n",
    "        ar.append(a1)\n",
    "        \n",
    "    maxi= max(ar)\n",
    "    mini= min(ar)\n",
    "    DP=float(maxi-mini)\n",
    "    print(\"data acceptance rates\")\n",
    "    print(ar)\n",
    "    print(\"data DP\")\n",
    "    print(DP) \n",
    "    \n",
    "    print(\"SVM accuracy--------------------------\")\n",
    "    prec=0\n",
    "    reca=0\n",
    "    accur=0\n",
    "    FP=0\n",
    "    FN=0\n",
    "    TP=0\n",
    "    TN=0\n",
    "    for i in range(n):\n",
    "            if r2[i]==1:\n",
    "                acc1=acc1+1 \n",
    "                if r[i]==1:\n",
    "                    TP=TP+1\n",
    "                else:\n",
    "                     FP=FP+1                \n",
    "            else:\n",
    "                if r[i]==1:\n",
    "                     FN=FN+1\n",
    "                else:\n",
    "                     TN=TN+1    \n",
    "\n",
    "        \n",
    "    prec= float(TP/(TP+FP))\n",
    "    reca= float(TP/(TP+FN))\n",
    "    accur= float((TP+TN)/n)\n",
    "    print(prec,reca,accur)\n",
    "    \n",
    "    epsilon=[.01]\n",
    "    fi= np.zeros(n,dtype=int) \n",
    "\n",
    "    \n",
    "    #alpha=[[1,.1,1,1,1,1],[1,1,1,1,.01,1],[1,1,1,1,1,.001]] \n",
    "    alpha=[[1,1,1,1,1,1]]\n",
    "    \n",
    "    1st\n",
    "    gamma=[[0.2970639032815199, 0.06707420614705382, 0.07105054247155332, 0.10202881505439576, 0.056, 0.043478260869565216],\n",
    "[0.25734024179620035, 0.06911190354898965, 0.0723736438211167, 0.09732431637753602, 0.06036363636363636, 0.08695652173913043],\n",
    "[0.21243523316062177, 0.07123450500933945, 0.07369674517068008, 0.09232578653337253, 0.06472727272727273, 0.08695652173913043],\n",
    "[0.16753022452504318, 0.07327220241127526, 0.07488753638528711, 0.08732725668920906, 0.06909090909090909, 0.08695652173913043],\n",
    "[0.12435233160621761, 0.07539480387162506, 0.07621063773485048, 0.0826227580123493, 0.07345454545454545, 0.08695652173913043],\n",
    "[0.08635578583765112, 0.07751740533197486, 0.07753373908441387, 0.07880035283740076, 0.07781818181818181, 0.08695652173913043]]\n",
    "        '''\n",
    "    gamma=[[0.2970639032815199, 0.06707420614705382, 0.07105054247155332, 0.10202881505439576, 0.056, 0.043478260869565216],\n",
    "[0.26252158894645944, 0.07361181864493123, 0.07687218840963218, 0.10173478388709203, 0.06545454545454546, 0.08695652173913043],\n",
    "[0.22279792746113988, 0.08023433520122261, 0.08269383434771103, 0.1014407527197883, 0.07418181818181818, 0.08695652173913043],\n",
    "[0.18134715025906736, 0.08685685175751401, 0.08851548028578989, 0.10085269038518083, 0.0829090909090909, 0.08695652173913043],\n",
    "[0.1381692573402418, 0.09322465613856343, 0.09407250595395607, 0.09997059688326963, 0.09090909090909091, 0.08695652173913043],\n",
    "[0.1381692573402418, 0.0999320767532688, 0.09989415189203493, 0.10673331373125551, 0.09963636363636363, 0.08695652173913043]]\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    gamma2=[[0.2970639033,0.2158894646,.127806563,.0967184801,.1070811744],\n",
    "[0.05815928,0.0556970623,0.0562913907,0.0560366786,.0578196638],\n",
    "[0.0631119344,0.0599364911,0.0571579783,0.0553056364,.0571579783],\n",
    "[0.0911496619,0.0782122905,0.072037636,0.0682152308,.0743898853],\n",
    "[0.0501818182,.0443636364 ,.0429090909,.0472727273,.0414545455],\n",
    "[0.0434782609,0.0434782609,0.0434782609,.0434782609,.0434782609]]\n",
    "\n",
    "\n",
    "    gamma=np.transpose(gamma2)\n",
    "    \n",
    "\n",
    "    \n",
    "    alpha=[[1,1,1,1,1,1],[.1,1,1,1,1,1], [.01,1,1,1,1,1],[.001,1,1,1,1,1],\n",
    "           [1,.1,1,1,1,1], [1,.01,1,1,1,1],[1,.001,1,1,1,1],\n",
    "           [1,1,.1,1,1,1], [1,1,.01,1,1,1],[1,1,.001,1,1,1],\n",
    "           [1,1,1,.1,1,1], [1,1,1,.01,1,1],[1,1,1,.001,1,1],\n",
    "           [1,1,1,1,.1,1], [1,1,1,1,.01,1],[1,1,1,1,.001,1],\n",
    "           [1,1,1,1,1,.1], [1,1,1,1,1,.01],[1,1,1,1,1,.001],\n",
    "        [.1,.1,1,1,1,1],[.01,.01,1,1,1,1], [.001,.001,1,1,1,1],[.001,.01,1,1,1,1],[.001,.1,1,1,1,1],\n",
    "         [.01,.1,1,1,1,1] ]\n",
    "    '''\n",
    "         \n",
    "    a=0\n",
    "    print(alpha)\n",
    "    \n",
    "    '''\n",
    "    #agarwal 1 15 25\n",
    "     alpha=[[1,1,1,1,1,1],[1,1,1,1,.01,1],  [.01,.1,1,1,1,1] ] \n",
    "    gamma2=   [[ 0.074266,0.0725389,0.0794473,0.0777202,0.0673575],\n",
    "[0.051367,0.051367,0.0517915,0.0524707,0.0533197],\n",
    "[0.0497486,0.0497486,0.0493517,0.0501455,0.0504102],\n",
    "[0.0629227,0.0649809,0.0729197,0.0738018,0.0743899],\n",
    "[0.0407273,0.0349091,0.0254545,0.024,0.024],\n",
    "[0.0869565,0.0869565,0,0,0]]\n",
    "    \n",
    "                \n",
    "    gamma=np.transpose(gamma2)\n",
    "    '''\n",
    "    prec_all=np.zeros((6,7),dtype=float)\n",
    "    rec_all=np.zeros((6,7),dtype=float)\n",
    "    acc_rate=np.zeros((6,7),dtype=float)\n",
    "    \n",
    "    t=0\n",
    "    for new in range(6):\n",
    "        for k in range(1):\n",
    "            for eps in epsilon:\n",
    "                u1,u2=min_sum_lpcna(data,gamma[new],eps,e,alpha[k],r2)\n",
    "                #######################Disp_impact#######################  \n",
    "                print(\"gamma-epsilon-delta\",gamma[new],eps)\n",
    "                accu_all=[]\n",
    "                DP_all=[]\n",
    "                precision_all=[]\n",
    "                recall_all=[]\n",
    "                ar_all=[]\n",
    "                acceptance_rate=np.zeros((7,28),dtype=float)\n",
    "                count=0\n",
    "                print(\"<--------------------------------------->\")\n",
    "                print(\"iteration t\",t)\n",
    "\n",
    "\n",
    "                for i in range(n):\n",
    "                     fi[i] = u1[i]\n",
    "\n",
    "\n",
    "                for j in range(s):\n",
    "                    print(\"sensitive attribute \",(j+1)) \n",
    "\n",
    "                    TP=0\n",
    "                    FP=0\n",
    "                    FN=0\n",
    "                    TN=0\n",
    "                    precision=0\n",
    "                    recall=0\n",
    "                    for i in range(n):\n",
    "                         if data[j][i]== 1 :                        \n",
    "                            if fi[i]==1 and r[i]==1:\n",
    "                                TP=TP+1\n",
    "                            if fi[i]==1 and r[i]==-1:\n",
    "                                FP=FP+1 \n",
    "                            if fi[i]==-1 and r[i]==1:\n",
    "                                FN=FN+1\n",
    "                            if fi[i]==-1 and r[i]==-1:\n",
    "                                TN=TN+1    \n",
    "                    if TP+FP !=0:\n",
    "                        precision=float(TP/(TP+FP))\n",
    "                    #print(\"precision\",precision)\n",
    "                    if TP+FN !=0:    \n",
    "                        recall=float(TP/(TP+FN))\n",
    "                    #print(\"recall\",recall)\n",
    "\n",
    "                    precision_all.append(precision)\n",
    "                    recall_all.append(recall)\n",
    "                    #print(\"TP,FP,TN,FN\")\n",
    "                    #print(TP,FP,TN,FN)\n",
    "\n",
    "                    a=0\n",
    "                    b=0\n",
    "                    acc1=0\n",
    "                    acc2=0\n",
    "                    for i in range(n):\n",
    "                            if data[j][i]== 1 :\n",
    "                                a=a+1\n",
    "                                if fi[i]==1:\n",
    "                                     acc1=acc1+1 \n",
    "\n",
    "        #                         print(\"total ,fair accepted, aceeptance rate:\")             \n",
    "                    a1=float(acc1/a)\n",
    "\n",
    "\n",
    "\n",
    "        #                         print(a)\n",
    "        #                         print(acc1)\n",
    "        #                         print(a1)\n",
    "                    ar_all.append(a1)\n",
    "                    prec_all[t][j]=precision\n",
    "                    rec_all[t][j]=recall\n",
    "                    acc_rate[t][j]=a1\n",
    "                t=t+1\n",
    "                count = count+1\n",
    "                maxi=max(ar_all)\n",
    "                mini= min(ar_all)\n",
    "                DP=float(maxi-mini)\n",
    "                print(\"individual acceptance rates\")\n",
    "                print(ar_all)\n",
    "                print(\"individul precision\")\n",
    "                print(precision_all)\n",
    "                print(\"individual recall\")\n",
    "                print(recall_all)\n",
    "                print(\"DP all\")\n",
    "                print(DP)\n",
    "                f_acc=0\n",
    "                for i in range(n):\n",
    "                     if fi[i] == r[i]:\n",
    "                            f_acc=f_acc+1\n",
    "                f_acc_l=float((f_acc*100)/n) \n",
    "\n",
    "        #######################################################################33   \n",
    "\n",
    "        #                         print(\"sensitive attribute \",(j+1)) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                TP=0\n",
    "                FP=0\n",
    "                FN=0\n",
    "                TN=0\n",
    "                precision=0\n",
    "                recall=0\n",
    "                accu=0\n",
    "                for i in range(n):\n",
    "                        if fi[i]==1 and r[i]==1:\n",
    "                            TP=TP+1\n",
    "                        if fi[i]==1 and r[i]==-1:\n",
    "                            FP=FP+1 \n",
    "                        if fi[i]==-1 and r[i]==1:\n",
    "                            FN=FN+1\n",
    "                        if fi[i]==-1 and r[i]==-1:\n",
    "                            TN=TN+1    \n",
    "\n",
    "                if TP+FP!=0:\n",
    "                    precision=float(TP/(TP+FP))\n",
    "                print(\"precision all\",precision)\n",
    "                if TP+FN!=0:\n",
    "                    recall=float(TP/(TP+FN))\n",
    "\n",
    "\n",
    "                print(\"recall all\",recall)\n",
    "                accu=float((TP+TN)/(TP+FN+TN+FP))\n",
    "\n",
    "\n",
    "                print(\"accuracy all\",accu)\n",
    "\n",
    "\n",
    "\n",
    "                print(\"TP,FP,TN,FN\")\n",
    "                print(TP,FP,TN,FN)\n",
    "        #                         print(\"total ,fair accepted, aceeptance rate:\")             \n",
    "                a1=float(acc1/a)\n",
    "\n",
    "\n",
    "    print(\"<--------------------------------------->\")\n",
    "    alpha_weight=np.arange(0,1.2,.2)        \n",
    "    return accu_all,DP_all,acc_rate,alpha_weight,prec_all,rec_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#NG\n",
    "import time\n",
    "import pulp as p \n",
    "def min_sum_lpcna(data1,beta,eps,e,alpha,y):\n",
    "    import pulp as p \n",
    "    import math\n",
    "    \n",
    "    m=data1.shape[0]\n",
    "    n=data1.shape[1]\n",
    "    print('dimension of data')\n",
    "    print(m,n)\n",
    "    \n",
    "    ################ sorted result\n",
    "    h1=[]\n",
    "    h2=[]\n",
    "    h3=[]\n",
    "    h4=[]\n",
    "    h5=[]\n",
    "    h6=[]\n",
    "   \n",
    "    key1=[]\n",
    "    key2=[]\n",
    "    key3=[]\n",
    "    key4=[]\n",
    "    key5=[]\n",
    "    key6=[]\n",
    "   \n",
    "    cost=np.zeros(n,dtype=int)\n",
    "    data2=np.zeros((m,n),dtype=int)\n",
    "    for i in range(n):\n",
    "        if data1[0][i]==1:            \n",
    "\n",
    "            h1.append(e[i][1])\n",
    "            key1.append(i)\n",
    "\n",
    "        elif data1[1][i]==1:\n",
    "            h2.append(e[i][1])\n",
    "            key2.append(i)\n",
    "            \n",
    "        if data1[2][i]==1:\n",
    "            h3.append(e[i][1])\n",
    "            key3.append(i)\n",
    "            \n",
    "        elif data1[3][i]==1:\n",
    "            h4.append(e[i][1])\n",
    "            key4.append(i)\n",
    "        elif data1[4][i]==1:\n",
    "            h5.append(e[i][1])\n",
    "            key5.append(i)\n",
    "        elif data1[5][i]==1:\n",
    "            h6.append(e[i][1])\n",
    "            key6.append(i)\n",
    "        elif data1[6][i]==1:\n",
    "            h7.append(e[i][1])\n",
    "            key7.append(i)\n",
    "#print(hc)\n",
    "#     print(key1)\n",
    "    \n",
    "    for i in range(1,len(h1)):\n",
    "        for j in range(i,0,-1):\n",
    "            var=0\n",
    "            var2=0\n",
    "            if h1[j-1]<h1[j]:\n",
    "                index=j\n",
    "                var=h1[j]\n",
    "                h1[j]=h1[j-1]\n",
    "                h1[j-1]=var\n",
    "\n",
    "                var2=key1[j]\n",
    "                key1[j]=key1[j-1]\n",
    "                key1[j-1]=var2\n",
    "            else:\n",
    "                break\n",
    "    \n",
    "\n",
    "    for i in range(1,len(h2)):\n",
    "        for j in range(i,0,-1):\n",
    "            var=0\n",
    "            var2=0\n",
    "            if h2[j-1]<h2[j]:\n",
    "                index=j\n",
    "                var=h2[j]\n",
    "                h2[j]=h2[j-1]\n",
    "                h2[j-1]=var\n",
    "\n",
    "                var2=key2[j]\n",
    "                key2[j]=key2[j-1]\n",
    "                key2[j-1]=var2\n",
    "            else:\n",
    "                break\n",
    "    for i in range(1,len(h3)):\n",
    "        for j in range(i,0,-1):\n",
    "            var=0\n",
    "            var2=0\n",
    "            if h3[j-1]<h3[j]:\n",
    "                index=j\n",
    "                var=h3[j]\n",
    "                h3[j]=h3[j-1]\n",
    "                h3[j-1]=var\n",
    "\n",
    "                var2=key3[j]\n",
    "                key3[j]=key3[j-1]\n",
    "                key3[j-1]=var2\n",
    "            else:\n",
    "                break\n",
    "    for i in range(1,len(h4)):\n",
    "        for j in range(i,0,-1):\n",
    "            var=0\n",
    "            var2=0\n",
    "            if h4[j-1]<h4[j]:\n",
    "                index=j\n",
    "                var=h4[j]\n",
    "                h4[j]=h4[j-1]\n",
    "                h4[j-1]=var\n",
    "\n",
    "                var2=key4[j]\n",
    "                key4[j]=key4[j-1]\n",
    "                key4[j-1]=var2\n",
    "            else:\n",
    "                break\n",
    "    for i in range(1,len(h5)):\n",
    "        for j in range(i,0,-1):\n",
    "            var=0\n",
    "            var2=0\n",
    "            if h3[j-1]<h5[j]:\n",
    "                index=j\n",
    "                var=h5[j]\n",
    "                h5[j]=h5[j-1]\n",
    "                h5[j-1]=var\n",
    "\n",
    "                var2=key5[j]\n",
    "                key5[j]=key5[j-1]\n",
    "                key5[j-1]=var2\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "                \n",
    "                \n",
    "    for i in range(1,len(h6)):\n",
    "        for j in range(i,0,-1):\n",
    "            var=0\n",
    "            var2=0\n",
    "            if h6[j-1]<h6[j]:\n",
    "                index=j\n",
    "                var=h6[j]\n",
    "                h6[j]=h6[j-1]\n",
    "                h6[j-1]=var\n",
    "\n",
    "                var2=key6[j]\n",
    "                key6[j]=key6[j-1]\n",
    "                key6[j-1]=var2\n",
    "            else:\n",
    "                break        \n",
    "                \n",
    "    '''       \n",
    "    #basic1\n",
    "    for j in range(len(key1)):    \n",
    "        #data2[0][key1[j]]=((j+1)/((beta[0]*len(key1))*((beta[0]*len(key1))+1)/2)*alpha[0])\n",
    "         \n",
    "        data2[0][key1[j]]=(j+1)*alpha[0]\n",
    "    for j in range(len(key2)):\n",
    "        data2[1][key2[j]]=(j+1)*alpha[1]\n",
    "    for j in range(len(key3)):\n",
    "        data2[2][key3[j]]=(j+1)*alpha[2]              \n",
    "        \n",
    "    for j in range(len(key4)):\n",
    "        data2[3][key4[j]]=(j+1)*alpha[3]\n",
    "        \n",
    "                             \n",
    "    for j in range(len(key5)):               \n",
    "        data2[4][key5[j]]=(j+1)*alpha[4]\n",
    "       \n",
    "    for j in range(len(key6)):\n",
    "        data2[5][key6[j]]=(j+1)*alpha[5]\n",
    "    \n",
    "    #basic2\n",
    "            \n",
    "    '''\n",
    "    '''\n",
    "    for j in range(len(key1)):    \n",
    "        #data2[0][key1[j]]=((j+1)/((beta[0]*len(key1))*((beta[0]*len(key1))+1)/2)*alpha[0])\n",
    "         \n",
    "        data2[0][key1[j]]=(j+1)*alpha[0]\n",
    "    for j in range(len(key2)):\n",
    "        data2[1][key2[j]]=(j+1)*alpha[1]\n",
    "    for j in range(len(key3)):\n",
    "        if data1[2][key3[j]]==1 and data1[0][key3[j]]==1: \n",
    "            data2[2][key3[j]]=(j+1)*(len(key1)/len(key3))*alpha[2]\n",
    "        else:\n",
    "            data2[2][key3[j]]=(j+1)*(len(key2)/len(key3))*alpha[2]                  \n",
    "        \n",
    "    for j in range(len(key4)):\n",
    "        if data1[3][key4[j]]==1 and data1[0][key4[j]]==1:                   \n",
    "            data2[3][key4[j]]=(j+1)*(len(key1)/len(key4))*alpha[3]\n",
    "        else :                     \n",
    "            data2[3][key4[j]]=(j+1)*(len(key2)/len(key4))*alpha[3]\n",
    "                             \n",
    "    for j in range(len(key5)):\n",
    "        if data1[4][key5[j]]==1 and data1[0][key5[j]]==1:                  \n",
    "            data2[4][key5[j]]=(j+1)*(len(key1)/len(key5))*alpha[4]\n",
    "        else:      \n",
    "            data2[4][key5[j]]=(j+1)*(len(key2)/len(key5))*alpha[4]\n",
    "    for j in range(len(key6)):\n",
    "        if data1[5][key6[j]]==1 and data1[0][key6[j]]==1:                    \n",
    "            data2[5][key6[j]]=(j+1)*(len(key1)/len(key6))*alpha[5]\n",
    "        else:                    \n",
    "             data2[5][key6[j]]=(j+1)*(len(key2)/len(key6))*alpha[5] \n",
    "          \n",
    "    '''\n",
    "    \n",
    "    for j in range(len(key1)): \n",
    "        if y[key1[j]]==1:\n",
    "            data2[0][key1[j]]=1\n",
    "        else:\n",
    "            data2[0][key1[j]]=2\n",
    "    for j in range(len(key2)):\n",
    "        if y[key2[j]]==1:\n",
    "            data2[1][key2[j]]=1\n",
    "        else:    \n",
    "            data2[1][key2[j]]=2\n",
    "    for j in range(len(key3)):\n",
    "        if y[key3[j]]==1:\n",
    "            data2[2][key3[j]]=1\n",
    "        else:\n",
    "            data2[2][key3[j]]=2\n",
    "    for j in range(len(key4)):\n",
    "        if y[key4[j]]==1:\n",
    "            data2[3][key4[j]]=1\n",
    "        else:\n",
    "            data2[3][key4[j]]=2\n",
    "    for j in range(len(key5)):\n",
    "        if y[key5[j]]==1:\n",
    "            data2[4][key5[j]]=1\n",
    "        else:\n",
    "            data2[4][key5[j]]=2\n",
    "    for j in range(len(key6)):\n",
    "        if y[key6[j]]==1:\n",
    "            data2[5][key6[j]]=1 \n",
    "        else:\n",
    "            data2[5][key6[j]]=2\n",
    "            \n",
    "    for j in range(n):\n",
    "        sum=0\n",
    "        for i in range(m):\n",
    "       \n",
    "            sum=sum+data2[i][j] \n",
    "        cost[j]=sum\n",
    "        \n",
    "        \n",
    "    ################\n",
    "    Lp_prob = p.LpProblem('Problem', p.LpMinimize)  \n",
    "   \n",
    "    \n",
    "#     X=np.zeros(n+1,dtype=p.LpVariable)\n",
    "    X=np.zeros(n+m+1,dtype=p.LpVariable)\n",
    "    sizes=np.zeros(m,dtype=int)\n",
    "#     report_index(index,data1,e):  \n",
    "    max_size=0\n",
    "    for i in range(m):\n",
    "        count=0\n",
    "        for j in range(n):\n",
    "            if data1[i][j]==1:\n",
    "                count=count+1 \n",
    "        if count>max_size:\n",
    "            max_size=count\n",
    "        sizes[i]=count\n",
    "        \n",
    "    #############################33\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ###############################\n",
    "        \n",
    "        \n",
    "        \n",
    "  \n",
    "    select_sizes=np.zeros(m,dtype=int)\n",
    "   \n",
    "    size_final=np.zeros(m,dtype=int)\n",
    "\n",
    "    \n",
    "    for i in range(n):\n",
    "        var1=str(i)       \n",
    "        X[i]=p.LpVariable(var1,lowBound=0,upBound=1,cat='Integer')\n",
    "   \n",
    "    X[n]=p.LpVariable(str(n),lowBound=0,upBound=1,cat='Continuous')  \n",
    "\n",
    "    #########objective function#####################\n",
    "    \n",
    "#     Lp_prob += 2*X[n+1]+10*X[n+2]+9*X[n+3]+3*X[n+4]\n",
    "    Lp_prob+= p.lpSum([(X[j])*cost[j] for j in range(n)]) \n",
    "  \n",
    "    \n",
    "\n",
    "    ##############constraint#################\n",
    "    for i in range(2*m):\n",
    "        if i<m:\n",
    "\n",
    "            Lp_prob += p.lpSum([(X[j])*(data1[i][j]) for j in range(n)]) >= math.floor(beta[i]*sizes[i])\n",
    "            Lp_prob += p.lpSum([(X[j])*(data1[i][j]) for j in range(n)]) <= math.ceil((beta[i]+eps)*sizes[i])\n",
    "           # Lp_prob += p.lpSum([(X[j])*(data1[i][j])*(sizes[i]-report_index(j,i,data1,e)+1) for j in range(n)]) <= X[n+i+1]\n",
    "\n",
    "   \n",
    "    #####################################\n",
    "    status = Lp_prob.solve()   # Solver \n",
    "    print(p.LpStatus[status]) \n",
    "    print(\"objective is:\")        \n",
    "    print(p.value(Lp_prob.objective))\n",
    "    print(\"discripency is:\") \n",
    "    print(p.value(X[n]))\n",
    "    x=np.zeros(n,dtype=float)\n",
    "\n",
    "   # The solution status \n",
    "    Synth1={}\n",
    "    Synth2={}\n",
    "    # # Printing the final solution \n",
    "    for i in range(n):\n",
    "        if(p.value(X[i])==1):\n",
    "            Synth1[i]=1 \n",
    "            Synth2[i]=-1\n",
    "#             if(data1[2][i]==1):\n",
    "#                 print(\"no\")\n",
    "        else:\n",
    "            Synth1[i]=-1\n",
    "            Synth2[i]=1\n",
    "    Synthu1=Synth1  \n",
    "    Synthu2=Synth2  \n",
    "    \n",
    "              \n",
    "    return Synthu1,Synthu2   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    24928\n",
      "1    11568\n",
      "2     4612\n",
      "3       80\n",
      "Name: marital, dtype: int64\n",
      "There are 28831 samples in the training set and 12357 samples in the test set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/subham/anaconda3/envs/tensorflow/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the SVM classifier on training data is 0.92\n",
      "The accuracy of the SVM classifier on test data is 0.91\n",
      "####Train prediction Label###############################################\n",
      "####Actual Train Label###############################################\n",
      "[[0.93664901 0.06335099]\n",
      " [0.93204958 0.06795042]\n",
      " [0.9399619  0.0600381 ]\n",
      " ...\n",
      " [0.93542819 0.06457181]\n",
      " [0.93341507 0.06658493]\n",
      " [0.79605492 0.20394508]]\n",
      "[0 0 0 ... 0 0 0]\n",
      "####Change to colors###############################################\n",
      "       age  job  marital  education  default  housing  loan  contact  month  \\\n",
      "0       39    3        0          3        0        1     0        1      2   \n",
      "1       55    3        0          0        0        1     0        1      8   \n",
      "2       39    3        0          3        1        0     0        0      1   \n",
      "3       56    8        0          3        0        1     0        1      3   \n",
      "4       49    3        0          3        0        1     0        1      5   \n",
      "...    ...  ...      ...        ...      ...      ...   ...      ...    ...   \n",
      "12352   35    4        0          4        0        0     0        1      3   \n",
      "12353   39    4        1          4        0        0     0        1      2   \n",
      "12354   43    1        0          2        0        1     0        1      0   \n",
      "12355   39    0        0          6        0        0     0        0      0   \n",
      "12356   46    2        0          1        0        0     1        1      3   \n",
      "\n",
      "       day_of_week  duration  campaign  pdays  previous  poutcome  \\\n",
      "0                4       635         3    999         0         0   \n",
      "1                4       248         2    999         0         0   \n",
      "2                3       207         1    999         0         0   \n",
      "3                3       176         7    999         0         0   \n",
      "4                4       271         1    999         0         0   \n",
      "...            ...       ...       ...    ...       ...       ...   \n",
      "12352            3        38         1    999         0         0   \n",
      "12353            0       617         2    999         0         0   \n",
      "12354            4        68         1    999         0         0   \n",
      "12355            2       174         1    999         0         0   \n",
      "12356            2       871         2    999         0         0   \n",
      "\n",
      "       emp.var.rate  cons.price.idx  cons.conf.idx  euribor3m  nr.employed  \n",
      "0               1.4          93.918          -42.7      4.957       5228.1  \n",
      "1              -1.8          93.075          -47.1      1.405       5099.1  \n",
      "2               1.4          94.465          -41.8      4.961       5228.1  \n",
      "3               1.4          93.444          -36.1      4.963       5228.1  \n",
      "4              -0.1          93.200          -42.0      4.021       5195.8  \n",
      "...             ...             ...            ...        ...          ...  \n",
      "12352           1.4          93.444          -36.1      4.964       5228.1  \n",
      "12353           1.4          93.918          -42.7      4.960       5228.1  \n",
      "12354          -1.8          92.893          -46.2      1.250       5099.1  \n",
      "12355           1.1          93.994          -36.4      4.858       5191.0  \n",
      "12356           1.4          93.444          -36.1      4.965       5228.1  \n",
      "\n",
      "[12357 rows x 20 columns]\n",
      "[0 0 0 ... 0 0 0]\n",
      "       y\n",
      "0      1\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "...   ..\n",
      "12352  0\n",
      "12353  1\n",
      "12354  0\n",
      "12355  0\n",
      "12356  1\n",
      "\n",
      "[12357 rows x 1 columns]\n",
      "       age  marital\n",
      "0       39        0\n",
      "1       55        0\n",
      "2       39        0\n",
      "3       56        0\n",
      "4       49        0\n",
      "...    ...      ...\n",
      "12352   35        0\n",
      "12353   39        1\n",
      "12354   43        0\n",
      "12355   39        0\n",
      "12356   46        0\n",
      "\n",
      "[12357 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/subham/anaconda3/envs/tensorflow/lib/python3.6/site-packages/pandas/core/indexing.py:671: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "/home/subham/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/subham/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   a_1  a_2  m_0  m_1  m_2  m_3\n",
      "0    0    1    1    0    0    0\n",
      "1    0    1    1    0    0    0\n",
      "2    0    1    1    0    0    0\n",
      "3    0    1    1    0    0    0\n",
      "4    0    1    1    0    0    0\n",
      "     0      1      2      3      4      5      6      7      8      9      \\\n",
      "a_1      0      0      0      0      0      0      0      0      1      0   \n",
      "a_2      1      1      1      1      1      1      1      1      0      1   \n",
      "m_0      1      1      1      1      1      1      1      0      0      1   \n",
      "m_1      0      0      0      0      0      0      0      1      1      0   \n",
      "m_2      0      0      0      0      0      0      0      0      0      0   \n",
      "m_3      0      0      0      0      0      0      0      0      0      0   \n",
      "\n",
      "     ...  12347  12348  12349  12350  12351  12352  12353  12354  12355  12356  \n",
      "a_1  ...      0      0      0      0      0      0      0      0      0      0  \n",
      "a_2  ...      1      1      1      1      1      1      1      1      1      1  \n",
      "m_0  ...      0      1      0      1      1      1      0      1      1      1  \n",
      "m_1  ...      1      0      1      0      0      0      1      0      0      0  \n",
      "m_2  ...      0      0      0      0      0      0      0      0      0      0  \n",
      "m_3  ...      0      0      0      0      0      0      0      0      0      0  \n",
      "\n",
      "[6 rows x 12357 columns]\n"
     ]
    }
   ],
   "source": [
    "# without accuracy\n",
    "import time\n",
    "# import pulp as p \n",
    "# from random import *\n",
    "data= pd.read_csv('data/bank_train.csv',skipinitialspace=True)\n",
    "\n",
    "print(data['marital'].value_counts())\n",
    "#marital\n",
    "#U=80, M=24928, S=11568, D=4612\n",
    "# m_3, m_0, m_1, m_2\n",
    "#age\n",
    "#>60 and <25= a_1\n",
    "#>=25and <=60 =a_2\n",
    "# print(data.head())\n",
    "# print(data.shape[0],data.shape[1])\n",
    "\n",
    "#sensitive columns name 0='age',2='marital'\n",
    "\n",
    "data_c = data.drop(columns=['age_group','y'])\n",
    "# print(sens)\n",
    "r=data[['y']]\n",
    "\n",
    "X_test,Y_test_pred,Y_test,e = Bank_svm(data_c , r)\n",
    "\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "# Y_test_pred.reset_index()\n",
    "Y_test.reset_index(drop=True, inplace=True)\n",
    "print(X_test)\n",
    "print(Y_test_pred)\n",
    "print(Y_test)\n",
    "sens=X_test[['age','marital']]\n",
    "print(sens)\n",
    "p=sens.shape[0]\n",
    "\n",
    "# for i in range(0,p):  \n",
    "#     if r.loc[i,'y'] == 1 :\n",
    "#                r.loc[i,\"y\"] = 1 \n",
    "#     else: \n",
    "#                r.loc[i,\"y\"] = 0 \n",
    "            \n",
    "for i in range(0,p):\n",
    "    if sens.loc[i,'age'] > 60 or sens.loc[i,'age'] < 25 :\n",
    "               sens.loc[i,'age'] = 1 \n",
    "    else :\n",
    "               sens.loc[i,'age'] = 2  \n",
    "            \n",
    "sens1 = pd.get_dummies(sens, columns=['age','marital'], prefix =['a','m'])\n",
    "print(sens1.head())\n",
    "sensitive = sens1.T\n",
    "\n",
    "print(sensitive)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensitive attribute  1\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "579\n",
      "213\n",
      "0.36787564766839376\n",
      "sensitive attribute  2\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "11778\n",
      "1175\n",
      "0.09976226863644082\n",
      "sensitive attribute  3\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "7558\n",
      "748\n",
      "0.09896798094734056\n",
      "sensitive attribute  4\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "3401\n",
      "497\n",
      "0.14613349014995589\n",
      "sensitive attribute  5\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1375\n",
      "140\n",
      "0.10181818181818182\n",
      "sensitive attribute  6\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "23\n",
      "3\n",
      "0.13043478260869565\n",
      "data acceptance rates\n",
      "[0.36787564766839376, 0.09976226863644082, 0.09896798094734056, 0.14613349014995589, 0.10181818181818182, 0.13043478260869565]\n",
      "data DP\n",
      "0.2689076667210532\n",
      "sensitive attribute  1\n",
      "prec reca accuracy for each sens\n",
      "0.7142857142857143 0.5633802816901409 0.7564766839378239\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "579\n",
      "168\n",
      "0.29015544041450775\n",
      "sensitive attribute  2\n",
      "prec reca accuracy for each sens\n",
      "0.6625 0.451063829787234 0.9223127865511972\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "11778\n",
      "800\n",
      "0.06792324673119375\n",
      "sensitive attribute  3\n",
      "prec reca accuracy for each sens\n",
      "0.6734317343173432 0.4879679144385027 0.9259063244244509\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "7558\n",
      "542\n",
      "0.071712093146335\n",
      "sensitive attribute  4\n",
      "prec reca accuracy for each sens\n",
      "0.659942363112392 0.4607645875251509 0.8865039694207586\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "3401\n",
      "347\n",
      "0.10202881505439576\n",
      "sensitive attribute  5\n",
      "prec reca accuracy for each sens\n",
      "0.717948717948718 0.4 0.9229090909090909\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1375\n",
      "78\n",
      "0.05672727272727273\n",
      "sensitive attribute  6\n",
      "prec reca accuracy for each sens\n",
      "0.0 0.0 0.8260869565217391\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "23\n",
      "1\n",
      "0.043478260869565216\n",
      "data acceptance rates\n",
      "[0.29015544041450775, 0.06792324673119375, 0.071712093146335, 0.10202881505439576, 0.05672727272727273, 0.043478260869565216]\n",
      "data DP\n",
      "0.24667717954494253\n",
      "SVM accuracy--------------------------\n",
      "0.6714876033057852 0.46829971181556196 0.9145423646516144\n",
      "[[1, 1, 1, 1, 1, 1]]\n",
      "dimension of data\n",
      "6 12357\n",
      "Optimal\n",
      "objective is:\n",
      "1932.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.2970639032815199, 0.06707420614705382, 0.07105054247155332, 0.10202881505439576, 0.056, 0.043478260869565216] 0.0\n",
      "<--------------------------------------->\n",
      "iteration t 0\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "individual acceptance rates\n",
      "[0.2970639032815199, 0.06707420614705382, 0.07105054247155332, 0.10202881505439576, 0.056, 0.043478260869565216]\n",
      "individul precision\n",
      "[0.7034883720930233, 0.660759493670886, 0.6722532588454376, 0.654178674351585, 0.7142857142857143, 0.0]\n",
      "individual recall\n",
      "[0.568075117370892, 0.4442553191489362, 0.48262032085561496, 0.4567404426559356, 0.39285714285714285, 0.0]\n",
      "DP all\n",
      "0.2535856424119547\n",
      "precision all 0.6683991683991684\n",
      "recall all 0.46325648414985593\n",
      "accuracy all 0.9138949583232177\n",
      "TP,FP,TN,FN\n",
      "643 319 10650 745\n",
      "dimension of data\n",
      "6 12357\n",
      "Optimal\n",
      "objective is:\n",
      "2172.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.26252158894645944, 0.07361181864493123, 0.07687218840963218, 0.10173478388709203, 0.06545454545454546, 0.08695652173913043] 0.0\n",
      "<--------------------------------------->\n",
      "iteration t 1\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "individual acceptance rates\n",
      "[0.26252158894645944, 0.07361181864493123, 0.07687218840963218, 0.10173478388709203, 0.06545454545454546, 0.08695652173913043]\n",
      "individul precision\n",
      "[0.7105263157894737, 0.6147635524798154, 0.612736660929432, 0.6589595375722543, 0.6333333333333333, 0.0]\n",
      "individual recall\n",
      "[0.5070422535211268, 0.45361702127659576, 0.47593582887700536, 0.45875251509054327, 0.40714285714285714, 0.0]\n",
      "DP all\n",
      "0.19706704349191398\n",
      "precision all 0.6290480863591756\n",
      "recall all 0.4618155619596542\n",
      "accuracy all 0.9089584850691915\n",
      "TP,FP,TN,FN\n",
      "641 378 10591 747\n",
      "dimension of data\n",
      "6 12357\n",
      "Optimal\n",
      "objective is:\n",
      "2438.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.22279792746113988, 0.08023433520122261, 0.08269383434771103, 0.1014407527197883, 0.07418181818181818, 0.08695652173913043] 0.0\n",
      "<--------------------------------------->\n",
      "iteration t 2\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "individual acceptance rates\n",
      "[0.22279792746113988, 0.08023433520122261, 0.08269383434771103, 0.1014407527197883, 0.07418181818181818, 0.08695652173913043]\n",
      "individul precision\n",
      "[0.7286821705426356, 0.5682539682539682, 0.5536, 0.6608695652173913, 0.5588235294117647, 0.0]\n",
      "individual recall\n",
      "[0.4413145539906103, 0.4570212765957447, 0.4625668449197861, 0.45875251509054327, 0.40714285714285714, 0.0]\n",
      "DP all\n",
      "0.1486161092793217\n",
      "precision all 0.5875232774674115\n",
      "recall all 0.4546109510086455\n",
      "accuracy all 0.902889050740471\n",
      "TP,FP,TN,FN\n",
      "631 443 10526 757\n",
      "dimension of data\n",
      "6 12357\n",
      "Optimal\n",
      "objective is:\n",
      "2702.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.18134715025906736, 0.08685685175751401, 0.08851548028578989, 0.10085269038518083, 0.0829090909090909, 0.08695652173913043] 0.0\n",
      "<--------------------------------------->\n",
      "iteration t 3\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "individual acceptance rates\n",
      "[0.18134715025906736, 0.08685685175751401, 0.08851548028578989, 0.10085269038518083, 0.0829090909090909, 0.08695652173913043]\n",
      "individul precision\n",
      "[0.7142857142857143, 0.5278592375366569, 0.4962630792227205, 0.6588921282798834, 0.5, 0.0]\n",
      "individual recall\n",
      "[0.352112676056338, 0.4595744680851064, 0.44385026737967914, 0.45472837022132795, 0.40714285714285714, 0.0]\n",
      "DP all\n",
      "0.09843805934997646\n",
      "precision all 0.5452127659574468\n",
      "recall all 0.4430835734870317\n",
      "accuracy all 0.8959294327102048\n",
      "TP,FP,TN,FN\n",
      "615 513 10456 773\n",
      "dimension of data\n",
      "6 12357\n",
      "Optimal\n",
      "objective is:\n",
      "2952.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.1381692573402418, 0.09322465613856343, 0.09407250595395607, 0.09997059688326963, 0.09090909090909091, 0.08695652173913043] 0.0\n",
      "<--------------------------------------->\n",
      "iteration t 4\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "individual acceptance rates\n",
      "[0.1381692573402418, 0.09322465613856343, 0.09407250595395607, 0.09997059688326963, 0.09090909090909091, 0.08695652173913043]\n",
      "individul precision\n",
      "[0.725, 0.4936247723132969, 0.44866385372714485, 0.6558823529411765, 0.464, 0.0]\n",
      "individual recall\n",
      "[0.27230046948356806, 0.46127659574468083, 0.4264705882352941, 0.448692152917505, 0.4142857142857143, 0.0]\n",
      "DP all\n",
      "0.051212735601111364\n",
      "precision all 0.5093378607809848\n",
      "recall all 0.4322766570605187\n",
      "accuracy all 0.8894553694262362\n",
      "TP,FP,TN,FN\n",
      "600 578 10391 788\n",
      "dimension of data\n",
      "6 12357\n",
      "Optimal\n",
      "objective is:\n",
      "3268.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.1381692573402418, 0.0999320767532688, 0.09989415189203493, 0.10673331373125551, 0.09963636363636363, 0.08695652173913043] 0.0\n",
      "<--------------------------------------->\n",
      "iteration t 5\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "individual acceptance rates\n",
      "[0.1381692573402418, 0.0999320767532688, 0.09989415189203493, 0.10673331373125551, 0.09963636363636363, 0.08695652173913043]\n",
      "individul precision\n",
      "[0.7375, 0.46219201359388273, 0.4185430463576159, 0.6308539944903582, 0.4233576642335766, 0.0]\n",
      "individual recall\n",
      "[0.27699530516431925, 0.46297872340425533, 0.42245989304812837, 0.4607645875251509, 0.4142857142857143, 0.0]\n",
      "DP all\n",
      "0.051212735601111364\n",
      "precision all 0.4797136038186158\n",
      "recall all 0.43443804034582134\n",
      "accuracy all 0.8835477866796148\n",
      "TP,FP,TN,FN\n",
      "603 654 10315 785\n",
      "<--------------------------------------->\n"
     ]
    }
   ],
   "source": [
    "accu_all,DP_all,acc_rate,alpha,prec,rec = main(sensitive, Y_test, Y_test_pred,e )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(acc_rate)\\nprint(prec)\\nprint(rec)\\nalpha=np.arange(0,1.2,.2)  \\nprint(accu)\\nprint(alpha)\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''#check\n",
    "print(acc_rate)\n",
    "print(prec)\n",
    "print(rec)\n",
    "alpha=np.arange(0,1.2,.2)  \n",
    "print(accu)\n",
    "print(alpha)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_precision=[]\n",
    "weighted_recall=[]\n",
    "p=[0,3]\n",
    "r=[1,2,4]\n",
    "print(np.transpose(acc_rate))\n",
    "weight_prec=0\n",
    "weight_p=0\n",
    "weight_rec=0\n",
    "weight_r=0\n",
    "dp_list=[]\n",
    "     \n",
    "sizes=[579, 11778, 7558, 3401,1375]\n",
    "for i in range(6):\n",
    "    weight_prec=0\n",
    "    weight_p=0\n",
    "    weight_rec=0\n",
    "    weight_r=0\n",
    "    acc_list=[]\n",
    "    for j in range(5):\n",
    "        #print(j)\n",
    "        if j in p:\n",
    "            weight_prec=weight_prec+sizes[j]*prec[i][j]\n",
    "            weight_p=weight_p+sizes[j]\n",
    "            #print(j)\n",
    "        if j in r:    \n",
    "            weight_rec=weight_rec+sizes[j]*rec[i][j]\n",
    "            weight_r=weight_r+sizes[j]\n",
    "        #print(acc_rate[i][j])    \n",
    "        acc_list.append(acc_rate[i][j])\n",
    "    #print(acc_list)\n",
    "    dp=max(acc_list)-min(acc_list)   \n",
    "    dp_list.append(dp)     \n",
    "    wp=weight_prec/weight_p\n",
    "    wr=weight_rec/weight_r\n",
    "    weighted_precision.append(wp)\n",
    "    weighted_recall.append(wr)\n",
    "print(weighted_precision, weighted_recall,accu,dp_list)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensitive attribute  1\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "579\n",
      "213\n",
      "0.36787564766839376\n",
      "sensitive attribute  2\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "11778\n",
      "1175\n",
      "0.09976226863644082\n",
      "sensitive attribute  3\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "7558\n",
      "748\n",
      "0.09896798094734056\n",
      "sensitive attribute  4\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "3401\n",
      "497\n",
      "0.14613349014995589\n",
      "sensitive attribute  5\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1375\n",
      "140\n",
      "0.10181818181818182\n",
      "sensitive attribute  6\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "23\n",
      "3\n",
      "0.13043478260869565\n",
      "data acceptance rates\n",
      "[0.36787564766839376, 0.09976226863644082, 0.09896798094734056, 0.14613349014995589, 0.10181818181818182, 0.13043478260869565]\n",
      "data DP\n",
      "0.2689076667210532\n",
      "sensitive attribute  1\n",
      "prec reca accuracy for each sens\n",
      "0.7142857142857143 0.5633802816901409 0.7564766839378239\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "579\n",
      "168\n",
      "0.29015544041450775\n",
      "sensitive attribute  2\n",
      "prec reca accuracy for each sens\n",
      "0.6625 0.451063829787234 0.9223127865511972\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "11778\n",
      "800\n",
      "0.06792324673119375\n",
      "sensitive attribute  3\n",
      "prec reca accuracy for each sens\n",
      "0.6734317343173432 0.4879679144385027 0.9259063244244509\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "7558\n",
      "542\n",
      "0.071712093146335\n",
      "sensitive attribute  4\n",
      "prec reca accuracy for each sens\n",
      "0.659942363112392 0.4607645875251509 0.8865039694207586\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "3401\n",
      "347\n",
      "0.10202881505439576\n",
      "sensitive attribute  5\n",
      "prec reca accuracy for each sens\n",
      "0.717948717948718 0.4 0.9229090909090909\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1375\n",
      "78\n",
      "0.05672727272727273\n",
      "sensitive attribute  6\n",
      "prec reca accuracy for each sens\n",
      "0.0 0.0 0.8260869565217391\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "23\n",
      "1\n",
      "0.043478260869565216\n",
      "data acceptance rates\n",
      "[0.29015544041450775, 0.06792324673119375, 0.071712093146335, 0.10202881505439576, 0.05672727272727273, 0.043478260869565216]\n",
      "data DP\n",
      "0.24667717954494253\n",
      "SVM accuracy--------------------------\n",
      "0.6714876033057852 0.46829971181556196 0.9145423646516144\n",
      "[[1, 1, 1, 1, 1, 1]]\n",
      "dimension of data\n",
      "6 12357\n",
      "Optimal\n",
      "objective is:\n",
      "1932.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.2970639032815199, 0.06707420614705382, 0.07105054247155332, 0.10202881505439576, 0.056, 0.043478260869565216] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 0\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "individual acceptance rates\n",
      "[0.2970639032815199, 0.06707420614705382, 0.07105054247155332, 0.10202881505439576, 0.056, 0.043478260869565216]\n",
      "individul precision\n",
      "[0.7034883720930233, 0.6620253164556962, 0.6741154562383612, 0.6512968299711815, 0.7272727272727273, 0.0]\n",
      "individual recall\n",
      "[0.568075117370892, 0.4451063829787234, 0.4839572192513369, 0.45472837022132795, 0.4, 0.0]\n",
      "DP all\n",
      "0.2535856424119547\n",
      "precision all 0.6694386694386695\n",
      "recall all 0.46397694524495675\n",
      "accuracy all 0.9140568099053168\n",
      "TP,FP,TN,FN\n",
      "644 318 10651 744\n",
      "dimension of data\n",
      "6 12357\n",
      "Optimal\n",
      "objective is:\n",
      "1954.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.25734024179620035, 0.06911190354898965, 0.0723736438211167, 0.09732431637753602, 0.06036363636363636, 0.08695652173913043] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 0\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "individual acceptance rates\n",
      "[0.25734024179620035, 0.06911190354898965, 0.0723736438211167, 0.09732431637753602, 0.06036363636363636, 0.08695652173913043]\n",
      "individul precision\n",
      "[0.7181208053691275, 0.6511056511056511, 0.6672760511882998, 0.6525679758308157, 0.6746987951807228, 0.0]\n",
      "individual recall\n",
      "[0.5023474178403756, 0.451063829787234, 0.4879679144385027, 0.4346076458752515, 0.4, 0.0]\n",
      "DP all\n",
      "0.19697660543256398\n",
      "precision all 0.6614745586708204\n",
      "recall all 0.4589337175792507\n",
      "accuracy all 0.9128429230395727\n",
      "TP,FP,TN,FN\n",
      "637 326 10643 751\n",
      "dimension of data\n",
      "6 12357\n",
      "Optimal\n",
      "objective is:\n",
      "2002.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.21243523316062177, 0.07123450500933945, 0.07369674517068008, 0.09232578653337253, 0.06472727272727273, 0.08695652173913043] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 0\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "individual acceptance rates\n",
      "[0.21243523316062177, 0.07123450500933945, 0.07369674517068008, 0.09232578653337253, 0.06472727272727273, 0.08695652173913043]\n",
      "individul precision\n",
      "[0.7154471544715447, 0.6328963051251489, 0.6409335727109515, 0.6560509554140127, 0.6292134831460674, 0.0]\n",
      "individual recall\n",
      "[0.4131455399061033, 0.45191489361702125, 0.4772727272727273, 0.41448692152917505, 0.4, 0.0]\n",
      "DP all\n",
      "0.14770796043334905\n",
      "precision all 0.6434511434511434\n",
      "recall all 0.44596541786743515\n",
      "accuracy all 0.9100105203528365\n",
      "TP,FP,TN,FN\n",
      "619 343 10626 769\n",
      "dimension of data\n",
      "6 12357\n",
      "Optimal\n",
      "objective is:\n",
      "2046.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.16753022452504318, 0.07327220241127526, 0.07488753638528711, 0.08732725668920906, 0.06909090909090909, 0.08695652173913043] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 0\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "individual acceptance rates\n",
      "[0.16753022452504318, 0.07327220241127526, 0.07488753638528711, 0.08732725668920906, 0.06909090909090909, 0.08695652173913043]\n",
      "individul precision\n",
      "[0.6804123711340206, 0.6164542294322132, 0.6201413427561837, 0.6430976430976431, 0.5894736842105263, 0.0]\n",
      "individual recall\n",
      "[0.30985915492957744, 0.45276595744680853, 0.4692513368983957, 0.3843058350100604, 0.4, 0.0]\n",
      "DP all\n",
      "0.09843931543413409\n",
      "precision all 0.6229166666666667\n",
      "recall all 0.430835734870317\n",
      "accuracy all 0.9067734887108522\n",
      "TP,FP,TN,FN\n",
      "598 362 10607 790\n",
      "dimension of data\n",
      "6 12357\n",
      "Optimal\n",
      "objective is:\n",
      "2104.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.12435233160621761, 0.07539480387162506, 0.07621063773485048, 0.0826227580123493, 0.07345454545454545, 0.08695652173913043] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 0\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "individual acceptance rates\n",
      "[0.12435233160621761, 0.07539480387162506, 0.07621063773485048, 0.0826227580123493, 0.07345454545454545, 0.08695652173913043]\n",
      "individul precision\n",
      "[0.6666666666666666, 0.5968468468468469, 0.5954861111111112, 0.6370106761565836, 0.5544554455445545, 0.0]\n",
      "individual recall\n",
      "[0.22535211267605634, 0.451063829787234, 0.4585561497326203, 0.36016096579476864, 0.4, 0.0]\n",
      "DP all\n",
      "0.05089778615167216\n",
      "precision all 0.6020833333333333\n",
      "recall all 0.4164265129682997\n",
      "accuracy all 0.9035364570688679\n",
      "TP,FP,TN,FN\n",
      "578 382 10587 810\n",
      "dimension of data\n",
      "6 12357\n",
      "Optimal\n",
      "objective is:\n",
      "2184.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.08635578583765112, 0.07751740533197486, 0.07753373908441387, 0.07880035283740076, 0.07781818181818181, 0.08695652173913043] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 0\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "individual acceptance rates\n",
      "[0.08808290155440414, 0.07743250127356088, 0.07753373908441387, 0.07880035283740076, 0.07781818181818181, 0.08695652173913043]\n",
      "individul precision\n",
      "[0.7058823529411765, 0.5756578947368421, 0.5665529010238908, 0.6417910447761194, 0.5327102803738317, 0.0]\n",
      "individual recall\n",
      "[0.16901408450704225, 0.44680851063829785, 0.44385026737967914, 0.3460764587525151, 0.40714285714285714, 0.0]\n",
      "DP all\n",
      "0.010650400280843256\n",
      "precision all 0.5825545171339563\n",
      "recall all 0.404178674351585\n",
      "accuracy all 0.9005422028000324\n",
      "TP,FP,TN,FN\n",
      "561 402 10567 827\n",
      "<--------------------------------------->\n"
     ]
    }
   ],
   "source": [
    "#LP-5 without cv values\n",
    "\n",
    "accu_all,DP_all,acceptance_rate,alpha_weight = main(sensitive, Y_test, Y_test_pred,e )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.2970639  0.25734024 0.21243523 0.16753022 0.12435233 0.08635579]\n",
      " [0.06707421 0.0691119  0.07123451 0.0732722  0.0753948  0.07751741]\n",
      " [0.07105054 0.07237364 0.07369675 0.07488754 0.07621064 0.07753374]\n",
      " [0.10202882 0.09732432 0.09232579 0.08732726 0.08262276 0.07880035]\n",
      " [0.056      0.06036364 0.06472727 0.06909091 0.07345455 0.07781818]\n",
      " [0.04347826 0.08695652 0.08695652 0.08695652 0.08695652 0.08695652]]\n",
      "[0.658889519139158, 0.6621044301782234, 0.664691759246754, 0.6485260922265532, 0.6413249521629499, 0.6511148808131968] [0.456289490696973, 0.46114100162040683, 0.45772202644722854, 0.4552787915159377, 0.45040785895964397, 0.4430955718084552] [0.914, 0.9128, 0.91, 0.9067, 0.9035, 0.9005] [0.2410639032815199, 0.19697660543256398, 0.14770796043334905, 0.09843931543413409, 0.05089778615167216, 0.008838380505676258]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "acc_rate=[[0.2970639032815199, 0.06707420614705382, 0.07105054247155332, 0.10202881505439576, 0.056, 0.043478260869565216],\n",
    "[0.25734024179620035, 0.06911190354898965, 0.0723736438211167, 0.09732431637753602, 0.06036363636363636, 0.08695652173913043],\n",
    "[0.21243523316062177, 0.07123450500933945, 0.07369674517068008, 0.09232578653337253, 0.06472727272727273, 0.08695652173913043],\n",
    "[0.16753022452504318, 0.07327220241127526, 0.07488753638528711, 0.08732725668920906, 0.06909090909090909, 0.08695652173913043],\n",
    "[0.12435233160621761, 0.07539480387162506, 0.07621063773485048, 0.0826227580123493, 0.07345454545454545, 0.08695652173913043],\n",
    "[0.08635578583765112, 0.07751740533197486, 0.07753373908441387, 0.07880035283740076, 0.07781818181818181, 0.08695652173913043]]\n",
    "\n",
    "prec=[[0.7034883720930233, 0.6620253164556962, 0.6741154562383612, 0.6512968299711815, 0.7272727272727273, 0.0],\n",
    "[0.7181208053691275, 0.6511056511056511, 0.6672760511882998, 0.6525679758308157, 0.6746987951807228, 0.0],\n",
    "[0.7154471544715447, 0.6328963051251489, 0.6409335727109515, 0.6560509554140127, 0.6292134831460674, 0.0],\n",
    "[0.6804123711340206, 0.6164542294322132, 0.6201413427561837, 0.6430976430976431, 0.5894736842105263, 0.0],\n",
    "[0.6666666666666666, 0.5968468468468469, 0.5954861111111112, 0.6370106761565836, 0.5544554455445545, 0.0],\n",
    "[0.7058823529411765, 0.5756578947368421, 0.5665529010238908, 0.6417910447761194, 0.5327102803738317, 0.0]]\n",
    "\n",
    "rec=[[0.568075117370892, 0.4451063829787234, 0.4839572192513369, 0.45472837022132795, 0.4, 0.0],\n",
    "[0.5023474178403756, 0.451063829787234, 0.4879679144385027, 0.4346076458752515, 0.4, 0.0],\n",
    "[0.4131455399061033, 0.45191489361702125, 0.4772727272727273, 0.41448692152917505, 0.4, 0.0],\n",
    "[0.30985915492957744, 0.45276595744680853, 0.4692513368983957, 0.3843058350100604, 0.4, 0.0],\n",
    "[0.22535211267605634, 0.451063829787234, 0.4585561497326203, 0.36016096579476864, 0.4, 0.0],\n",
    "[0.16901408450704225, 0.44680851063829785, 0.44385026737967914, 0.3460764587525151, 0.40714285714285714, 0.0]]\n",
    "\n",
    "accu=[0.9140,0.9128,0.9100,0.9067, 0.9035, 0.9005]\n",
    "weighted_precision=[]\n",
    "weighted_recall=[]\n",
    "p=[0,3]\n",
    "r=[1,2,4]\n",
    "print(np.transpose(acc_rate))\n",
    "weight_prec=0\n",
    "weight_p=0\n",
    "weight_rec=0\n",
    "weight_r=0\n",
    "dp_list=[]\n",
    "     \n",
    "sizes=[579, 11778, 7558, 3401,1375]\n",
    "for i in range(6):\n",
    "    acc_list=[]\n",
    "    weight_prec=0\n",
    "    weight_p=0\n",
    "    weight_rec=0\n",
    "    weight_r=0\n",
    "    for j in range(5):\n",
    "        #print(j)\n",
    "        if j in p:\n",
    "            weight_prec=weight_prec+sizes[j]*prec[i][j]\n",
    "            weight_p=weight_p+sizes[j]\n",
    "            #print(j)\n",
    "        if j in r:    \n",
    "            weight_rec=weight_rec+sizes[j]*rec[i][j]\n",
    "            weight_r=weight_r+sizes[j]\n",
    "        #print(acc_rate[i][j])    \n",
    "        acc_list.append(acc_rate[i][j])\n",
    "    #print(acc_list)\n",
    "    dp=max(acc_list)-min(acc_list)   \n",
    "    dp_list.append(dp)     \n",
    "    wp=weight_prec/weight_p\n",
    "    wr=weight_rec/weight_r\n",
    "    weighted_precision.append(wp)\n",
    "    weighted_recall.append(wr)\n",
    "print(weighted_precision, weighted_recall,accu,dp_list)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensitive attribute  1\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "579\n",
      "213\n",
      "0.36787564766839376\n",
      "sensitive attribute  2\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "11778\n",
      "1175\n",
      "0.09976226863644082\n",
      "sensitive attribute  3\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "7558\n",
      "748\n",
      "0.09896798094734056\n",
      "sensitive attribute  4\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "3401\n",
      "497\n",
      "0.14613349014995589\n",
      "sensitive attribute  5\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1375\n",
      "140\n",
      "0.10181818181818182\n",
      "sensitive attribute  6\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "23\n",
      "3\n",
      "0.13043478260869565\n",
      "data acceptance rates\n",
      "[0.36787564766839376, 0.09976226863644082, 0.09896798094734056, 0.14613349014995589, 0.10181818181818182, 0.13043478260869565]\n",
      "data DP\n",
      "0.2689076667210532\n",
      "sensitive attribute  1\n",
      "prec reca accuracy for each sens\n",
      "0.7142857142857143 0.5633802816901409 0.7564766839378239\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "579\n",
      "168\n",
      "0.29015544041450775\n",
      "sensitive attribute  2\n",
      "prec reca accuracy for each sens\n",
      "0.6625 0.451063829787234 0.9223127865511972\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "11778\n",
      "800\n",
      "0.06792324673119375\n",
      "sensitive attribute  3\n",
      "prec reca accuracy for each sens\n",
      "0.6734317343173432 0.4879679144385027 0.9259063244244509\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "7558\n",
      "542\n",
      "0.071712093146335\n",
      "sensitive attribute  4\n",
      "prec reca accuracy for each sens\n",
      "0.659942363112392 0.4607645875251509 0.8865039694207586\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "3401\n",
      "347\n",
      "0.10202881505439576\n",
      "sensitive attribute  5\n",
      "prec reca accuracy for each sens\n",
      "0.717948717948718 0.4 0.9229090909090909\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1375\n",
      "78\n",
      "0.05672727272727273\n",
      "sensitive attribute  6\n",
      "prec reca accuracy for each sens\n",
      "0.0 0.0 0.8260869565217391\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "23\n",
      "1\n",
      "0.043478260869565216\n",
      "data acceptance rates\n",
      "[0.29015544041450775, 0.06792324673119375, 0.071712093146335, 0.10202881505439576, 0.05672727272727273, 0.043478260869565216]\n",
      "data DP\n",
      "0.24667717954494253\n",
      "SVM accuracy--------------------------\n",
      "0.6714876033057852 0.46829971181556196 0.9145423646516144\n",
      "[[1, 1, 1, 1, 1, 1]]\n",
      "dimension of data\n",
      "6 12357\n",
      "Optimal\n",
      "objective is:\n",
      "1932.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.2970639032815199, 0.06707420614705382, 0.07105054247155332, 0.10202881505439576, 0.056, 0.043478260869565216] 0.0\n",
      "<--------------------------------------->\n",
      "iteration t 0\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "individual acceptance rates\n",
      "[0.2970639032815199, 0.06707420614705382, 0.07105054247155332, 0.10202881505439576, 0.056, 0.043478260869565216]\n",
      "individul precision\n",
      "[0.7034883720930233, 0.660759493670886, 0.6722532588454376, 0.654178674351585, 0.7142857142857143, 0.0]\n",
      "individual recall\n",
      "[0.568075117370892, 0.4442553191489362, 0.48262032085561496, 0.4567404426559356, 0.39285714285714285, 0.0]\n",
      "DP all\n",
      "0.2535856424119547\n",
      "precision all 0.6683991683991684\n",
      "recall all 0.46325648414985593\n",
      "accuracy all 0.9138949583232177\n",
      "TP,FP,TN,FN\n",
      "643 319 10650 745\n",
      "dimension of data\n",
      "6 12357\n",
      "Optimal\n",
      "objective is:\n",
      "1954.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.25734024179620035, 0.06911190354898965, 0.0723736438211167, 0.09732431637753602, 0.06036363636363636, 0.08695652173913043] 0.0\n",
      "<--------------------------------------->\n",
      "iteration t 0\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "individual acceptance rates\n",
      "[0.25734024179620035, 0.06911190354898965, 0.0723736438211167, 0.09732431637753602, 0.06036363636363636, 0.08695652173913043]\n",
      "individul precision\n",
      "[0.7046979865771812, 0.6511056511056511, 0.6617915904936015, 0.6555891238670695, 0.6746987951807228, 0.0]\n",
      "individual recall\n",
      "[0.49295774647887325, 0.451063829787234, 0.4839572192513369, 0.43661971830985913, 0.4, 0.0]\n",
      "DP all\n",
      "0.19697660543256398\n",
      "precision all 0.6593977154724818\n",
      "recall all 0.45749279538904897\n",
      "accuracy all 0.9125192198753743\n",
      "TP,FP,TN,FN\n",
      "635 328 10641 753\n",
      "dimension of data\n",
      "6 12357\n",
      "Optimal\n",
      "objective is:\n",
      "2002.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.21243523316062177, 0.07123450500933945, 0.07369674517068008, 0.09232578653337253, 0.06472727272727273, 0.08695652173913043] 0.0\n",
      "<--------------------------------------->\n",
      "iteration t 0\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "individual acceptance rates\n",
      "[0.21243523316062177, 0.07123450500933945, 0.07369674517068008, 0.09232578653337253, 0.06472727272727273, 0.08695652173913043]\n",
      "individul precision\n",
      "[0.7073170731707317, 0.6328963051251489, 0.6409335727109515, 0.6496815286624203, 0.6404494382022472, 0.0]\n",
      "individual recall\n",
      "[0.4084507042253521, 0.45191489361702125, 0.4772727272727273, 0.4104627766599598, 0.40714285714285714, 0.0]\n",
      "DP all\n",
      "0.14770796043334905\n",
      "precision all 0.6424116424116424\n",
      "recall all 0.4452449567723343\n",
      "accuracy all 0.9098486687707372\n",
      "TP,FP,TN,FN\n",
      "618 344 10625 770\n",
      "dimension of data\n",
      "6 12357\n",
      "Optimal\n",
      "objective is:\n",
      "2046.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.16753022452504318, 0.07327220241127526, 0.07488753638528711, 0.08732725668920906, 0.06909090909090909, 0.08695652173913043] 0.0\n",
      "<--------------------------------------->\n",
      "iteration t 0\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "individual acceptance rates\n",
      "[0.16753022452504318, 0.07327220241127526, 0.07488753638528711, 0.08732725668920906, 0.06909090909090909, 0.08695652173913043]\n",
      "individul precision\n",
      "[0.7010309278350515, 0.6164542294322132, 0.6219081272084805, 0.6464646464646465, 0.5894736842105263, 0.0]\n",
      "individual recall\n",
      "[0.3192488262910798, 0.45276595744680853, 0.47058823529411764, 0.386317907444668, 0.4, 0.0]\n",
      "DP all\n",
      "0.09843931543413409\n",
      "precision all 0.625\n",
      "recall all 0.4322766570605187\n",
      "accuracy all 0.9070971918750506\n",
      "TP,FP,TN,FN\n",
      "600 360 10609 788\n",
      "dimension of data\n",
      "6 12357\n",
      "Optimal\n",
      "objective is:\n",
      "2104.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.12435233160621761, 0.07539480387162506, 0.07621063773485048, 0.0826227580123493, 0.07345454545454545, 0.08695652173913043] 0.0\n",
      "<--------------------------------------->\n",
      "iteration t 0\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "individual acceptance rates\n",
      "[0.12435233160621761, 0.07539480387162506, 0.07621063773485048, 0.0826227580123493, 0.07345454545454545, 0.08695652173913043]\n",
      "individul precision\n",
      "[0.6944444444444444, 0.6002252252252253, 0.6024305555555556, 0.6370106761565836, 0.5643564356435643, 0.0]\n",
      "individual recall\n",
      "[0.2347417840375587, 0.45361702127659576, 0.46390374331550804, 0.36016096579476864, 0.40714285714285714, 0.0]\n",
      "DP all\n",
      "0.05089778615167216\n",
      "precision all 0.6072916666666667\n",
      "recall all 0.420028818443804\n",
      "accuracy all 0.9043457149793639\n",
      "TP,FP,TN,FN\n",
      "583 377 10592 805\n",
      "dimension of data\n",
      "6 12357\n",
      "Optimal\n",
      "objective is:\n",
      "2186.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.08635578583765112, 0.07751740533197486, 0.07753373908441387, 0.07880035283740076, 0.07781818181818181, 0.08695652173913043] 0.0\n",
      "<--------------------------------------->\n",
      "iteration t 0\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "individual acceptance rates\n",
      "[0.08635578583765112, 0.07751740533197486, 0.07753373908441387, 0.07880035283740076, 0.07781818181818181, 0.08695652173913043]\n",
      "individul precision\n",
      "[0.68, 0.5739320920043811, 0.5665529010238908, 0.6305970149253731, 0.5327102803738317, 0.0]\n",
      "individual recall\n",
      "[0.1596244131455399, 0.4459574468085106, 0.44385026737967914, 0.34004024144869216, 0.40714285714285714, 0.0]\n",
      "DP all\n",
      "0.00943911640715557\n",
      "precision all 0.5794392523364486\n",
      "recall all 0.4020172910662824\n",
      "accuracy all 0.9000566480537348\n",
      "TP,FP,TN,FN\n",
      "558 405 10564 830\n",
      "<--------------------------------------->\n"
     ]
    }
   ],
   "source": [
    "#without cv values--part 2\n",
    "\n",
    "accu_all,DP_all,acceptance_rate,alpha_weight = main(sensitive, Y_test, Y_test_pred,e )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
