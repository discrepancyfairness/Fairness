{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries necessary for this project\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "from time import time\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Import 'GridSearchCV', 'make_scorer', and any other necessary libraries\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Import the three supervised learning models from sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA    \n",
    "\n",
    "# Pretty display for notebooks\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/subham/anaconda3/lib/python3.7/site-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['time']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline\n",
    "from random import *\n",
    "from subprocess import check_output\n",
    "def synthetic_svm(X,Y):\n",
    "    #Split data into training and test datasets (training will be based on 70% of data)\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0,shuffle=True) \n",
    "    #test_size: if integer, number of examples into test dataset; if between 0.0 and 1.0, means proportion\n",
    "    print('There are {} samples in the training set and {} samples in the test set'.format(X_train.shape[0], X_test.shape[0]))\n",
    "\n",
    "    \n",
    "    #Scaling data\n",
    "    #from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    from sklearn.model_selection import cross_val_score\n",
    "\n",
    "    #sc = StandardScaler(with_mean=False)\n",
    "    \n",
    "    \n",
    "    #sc.fit(X_train)\n",
    "    #X_train_std = sc.transform(X_train)\n",
    "    #X_test_std = sc.transform(X_test)\n",
    "\n",
    "    #X_train_std and X_test_std are the scaled datasets to be used in algorithms\n",
    "    #.1,.10\n",
    "    #Applying SVC (Support Vector Classification)\n",
    "    from sklearn.svm import SVC\n",
    "    svm = SVC(kernel='rbf', random_state=0, gamma=.1, C=10.0,probability=True)\n",
    "    \n",
    "    print(Y_train.dtypes)\n",
    "    Y_train=Y_train.astype('int')\n",
    "    print(Y_train.dtypes)\n",
    "    \n",
    "    print(Y_test.dtypes)\n",
    "    Y_test=Y_test.astype('int')\n",
    "    print(Y_test.dtypes)\n",
    "    \n",
    "    \n",
    "    svm.fit(X_train, Y_train)\n",
    "    print('The accuracy of the SVM classifier on training data is {:.2f}'.format(svm.score(X_train, Y_train)))\n",
    "    print('The accuracy of the SVM classifier on test data is {:.2f}'.format(svm.score(X_test, Y_test)))\n",
    "    print('####Train prediction Label###############################################')\n",
    "    Y_train_pred=svm.predict(X_train)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #print(y_1)\n",
    "    Y_test_pred=svm.predict(X_test)\n",
    "    d=svm.decision_function(X_test)\n",
    "    e=svm.predict_proba(X_test)\n",
    "    print(e)\n",
    "    print(d)\n",
    "    \n",
    "    \n",
    "           \n",
    "\n",
    "        \n",
    "    \n",
    "    return X_test,Y_test,Y_test_pred,e \n",
    "     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#without accuracy ---> with cv\n",
    "def main(datax, y_test, y_test_pred,e): \n",
    "        \n",
    "    n=datax.shape[1]\n",
    "    s=datax.shape[0]    \n",
    "    data = np.zeros((s, n), dtype = int)\n",
    "    \n",
    "    r = np.zeros(n, dtype = int) \n",
    "    \n",
    "    for i in range(n):\n",
    "        if int(y_test.iloc[i])==1 :\n",
    "            r[i]=1\n",
    "        else :\n",
    "            r[i]= -1  \n",
    "    \n",
    "    r2 = np.zeros(n, dtype = int) \n",
    "    for i in range(n):\n",
    "        if int(y_test_pred[i])==1 :\n",
    "            r2[i]=1\n",
    "        else :\n",
    "            r2[i]= -1          \n",
    "    ar=[]\n",
    "    \n",
    "    for j in range(s):\n",
    "        print(\"sensitive attribute \",(j+1)) \n",
    "        a=0\n",
    "        b=0\n",
    "        acc1=0\n",
    "        acc2=0\n",
    "        for i in range(n):\n",
    "                data[j][i]= datax.iloc[j,i]\n",
    "                if data[j][i]== 1 :\n",
    "                    a=a+1\n",
    "                    if r[i]==1:\n",
    "                         acc1=acc1+1 \n",
    "\n",
    "        print(\"ACTUAL----------total ,accepted, aceeptance rate:\")             \n",
    "        a1=float(acc1/a)\n",
    "        print(a)\n",
    "        \n",
    "        print(acc1)\n",
    "        print(a1)\n",
    "        ar.append(a1)\n",
    "        \n",
    "    maxi= max(ar)\n",
    "    mini= min(ar)\n",
    "    DP=float(maxi-mini)\n",
    "    print(\"data acceptance rates\")\n",
    "    print(ar)\n",
    "    beta_act = []\n",
    "    beta_act = ar\n",
    "    print(\"data DP\")\n",
    "    print(DP)\n",
    "    \n",
    "    ar=[]\n",
    "    \n",
    "    for j in range(s):\n",
    "        print(\"sensitive attribute \",(j+1)) \n",
    "        a=0\n",
    "        b=0\n",
    "        acc1=0\n",
    "        acc2=0\n",
    "        prec=0\n",
    "        reca=0\n",
    "        accur=0\n",
    "        FP=0\n",
    "        FN=0\n",
    "        TP=0\n",
    "        TN=0\n",
    "        for i in range(n):\n",
    "             if data[j][i]== 1 :\n",
    "                    a=a+1\n",
    "                    if r2[i]==1:\n",
    "                        acc1=acc1+1 \n",
    "                        if r[i]==1:\n",
    "                            TP=TP+1\n",
    "                        else:\n",
    "                             FP=FP+1                \n",
    "                    else:\n",
    "                        if r[i]==1:\n",
    "                            FN=FN+1\n",
    "                        else:\n",
    "                            TN=TN+1    \n",
    "        \n",
    "        print(\"prec reca accuracy for each sens\") \n",
    "        prec= float(TP/(TP+FP))\n",
    "        reca= float(TP/(TP+FN))\n",
    "        accur= float((TP+TN)/a)\n",
    "        print(prec,reca,accur)\n",
    "        \n",
    "        print(\"SVM----------total , accepted, aceeptance rate:\")             \n",
    "        \n",
    "        a1=float(acc1/a)\n",
    "        print(a)\n",
    "        \n",
    "        print(acc1)\n",
    "        print(a1)\n",
    "        ar.append(a1)\n",
    "        \n",
    "    maxi= max(ar)\n",
    "    mini= min(ar)\n",
    "    DP=float(maxi-mini)\n",
    "    print(\"data acceptance rates\")\n",
    "    print(ar)\n",
    "    print(\"data DP\")\n",
    "    print(DP) \n",
    "    \n",
    "    print(\"SVM accuracy--------------------------\")\n",
    "    prec=0\n",
    "    reca=0\n",
    "    accur=0\n",
    "    FP=0\n",
    "    FN=0\n",
    "    TP=0\n",
    "    TN=0\n",
    "    for i in range(n):\n",
    "            if r2[i]==1:\n",
    "                acc1=acc1+1 \n",
    "                if r[i]==1:\n",
    "                    TP=TP+1\n",
    "                else:\n",
    "                     FP=FP+1                \n",
    "            else:\n",
    "                if r[i]==1:\n",
    "                     FN=FN+1\n",
    "                else:\n",
    "                     TN=TN+1    \n",
    "\n",
    "        \n",
    "    prec= float(TP/(TP+FP))\n",
    "    reca= float(TP/(TP+FN))\n",
    "    accur= float((TP+TN)/n)\n",
    "    print(prec,reca,accur)\n",
    "    \n",
    "######################\n",
    "    delta=1\n",
    "\n",
    "    epsilon=[.01]\n",
    "    beta_converge = [.5,.4]\n",
    "    #beta_converge = [.15]\n",
    "    alpha = [0,0.2,0.4,0.6,0.8,1]\n",
    "    \n",
    "    \n",
    "    \n",
    "    fi= np.zeros(n,dtype=int) \n",
    "    t=0\n",
    "  \n",
    "    #gamma = [0.175442,    0.142103, 0.166039,    0.164754,  0.153465,    0.14,  0.104348 ]\n",
    "    for eps in epsilon:\n",
    "        for beta_avg in beta_converge:\n",
    "            print(\"----------------This is for covergence at beta = \",beta_avg, \" ----------------\")\n",
    "            for a in alpha:\n",
    "                # no weighting basic weighting 1\n",
    "                u1,u2=min_max_lp_all_ng5(data,beta_act,eps,e,beta_avg,a)\n",
    "                #######################Disp_impact#######################  \n",
    "                print(\"alpha, beta_avg\",a,beta_avg)\n",
    "                accu_all=[]\n",
    "                DP_all=[]\n",
    "                precision_all=[]\n",
    "                recall_all=[]\n",
    "                ar_all=[]\n",
    "                acceptance_rate=np.zeros((7,28),dtype=float)\n",
    "                count=0\n",
    "                print(\"<--------------------------------------->\")\n",
    "                t=t+1\n",
    "                print(\"iteration t\",t)\n",
    "        #                 for alpha in np.arange(0,1.05,0.05):\n",
    "        #                     print(\"alpha: \",alpha)\n",
    "        #                     for i in range(n):\n",
    "\n",
    "        #                         z=random()\n",
    "        #                         if z < alpha:\n",
    "        #                                fi[i]= u1[i] \n",
    "\n",
    "        #                         else:\n",
    "        #                                fi[i]= r2[i]\n",
    "\n",
    "                for i in range(n):\n",
    "                     fi[i] = u1[i]\n",
    "\n",
    "\n",
    "                for j in range(s):\n",
    "                    print(\"sensitive attribute \",(j+1)) \n",
    "\n",
    "                    TP=0\n",
    "                    FP=0\n",
    "                    FN=0\n",
    "                    TN=0\n",
    "                    precision=0\n",
    "                    recall=0\n",
    "                    for i in range(n):\n",
    "                         if data[j][i]== 1 :                        \n",
    "                            if fi[i]==1 and r[i]==1:\n",
    "                                TP=TP+1\n",
    "                            if fi[i]==1 and r[i]==-1:\n",
    "                                FP=FP+1 \n",
    "                            if fi[i]==-1 and r[i]==1:\n",
    "                                FN=FN+1\n",
    "                            if fi[i]==-1 and r[i]==-1:\n",
    "                                TN=TN+1    \n",
    "                    if TP+FP !=0:\n",
    "                        precision=float(TP/(TP+FP))\n",
    "                    #print(\"precision\",precision)\n",
    "                    if TP+FN !=0:    \n",
    "                        recall=float(TP/(TP+FN))\n",
    "                   # print(\"recall\",recall)\n",
    "\n",
    "                    precision_all.append(precision)\n",
    "                    recall_all.append(recall)\n",
    "                    #print(\"TP,FP,TN,FN\")\n",
    "                    #print(TP,FP,TN,FN)\n",
    "\n",
    "                    a=0\n",
    "                    b=0\n",
    "                    acc1=0\n",
    "                    acc2=0\n",
    "                    for i in range(n):\n",
    "                            if data[j][i]== 1 :\n",
    "                                a=a+1\n",
    "                                if fi[i]==1:\n",
    "                                     acc1=acc1+1 \n",
    "\n",
    "        #                         print(\"total ,fair accepted, aceeptance rate:\")             \n",
    "                    a1=float(acc1/a)\n",
    "\n",
    "\n",
    "\n",
    "        #                         print(a)\n",
    "        #                         print(acc1)\n",
    "        #                         print(a1)\n",
    "                    ar_all.append(a1)\n",
    "\n",
    "                count = count+1\n",
    "                maxi=max(ar_all)\n",
    "                mini= min(ar_all)\n",
    "                DP=float(maxi-mini)\n",
    "                print(\"individual acceptance rates\")\n",
    "                print(ar_all)\n",
    "                print(\"individul precision\")\n",
    "                print(precision_all)\n",
    "                print(\"individual recall\")\n",
    "                print(recall_all)\n",
    "                print(\"DP all\")\n",
    "                print(DP)\n",
    "                f_acc=0\n",
    "                for i in range(n):\n",
    "                     if fi[i] == r[i]:\n",
    "                            f_acc=f_acc+1\n",
    "                f_acc_l=float((f_acc*100)/n) \n",
    "\n",
    "        #######################################################################33   \n",
    "\n",
    "        #                         print(\"sensitive attribute \",(j+1)) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                TP=0\n",
    "                FP=0\n",
    "                FN=0\n",
    "                TN=0\n",
    "                precision=0\n",
    "                recall=0\n",
    "                accu=0\n",
    "                for i in range(n):\n",
    "                        if fi[i]==1 and r[i]==1:\n",
    "                            TP=TP+1\n",
    "                        if fi[i]==1 and r[i]==-1:\n",
    "                            FP=FP+1 \n",
    "                        if fi[i]==-1 and r[i]==1:\n",
    "                            FN=FN+1\n",
    "                        if fi[i]==-1 and r[i]==-1:\n",
    "                            TN=TN+1    \n",
    "\n",
    "                if TP+FP!=0:\n",
    "                    precision=float(TP/(TP+FP))\n",
    "                print(\"precision all\",precision)\n",
    "                if TP+FN!=0:\n",
    "                    recall=float(TP/(TP+FN))\n",
    "\n",
    "\n",
    "                print(\"recall all\",recall)\n",
    "                accu=float((TP+TN)/(TP+FN+TN+FP))\n",
    "\n",
    "\n",
    "                print(\"accuracy all\",accu)\n",
    "\n",
    "\n",
    "\n",
    "                print(\"TP,FP,TN,FN\")\n",
    "                print(TP,FP,TN,FN)\n",
    "        #                         print(\"total ,fair accepted, aceeptance rate:\")             \n",
    "                a1=float(acc1/a)\n",
    "          \n",
    "\n",
    "    print(\"<--------------------------------------->\")\n",
    "    alpha_weight=np.arange(0,1.05,.05)        \n",
    "    return accu_all,DP_all,acceptance_rate,alpha_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#without accuracy ---> without cv\n",
    "def main2(datax, y_test, y_test_pred,e): \n",
    "        \n",
    "    n=datax.shape[1]\n",
    "    s=datax.shape[0]    \n",
    "    data = np.zeros((s, n), dtype = int)\n",
    "    \n",
    "    r = np.zeros(n, dtype = int) \n",
    "    \n",
    "    for i in range(n):\n",
    "        if int(y_test.iloc[i])==1 :\n",
    "            r[i]=1\n",
    "        else :\n",
    "            r[i]= -1  \n",
    "    \n",
    "    r2 = np.zeros(n, dtype = int) \n",
    "    for i in range(n):\n",
    "        if int(y_test_pred[i])==1 :\n",
    "            r2[i]=1\n",
    "        else :\n",
    "            r2[i]= -1          \n",
    "    ar=[]\n",
    "    \n",
    "    for j in range(s):\n",
    "        print(\"sensitive attribute \",(j+1)) \n",
    "        a=0\n",
    "        b=0\n",
    "        acc1=0\n",
    "        acc2=0\n",
    "        for i in range(n):\n",
    "                data[j][i]= datax.iloc[j,i]\n",
    "                if data[j][i]== 1 :\n",
    "                    a=a+1\n",
    "                    if r[i]==1:\n",
    "                         acc1=acc1+1 \n",
    "\n",
    "        print(\"ACTUAL----------total ,accepted, aceeptance rate:\")             \n",
    "        a1=float(acc1/a)\n",
    "        print(a)\n",
    "        \n",
    "        print(acc1)\n",
    "        print(a1)\n",
    "        ar.append(a1)\n",
    "        \n",
    "    maxi= max(ar)\n",
    "    mini= min(ar)\n",
    "    DP=float(maxi-mini)\n",
    "    print(\"data acceptance rates\")\n",
    "    print(ar)\n",
    "    beta_act = []\n",
    "    beta_act = [0.5229226361031518, 0.5017626321974148, 0.7256857855361596, 0.28905597326649957, 0.4561891515994437, 0.5300416418798334, 0.5399416909620991, 0.42773722627737226, 0.3055555555555556, 0.7062706270627063, 0.4864864864864865, 0.514933628318584, 0.5434131736526946, 0.4942263279445728, 0.2857142857142857, 0.7140562248995984, 0.2900964066608238, 0.7053216838760922, 0.5892053973013494, 0.4766301211771495]\n",
    "    print(\"data DP\")\n",
    "    print(DP)\n",
    "    \n",
    "    ar=[]\n",
    "    \n",
    "    for j in range(s):\n",
    "        print(\"sensitive attribute \",(j+1)) \n",
    "        a=0\n",
    "        b=0\n",
    "        acc1=0\n",
    "        acc2=0\n",
    "        prec=0\n",
    "        reca=0\n",
    "        accur=0\n",
    "        FP=0\n",
    "        FN=0\n",
    "        TP=0\n",
    "        TN=0\n",
    "        for i in range(n):\n",
    "             if data[j][i]== 1 :\n",
    "                    a=a+1\n",
    "                    if r2[i]==1:\n",
    "                        acc1=acc1+1 \n",
    "                        if r[i]==1:\n",
    "                            TP=TP+1\n",
    "                        else:\n",
    "                             FP=FP+1                \n",
    "                    else:\n",
    "                        if r[i]==1:\n",
    "                            FN=FN+1\n",
    "                        else:\n",
    "                            TN=TN+1    \n",
    "        \n",
    "        print(\"prec reca accuracy for each sens\") \n",
    "        prec= float(TP/(TP+FP))\n",
    "        reca= float(TP/(TP+FN))\n",
    "        accur= float((TP+TN)/a)\n",
    "        print(prec,reca,accur)\n",
    "        \n",
    "        print(\"SVM----------total , accepted, aceeptance rate:\")             \n",
    "        \n",
    "        a1=float(acc1/a)\n",
    "        print(a)\n",
    "        \n",
    "        print(acc1)\n",
    "        print(a1)\n",
    "        ar.append(a1)\n",
    "        \n",
    "    maxi= max(ar)\n",
    "    mini= min(ar)\n",
    "    DP=float(maxi-mini)\n",
    "    print(\"data acceptance rates\")\n",
    "    print(ar)\n",
    "    print(\"data DP\")\n",
    "    print(DP) \n",
    "    \n",
    "    print(\"SVM accuracy--------------------------\")\n",
    "    prec=0\n",
    "    reca=0\n",
    "    accur=0\n",
    "    FP=0\n",
    "    FN=0\n",
    "    TP=0\n",
    "    TN=0\n",
    "    for i in range(n):\n",
    "            if r2[i]==1:\n",
    "                acc1=acc1+1 \n",
    "                if r[i]==1:\n",
    "                    TP=TP+1\n",
    "                else:\n",
    "                     FP=FP+1                \n",
    "            else:\n",
    "                if r[i]==1:\n",
    "                     FN=FN+1\n",
    "                else:\n",
    "                     TN=TN+1    \n",
    "\n",
    "        \n",
    "    prec= float(TP/(TP+FP))\n",
    "    reca= float(TP/(TP+FN))\n",
    "    accur= float((TP+TN)/n)\n",
    "    print(prec,reca,accur)\n",
    "    \n",
    "######################\n",
    "    delta=1\n",
    "\n",
    "    epsilon=[.01]\n",
    "    beta_converge = [.5,.4]\n",
    "    #beta_converge = [.15]\n",
    "    alpha = [0,0.2,0.4,0.6,0.8,1]\n",
    "    \n",
    "    \n",
    "    \n",
    "    fi= np.zeros(n,dtype=int) \n",
    "    t=0\n",
    "  \n",
    "    #gamma = [0.175442,    0.142103, 0.166039,    0.164754,  0.153465,    0.14,  0.104348 ]\n",
    "    for eps in epsilon:\n",
    "        for beta_avg in beta_converge:\n",
    "            print(\"----------------This is for covergence at beta = \",beta_avg, \" ----------------\")\n",
    "            #for a in alpha:\n",
    "            # no weighting basic weighting 1\n",
    "            u1,u2=min_max_lp_all_ng6(data,beta_act,eps,e,beta_avg,a)\n",
    "            #######################Disp_impact#######################  \n",
    "            print(\" beta_avg\",beta_avg)\n",
    "            accu_all=[]\n",
    "            DP_all=[]\n",
    "            precision_all=[]\n",
    "            recall_all=[]\n",
    "            ar_all=[]\n",
    "            acceptance_rate=np.zeros((7,28),dtype=float)\n",
    "            count=0\n",
    "            print(\"<--------------------------------------->\")\n",
    "            t=t+1\n",
    "            print(\"iteration t\",t)\n",
    "    #                 for alpha in np.arange(0,1.05,0.05):\n",
    "    #                     print(\"alpha: \",alpha)\n",
    "    #                     for i in range(n):\n",
    "\n",
    "    #                         z=random()\n",
    "    #                         if z < alpha:\n",
    "    #                                fi[i]= u1[i] \n",
    "\n",
    "    #                         else:\n",
    "    #                                fi[i]= r2[i]\n",
    "\n",
    "            for i in range(n):\n",
    "                 fi[i] = u1[i]\n",
    "\n",
    "\n",
    "            for j in range(s):\n",
    "                print(\"sensitive attribute \",(j+1)) \n",
    "\n",
    "                TP=0\n",
    "                FP=0\n",
    "                FN=0\n",
    "                TN=0\n",
    "                precision=0\n",
    "                recall=0\n",
    "                for i in range(n):\n",
    "                     if data[j][i]== 1 :                        \n",
    "                        if fi[i]==1 and r[i]==1:\n",
    "                            TP=TP+1\n",
    "                        if fi[i]==1 and r[i]==-1:\n",
    "                            FP=FP+1 \n",
    "                        if fi[i]==-1 and r[i]==1:\n",
    "                            FN=FN+1\n",
    "                        if fi[i]==-1 and r[i]==-1:\n",
    "                            TN=TN+1    \n",
    "                if TP+FP !=0:\n",
    "                    precision=float(TP/(TP+FP))\n",
    "                #print(\"precision\",precision)\n",
    "                if TP+FN !=0:    \n",
    "                    recall=float(TP/(TP+FN))\n",
    "               # print(\"recall\",recall)\n",
    "\n",
    "                precision_all.append(precision)\n",
    "                recall_all.append(recall)\n",
    "                #print(\"TP,FP,TN,FN\")\n",
    "                #print(TP,FP,TN,FN)\n",
    "\n",
    "                a=0\n",
    "                b=0\n",
    "                acc1=0\n",
    "                acc2=0\n",
    "                for i in range(n):\n",
    "                        if data[j][i]== 1 :\n",
    "                            a=a+1\n",
    "                            if fi[i]==1:\n",
    "                                 acc1=acc1+1 \n",
    "\n",
    "    #                         print(\"total ,fair accepted, aceeptance rate:\")             \n",
    "                a1=float(acc1/a)\n",
    "\n",
    "\n",
    "\n",
    "    #                         print(a)\n",
    "    #                         print(acc1)\n",
    "    #                         print(a1)\n",
    "                ar_all.append(a1)\n",
    "\n",
    "            count = count+1\n",
    "            maxi=max(ar_all)\n",
    "            mini= min(ar_all)\n",
    "            DP=float(maxi-mini)\n",
    "            print(\"individual acceptance rates\")\n",
    "            print(ar_all)\n",
    "            print(\"individul precision\")\n",
    "            print(precision_all)\n",
    "            print(\"individual recall\")\n",
    "            print(recall_all)\n",
    "            print(\"DP all\")\n",
    "            print(DP)\n",
    "            f_acc=0\n",
    "            for i in range(n):\n",
    "                 if fi[i] == r[i]:\n",
    "                        f_acc=f_acc+1\n",
    "            f_acc_l=float((f_acc*100)/n) \n",
    "\n",
    "    #######################################################################33   \n",
    "\n",
    "    #                         print(\"sensitive attribute \",(j+1)) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            TP=0\n",
    "            FP=0\n",
    "            FN=0\n",
    "            TN=0\n",
    "            precision=0\n",
    "            recall=0\n",
    "            accu=0\n",
    "            for i in range(n):\n",
    "                    if fi[i]==1 and r[i]==1:\n",
    "                        TP=TP+1\n",
    "                    if fi[i]==1 and r[i]==-1:\n",
    "                        FP=FP+1 \n",
    "                    if fi[i]==-1 and r[i]==1:\n",
    "                        FN=FN+1\n",
    "                    if fi[i]==-1 and r[i]==-1:\n",
    "                        TN=TN+1    \n",
    "\n",
    "            if TP+FP!=0:\n",
    "                precision=float(TP/(TP+FP))\n",
    "            print(\"precision all\",precision)\n",
    "            if TP+FN!=0:\n",
    "                recall=float(TP/(TP+FN))\n",
    "\n",
    "\n",
    "            print(\"recall all\",recall)\n",
    "            accu=float((TP+TN)/(TP+FN+TN+FP))\n",
    "\n",
    "\n",
    "            print(\"accuracy all\",accu)\n",
    "\n",
    "\n",
    "\n",
    "            print(\"TP,FP,TN,FN\")\n",
    "            print(TP,FP,TN,FN)\n",
    "    #                         print(\"total ,fair accepted, aceeptance rate:\")             \n",
    "            a1=float(acc1/a)\n",
    "\n",
    "\n",
    "    print(\"<--------------------------------------->\")\n",
    "    alpha_weight=np.arange(0,1.05,.05)        \n",
    "    return accu_all,DP_all,acceptance_rate,alpha_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #without accuracy ---> 2\n",
    "# def main1(datax, y_test, y_test_pred): \n",
    "        \n",
    "#     n=datax.shape[1]\n",
    "#     s=datax.shape[0]    \n",
    "#     data = np.zeros((s, n), dtype = int)\n",
    "    \n",
    "#     r = np.zeros(n, dtype = int) \n",
    "    \n",
    "#     for i in range(n):\n",
    "#         if int(y_test.iloc[i])==1 :\n",
    "#             r[i]=1\n",
    "#         else :\n",
    "#             r[i]= -1  \n",
    "    \n",
    "#     r2 = np.zeros(n, dtype = int) \n",
    "#     for i in range(n):\n",
    "#         if int(y_test_pred[i])==1 :\n",
    "#             r2[i]=1\n",
    "#         else :\n",
    "#             r2[i]= -1          \n",
    "#     ar=[]\n",
    "    \n",
    "#     for j in range(s):\n",
    "#         print(\"sensitive attribute \",(j+1)) \n",
    "#         a=0\n",
    "#         b=0\n",
    "#         acc1=0\n",
    "#         acc2=0\n",
    "#         for i in range(n):\n",
    "#                 data[j][i]= datax.iloc[j,i]\n",
    "#                 if data[j][i]== 1 :\n",
    "#                     a=a+1\n",
    "#                     if r[i]==1:\n",
    "#                          acc1=acc1+1 \n",
    "\n",
    "#         print(\"ACTUAL----------total ,accepted, aceeptance rate:\")             \n",
    "#         a1=float(acc1/a)\n",
    "#         print(a)\n",
    "        \n",
    "#         print(acc1)\n",
    "#         print(a1)\n",
    "#         ar.append(a1)\n",
    "        \n",
    "#     maxi= max(ar)\n",
    "#     mini= min(ar)\n",
    "#     DP=float(maxi-mini)\n",
    "#     print(\"data acceptance rates\")\n",
    "#     print(ar)\n",
    "#     print(\"data DP\")\n",
    "#     print(DP)\n",
    "    \n",
    "#     ar=[]\n",
    "    \n",
    "#     for j in range(s):\n",
    "#         print(\"sensitive attribute \",(j+1)) \n",
    "#         a=0\n",
    "#         b=0\n",
    "#         acc1=0\n",
    "#         acc2=0\n",
    "#         prec=0\n",
    "#         reca=0\n",
    "#         accur=0\n",
    "#         FP=0\n",
    "#         FN=0\n",
    "#         TP=0\n",
    "#         TN=0\n",
    "#         for i in range(n):\n",
    "#              if data[j][i]== 1 :\n",
    "#                     a=a+1\n",
    "#                     if r2[i]==1:\n",
    "#                         acc1=acc1+1 \n",
    "#                         if r[i]==1:\n",
    "#                             TP=TP+1\n",
    "#                         else:\n",
    "#                              FP=FP+1                \n",
    "#                     else:\n",
    "#                         if r[i]==1:\n",
    "#                             FN=FN+1\n",
    "#                         else:\n",
    "#                             TN=TN+1    \n",
    "        \n",
    "#         print(\"prec reca accuracy for each sens\") \n",
    "#         prec= float(TP/(TP+FP))\n",
    "#         reca= float(TP/(TP+FN))\n",
    "#         accur= float((TP+TN)/a)\n",
    "#         print(prec,reca,accur)\n",
    "        \n",
    "#         print(\"SVM----------total , accepted, aceeptance rate:\")             \n",
    "        \n",
    "#         a1=float(acc1/a)\n",
    "#         print(a)\n",
    "        \n",
    "#         print(acc1)\n",
    "#         print(a1)\n",
    "#         ar.append(a1)\n",
    "        \n",
    "#     maxi= max(ar)\n",
    "#     mini= min(ar)\n",
    "#     DP=float(maxi-mini)\n",
    "#     print(\"data acceptance rates\")\n",
    "#     print(ar)\n",
    "#     print(\"data DP\")\n",
    "#     print(DP) \n",
    "    \n",
    "#     print(\"SVM accuracy--------------------------\")\n",
    "#     prec=0\n",
    "#     reca=0\n",
    "#     accur=0\n",
    "#     FP=0\n",
    "#     FN=0\n",
    "#     TP=0\n",
    "#     TN=0\n",
    "#     for i in range(n):\n",
    "#             if r2[i]==1:\n",
    "#                 acc1=acc1+1 \n",
    "#                 if r[i]==1:\n",
    "#                     TP=TP+1\n",
    "#                 else:\n",
    "#                      FP=FP+1                \n",
    "#             else:\n",
    "#                 if r[i]==1:\n",
    "#                      FN=FN+1\n",
    "#                 else:\n",
    "#                      TN=TN+1    \n",
    "\n",
    "        \n",
    "#     prec= float(TP/(TP+FP))\n",
    "#     reca= float(TP/(TP+FN))\n",
    "#     accur= float((TP+TN)/n)\n",
    "#     print(prec,reca,accur)\n",
    "#     print(\"precision recall accuracy\")\n",
    "    \n",
    "# #     delta1=[.70,.75,.80,.85,.90,.95]\n",
    "#     #gamma=.05,.06,.07\n",
    "#     #delta1=[.80,.85,.90,.95]\n",
    "# # (for reproducibility)  \n",
    "\n",
    "# # delta1=[.8], gama=[.1], epsilon=[.05]  \n",
    "# # delta1=[.8], gama=[.15], epsilon=[.01]\n",
    " \n",
    "# #     delta1=np.arange(1,.79,-.01)\n",
    "\n",
    "#     delta=1\n",
    "# #     gama=[.1572]\n",
    "# #     epsilon=[.0090]\n",
    "#     epsilon=[.02,.04,.10,.15,.20,.30,.40,.50,.60]\n",
    "#     gama=[.35]\n",
    "    \n",
    "# #     epsilon=[.02,.04,.10,.15,.20,.30,.40,.50,.60]\n",
    "# #     gama=[.30,.40,.50,.60,.70]\n",
    "# #     epsilon=[.0090]\n",
    "#     fi= np.zeros(n,dtype=int) \n",
    "# #     for delta in delta1:\n",
    "#     for gamma in gama:\n",
    "#         for eps in epsilon:\n",
    "#             u1,u2=min_max_lp_all_ng5(data,gamma,eps,r2,delta) (data1,beta,eps,e,beta_avg,alpha):\n",
    "#             #######################Disp_impact#######################  \n",
    "#             print(\"gamma-epsilon-delta\",gamma,eps,delta)\n",
    "#             accu_all=[]\n",
    "#             DP_all=[]\n",
    "#             precision_all=[]\n",
    "#             recall_all=[]\n",
    "#             acceptance_rate=np.zeros((7,28),dtype=float)\n",
    "#             count=0\n",
    "# #                 for alpha in np.arange(0,1.05,0.05):\n",
    "# #                     print(\"alpha: \",alpha)\n",
    "# #                     for i in range(n):\n",
    "\n",
    "# #                         z=random()\n",
    "# #                         if z < alpha:\n",
    "# #                                fi[i]= u1[i] \n",
    "\n",
    "# #                         else:\n",
    "# #                                fi[i]= r2[i]\n",
    "\n",
    "#             for i in range(n):\n",
    "#                  fi[i] = u1[i]\n",
    "#             ar=[]\n",
    "\n",
    "#             print(\"################################\")\n",
    "            \n",
    "#             for j in range(s):\n",
    "#                 print(\"sensitive attribute \",(j+1)) \n",
    "#                 a=0\n",
    "#                 b=0\n",
    "#                 acc1=0\n",
    "#                 acc2=0\n",
    "#                 prec=0\n",
    "#                 reca=0\n",
    "#                 accur=0\n",
    "#                 FP=0\n",
    "#                 FN=0\n",
    "#                 TP=0\n",
    "#                 TN=0\n",
    "#                 for i in range(n):\n",
    "#                      if data[j][i]== 1 :\n",
    "#                             a=a+1\n",
    "#                             if fi[i]==1:\n",
    "#                                 acc1=acc1+1 \n",
    "#                                 if r[i]==1:\n",
    "#                                     TP=TP+1\n",
    "#                                 else:\n",
    "#                                      FP=FP+1                \n",
    "#                             else:\n",
    "#                                 if r[i]==1:\n",
    "#                                     FN=FN+1\n",
    "#                                 else:\n",
    "#                                     TN=TN+1    \n",
    "\n",
    "#                 print(\"lp ############### prec reca accuracy for each sens\") \n",
    "#                 prec= float(TP/(TP+FP))\n",
    "#                 reca= float(TP/(TP+FN))\n",
    "#                 accur= float((TP+TN)/a)\n",
    "#                 print(prec,reca,accur)\n",
    "\n",
    "#                 print(\"By lp---------total , accepted, aceeptance rate:\")             \n",
    "\n",
    "#                 a1=float(acc1/a)\n",
    "#                 print(a,acc1,a1)\n",
    "#                 ar.append(a1)\n",
    "\n",
    "\n",
    "#             maxi= max(ar)\n",
    "#             mini= min(ar)\n",
    "#             DP=float(maxi-mini)\n",
    "#             print(\"data acceptance rates\")\n",
    "#             print(ar)\n",
    "#             print(\"data DP\")\n",
    "#             print(DP) \n",
    "#             TP=0\n",
    "#             FP=0\n",
    "#             FN=0\n",
    "#             TN=0\n",
    "#             accurr=0\n",
    "#             precision=0\n",
    "#             recall=0\n",
    "#             for i in range(n):\n",
    "#                     if fi[i]==1 and r[i]==1:\n",
    "#                         TP=TP+1\n",
    "#                     if fi[i]==-1 and r[i]==-1:\n",
    "#                         TN=TN+1     \n",
    "#                     if fi[i]==1 and r[i]==-1:\n",
    "#                         FP=FP+1 \n",
    "#                     if fi[i]==-1 and r[i]==1:\n",
    "#                         FN=FN+1\n",
    "#             print(\"total accepted \")            \n",
    "#             precision=float(TP/(TP+FP))\n",
    "#             print(\"finalprecision\",precision)\n",
    "#             recall=float(TP/(TP+FN))\n",
    "#             print(\"finalrecall\",recall)\n",
    "#             accurr=float((TP+TN)/(TP+TN+FP+FN))\n",
    "#             print(\"finalaccuracy\",accurr)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "      \n",
    "#     alpha_weight=np.arange(0,1.05,.05)        \n",
    "#     return accu_all,DP_all,acceptance_rate,alpha_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic weighting 1\n",
    "\n",
    "\n",
    "\n",
    "####################3LP5#######################3\n",
    "\n",
    "\n",
    "\n",
    "#accu_all,DP_all,acceptance_rate,alpha_weight = main(sensitive, Y_test, Y_test_pred,e)\n",
    "#agar2\n",
    "# bilal - am_ind asian black other white female male(actual precision sequence)\n",
    "# 6         5          4          2                1              0                  3\n",
    "\n",
    "# ours -s_male, s_female  r_white, r_black, r_asian-pac-islander','r_amer-indian-eskimo','r_other\n",
    "           \n",
    "# beta=[6         5          4          2                1              0                  3]\n",
    "\n",
    "# beta=[beta[6], beta[5],beta[4],beta[2],beta[1],beta[0],beta[3]]\n",
    "\n",
    "\n",
    "#bilal -female male  am_ind asian  black other white (actual acceptance rate sequence)\n",
    "#          0      1     2       3     4      5    6   \n",
    "# ours -s_male, s_female  r_white, r_black, r_asian-pac-islander','r_amer-indian-eskimo', 'r_other\n",
    "           \n",
    "# beta=[1       0        6          4            3                   2                    5]\n",
    "\n",
    "\n",
    "#NG\n",
    "import time\n",
    "import pulp as p \n",
    "def min_max_lp_all_ng5(data1,beta_act,eps,e,beta_avg,alpha):\n",
    "    import pulp as p \n",
    "    import math\n",
    "    \n",
    "    \n",
    "    m=data1.shape[0]\n",
    "    n=data1.shape[1]\n",
    "    print('dimension of data')\n",
    "    print(m,n)\n",
    "    \n",
    "    ################ sorted result\n",
    "    h = [[] for i in range(m)]\n",
    "    key = [[] for i in range(m)]\n",
    "\n",
    "    h1=[]\n",
    "    h2=[]\n",
    "    h3=[]\n",
    "    h4=[]\n",
    "    h5=[]\n",
    "    h6=[]\n",
    "    h7=[]\n",
    "    key1=[]\n",
    "    key2=[]\n",
    "    key3=[]\n",
    "    key4=[]\n",
    "    key5=[]\n",
    "    key6=[]\n",
    "    key7=[]\n",
    "    cost=np.zeros(n,dtype=int)\n",
    "    data2=np.zeros((m,n),dtype=int)\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            if data1[j][i]==1:           \n",
    "                h[j].append(e[i][1])\n",
    "                key[j].append(i)\n",
    "\n",
    "            \n",
    "    '''    \n",
    "        elif data1[1][i]==1:\n",
    "            h2.append(e[i][1])\n",
    "            key2.append(i)\n",
    "            \n",
    "        elif data1[2][i]==1:\n",
    "            h3.append(e[i][1])\n",
    "            key3.append(i)\n",
    "            \n",
    "        elif data1[3][i]==1:\n",
    "            h4.append(e[i][1])\n",
    "            key4.append(i)\n",
    "        elif data1[4][i]==1:\n",
    "            h5.append(e[i][1])\n",
    "            key5.append(i)\n",
    "        elif data1[5][i]==1:\n",
    "            h6.append(e[i][1])\n",
    "            key6.append(i)\n",
    "        elif data1[6][i]==1:\n",
    "            h7.append(e[i][1])\n",
    "            key7.append(i)\n",
    "      '''      \n",
    "#print(hc)\n",
    "#     print(key1)\n",
    "    for k in range(m):\n",
    "        for i in range(1,len(h[k])):\n",
    "            for j in range(i,0,-1):\n",
    "                var=0\n",
    "                var2=0\n",
    "                if h[k][j-1]<h[k][j]:\n",
    "                    index=j\n",
    "                    var=h[k][j]\n",
    "                    h[k][j]=h[k][j-1]\n",
    "                    h[k][j-1]=var\n",
    "                    var2=key[k][j]\n",
    "                    key[k][j]=key[k][j-1]\n",
    "                    key[k][j-1]=var2\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "    '''\n",
    "    for i in range(1,len(h2)):\n",
    "        for j in range(i,0,-1):\n",
    "            var=0\n",
    "            var2=0\n",
    "            if h2[j-1]<h2[j]:\n",
    "                index=j\n",
    "                var=h2[j]\n",
    "                h2[j]=h2[j-1]\n",
    "                h2[j-1]=var\n",
    "\n",
    "                var2=key2[j]\n",
    "                key2[j]=key2[j-1]\n",
    "                key2[j-1]=var2\n",
    "            else:\n",
    "                break\n",
    "    for i in range(1,len(h3)):\n",
    "        for j in range(i,0,-1):\n",
    "            var=0\n",
    "            var2=0\n",
    "            if h3[j-1]<h3[j]:\n",
    "                index=j\n",
    "                var=h3[j]\n",
    "                h3[j]=h3[j-1]\n",
    "                h3[j-1]=var\n",
    "\n",
    "                var2=key3[j]\n",
    "                key3[j]=key3[j-1]\n",
    "                key3[j-1]=var2\n",
    "            else:\n",
    "                break\n",
    "    for i in range(1,len(h4)):\n",
    "        for j in range(i,0,-1):\n",
    "            var=0\n",
    "            var2=0\n",
    "            if h4[j-1]<h4[j]:\n",
    "                index=j\n",
    "                var=h4[j]\n",
    "                h4[j]=h4[j-1]\n",
    "                h4[j-1]=var\n",
    "\n",
    "                var2=key4[j]\n",
    "                key4[j]=key4[j-1]\n",
    "                key4[j-1]=var2\n",
    "            else:\n",
    "                break\n",
    "    for i in range(1,len(h5)):\n",
    "        for j in range(i,0,-1):\n",
    "            var=0\n",
    "            var2=0\n",
    "            if h3[j-1]<h5[j]:\n",
    "                index=j\n",
    "                var=h5[j]\n",
    "                h5[j]=h5[j-1]\n",
    "                h5[j-1]=var\n",
    "\n",
    "                var2=key5[j]\n",
    "                key5[j]=key5[j-1]\n",
    "                key5[j-1]=var2\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "                \n",
    "                \n",
    "    for i in range(1,len(h6)):\n",
    "        for j in range(i,0,-1):\n",
    "            var=0\n",
    "            var2=0\n",
    "            if h6[j-1]<h6[j]:\n",
    "                index=j\n",
    "                var=h6[j]\n",
    "                h6[j]=h6[j-1]\n",
    "                h6[j-1]=var\n",
    "\n",
    "                var2=key6[j]\n",
    "                key6[j]=key6[j-1]\n",
    "                key6[j-1]=var2\n",
    "            else:\n",
    "                break        \n",
    "                \n",
    "\n",
    "    for i in range(1,len(h7)):\n",
    "        for j in range(i,0,-1):\n",
    "            var=0\n",
    "            var2=0\n",
    "            if h7[j-1]<h7[j]:\n",
    "                index=j\n",
    "                var=h7[j]\n",
    "                h7[j]=h7[j-1]\n",
    "                h7[j-1]=var\n",
    "\n",
    "                var2=key7[j]\n",
    "                key7[j]=key7[j-1]\n",
    "                key7[j-1]=var2\n",
    "            else:\n",
    "                break \n",
    "              \n",
    "    for i in range(m):\n",
    "        for j in range(len(key[i])):    \n",
    "            data2[i][key[i][j]]=j+1                \n",
    "    for j in range(len(key2)):\n",
    "         data2[1][key2[j]]=j+1\n",
    "    for j in range(len(key3)):\n",
    "         data2[2][key3[j]]=j+1\n",
    "    for j in range(len(key4)):\n",
    "         data2[3][key4[j]]=j+1 \n",
    "    for j in range(len(key5)):\n",
    "         data2[4][key5[j]]=j+1\n",
    "    for j in range(len(key6)):\n",
    "         data2[5][key6[j]]=j+1   \n",
    "    for j in range(len(key7)):\n",
    "         data2[6][key7[j]]=j+1\n",
    "    \n",
    "    \n",
    "    '''            \n",
    "    for i in range(m):\n",
    "        for j in range(len(key1)):    \n",
    "            if h[i][j]==h[i][j-1] and j>=1:\n",
    "                data2[i][key1[j]]=data2[i][key1[j-1]]\n",
    "            else:    \n",
    "                data2[i][key1[j]]=j+1\n",
    "     \n",
    "    '''\n",
    "    for j in range(len(key2)):\n",
    "        if h2[j]==h2[j-1] and j>=1:\n",
    "            data2[1][key2[j]]=data2[0][key2[j-1]]\n",
    "        else:    \n",
    "            data2[1][key2[j]]=j+1\n",
    "    for j in range(len(key3)):\n",
    "        if h3[j]==h3[j-1] and j>=1:\n",
    "            data2[2][key3[j]]=data2[2][key3[j-1]]\n",
    "        else:    \n",
    "            data2[2][key3[j]]=j+1\n",
    "    for j in range(len(key4)):\n",
    "        if h4[j]==h4[j-1] and j>=1:\n",
    "            data2[3][key4[j]]=data2[3][key4[j-1]]\n",
    "        else:    \n",
    "            data2[3][key4[j]]=j+1\n",
    "    for j in range(len(key5)):\n",
    "        if h5[j]==h5[j-1] and j>=1:\n",
    "            data2[4][key5[j]]=data2[4][key5[j-1]]\n",
    "        else:    \n",
    "            data2[4][key5[j]]=j+1\n",
    "    for j in range(len(key6)):\n",
    "        if h6[j]==h6[j-1] and j>=1:\n",
    "            data2[5][key6[j]]=data2[5][key6[j-1]]\n",
    "        else:    \n",
    "            data2[5][key6[j]]=j+1\n",
    "    for j in range(len(key7)):\n",
    "        if h7[j]==h7[j-1] and j>=1:\n",
    "            data2[6][key7[j]]=data2[6][key7[j-1]]\n",
    "        else:    \n",
    "            data2[6][key7[j]]=j+1 \n",
    "    '''\n",
    "    \n",
    "    alpha2=[1,1,1,1,1,1,1]\n",
    "    \n",
    "    '''\n",
    "    for j in range(len(key1)):    \n",
    "        #data2[0][key1[j]]=((j+1)/((beta[0]*len(key1))*((beta[0]*len(key1))+1)/2)*alpha[0])\n",
    "         \n",
    "        data2[0][key1[j]]=(j+1)*alpha2[0]\n",
    "    for j in range(len(key2)):\n",
    "        data2[1][key2[j]]=(j+1)*alpha2[1]\n",
    "    for j in range(len(key3)):\n",
    "        if data1[2][key3[j]]==1 and data1[0][key3[j]]==1: \n",
    "            data2[2][key3[j]]=(j+1)*(len(key1)/len(key3))*alpha2[2]\n",
    "        else:\n",
    "            data2[2][key3[j]]=(j+1)*(len(key2)/len(key3))*alpha2[2]                  \n",
    "        \n",
    "    for j in range(len(key4)):\n",
    "        if data1[3][key4[j]]==1 and data1[0][key4[j]]==1:                   \n",
    "            data2[3][key4[j]]=(j+1)*(len(key1)/len(key4))*alpha2[3]\n",
    "        else :                     \n",
    "            data2[3][key4[j]]=(j+1)*(len(key2)/len(key4))*alpha2[3]\n",
    "                             \n",
    "    for j in range(len(key5)):\n",
    "        if data1[4][key5[j]]==1 and data1[0][key5[j]]==1:                  \n",
    "            data2[4][key5[j]]=(j+1)*(len(key1)/len(key5))*alpha2[4]\n",
    "        else:      \n",
    "            data2[4][key5[j]]=(j+1)*(len(key2)/len(key5))*alpha2[4]\n",
    "    for j in range(len(key6)):\n",
    "        if data1[5][key6[j]]==1 and data1[0][key6[j]]==1:                    \n",
    "            data2[5][key6[j]]=(j+1)*(len(key1)/len(key6))*alpha2[5]\n",
    "        else:                    \n",
    "             data2[5][key6[j]]=(j+1)*(len(key2)/len(key6))*alpha2[5]\n",
    "    for j in range(len(key7)):\n",
    "        if data1[5][key7[j]]==1 and data1[0][key7[j]]==1:                    \n",
    "            data2[5][key7[j]]=(j+1)*(len(key1)/len(key7))*alpha2[5]\n",
    "        else:                    \n",
    "             data2[5][key7[j]]=(j+1)*(len(key2)/len(key7))*alpha2[5]            \n",
    "    '''            \n",
    "    \n",
    "    for j in range(n):\n",
    "        summ=0\n",
    "        for i in range(m):\n",
    "       \n",
    "            summ=summ+data2[i][j] \n",
    "        cost[j]=summ\n",
    "        \n",
    "        \n",
    "    ################\n",
    "    \n",
    "    \n",
    "    Lp_prob = p.LpProblem('Problem', p.LpMinimize)  \n",
    "   \n",
    "    \n",
    "#     X=np.zeros(n+1,dtype=p.LpVariable)\n",
    "    X=np.zeros(n+m+1,dtype=p.LpVariable)\n",
    "    Y=np.zeros(m,dtype=p.LpVariable)\n",
    "    \n",
    "    sizes=np.zeros(m,dtype=int)\n",
    "    w=np.zeros(m,dtype=int)\n",
    "#     report_index(index,data1,e):  \n",
    "    max_size_index = 0\n",
    "    max_size=0\n",
    "    for i in range(m):\n",
    "        count=0\n",
    "        for j in range(n):\n",
    "            if data1[i][j]==1:\n",
    "                count=count+1        \n",
    "        if count>max_size:\n",
    "            max_size=count\n",
    "            max_size_index = i\n",
    "        sizes[i]=count\n",
    "    print(sizes)    \n",
    "    #############################33\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ###############################\n",
    "    \n",
    "   \n",
    "    beta_actual = beta_act\n",
    "    #beta_actual = [0.5229226361031518, 0.49236192714453586, 0.7256857855361596, 0.2756892230576441, 0.4561891515994437, 0.520523497917906, 0.5311953352769679, 0.42627737226277373, 0.3021885521885522, 0.6963696369636964]\n",
    "        \n",
    "    \n",
    "    \n",
    "    select_sizes=np.zeros(m,dtype=int)\n",
    "   \n",
    "    size_final=np.zeros(m,dtype=int)\n",
    "\n",
    "    for i in range(m):\n",
    "        var1 = str(n+100+i)\n",
    "        Y[i]=p.LpVariable(var1,lowBound=0,upBound=1,cat='Continuous')\n",
    "    \n",
    "    for i in range(n):\n",
    "        var1=str(i)       \n",
    "        X[i]=p.LpVariable(var1,lowBound=0,upBound=1,cat='Integer')\n",
    "   \n",
    "    X[n]=p.LpVariable(str(n),lowBound=0,upBound=1,cat='Continuous')  \n",
    "#     for i in range(m):\n",
    "#         k=n+i+1\n",
    "#         var1=str(k)     \n",
    "#         alpha=(((sizes[i])*(sizes[i]+1))/2)\n",
    "#         X[i]=p.LpVariable(var1,lowBound=(((beta*sizes[i])*(beta*sizes[i]+1))/2),upBound=alpha,cat='Continuous')\n",
    "    \n",
    "        \n",
    "#     X[n]=  p.LpVariable(\"z1\",lowBound=0)\n",
    "    #X[n+1]=  p.LpVariable(\"z2\",lowBound=0)\n",
    "  \n",
    "\n",
    "    #########objective function#####################\n",
    "    \n",
    "#     Lp_prob += 2*X[n+1]+10*X[n+2]+9*X[n+3]+3*X[n+4]\n",
    "    #alpha=0.8\n",
    "    #beta_avg = 0.10\n",
    "    for i in range(m):\n",
    "        w[i] = (sizes[max_size_index]/sizes[i])\n",
    "            \n",
    "        \n",
    "    \n",
    "    \n",
    "    Lp_prob+= p.lpSum([(X[j])*(p.lpSum([w[i]*data2[i][j] for i in range(m)])) for j in range(n)])  #sum(Z[j]*(sum(W[i]*data2[i][j] for i in range(m))) for j in range(n))\n",
    "    #Lp_prob+=1  \n",
    "    \n",
    "    #Lp_prob += Y[0]*sizes[0] + Y[1]*sizes[1] >= p.lpSum([Y[j]*sizes[j] for j in np.arange(2,6)])\n",
    "    #Lp_prob += Y[0]*sizes[0] + Y[1]*sizes[1] <= p.lpSum([Y[j]*sizes[j] for j in np.arange(2,6)])\n",
    "    ##############constraint#################\n",
    "    for i in range(2*m):\n",
    "        if i<m:\n",
    "\n",
    "            Lp_prob += p.lpSum([(X[j])*(data1[i][j]) for j in range(n)]) >= Y[i]*sizes[i]\n",
    "            Lp_prob += p.lpSum([(X[j])*(data1[i][j]) for j in range(n)]) <= (Y[i]+eps)*sizes[i]\n",
    "    \n",
    "    \n",
    "    for i in range(m):\n",
    "        if beta_actual[i] >= beta_avg:\n",
    "            Lp_prob += Y[i] >= (1-alpha)*beta_actual[i] +alpha*beta_avg\n",
    "            Lp_prob += Y[i] <= beta_actual[i]\n",
    "        else:\n",
    "            Lp_prob += Y[i] >= (1-alpha)*beta_actual[i] + alpha*beta_avg\n",
    "            Lp_prob += Y[i] <= beta_avg    \n",
    "            \n",
    "           # Lp_prob += p.lpSum([(X[j])*(data1[i][j])*(sizes[i]-report_index(j,i,data1,e)+1) for j in range(n)]) <= X[n+i+1]\n",
    "\n",
    "#              Lp_prob += p.lpSum([(X[j])*(data1[i][j])*(sizes[i]-report_index(j,i,data1,e)+1)/1000 for j in range(n)]) >= X[n+i+1]\n",
    "    \n",
    "            \n",
    "#         else:        \n",
    "#             Lp_prob += X[n] >= p.lpSum([-1*2*(X[j]-0.5)*data1[i-m][j] for j in range(n)])   /((sizes[i])*(sizes[i]+1)/2)\n",
    "            #Lp_prob += X[n+1] >= p.lpSum([-1*2*(X[j]-0.5)+r[j] for j in range(n)]) \n",
    "#     Lp_prob += X[n+1] >= p.lpSum([2*(X[j]-0.5)-r[j] for j in range(n)])\n",
    "#     Lp_prob += X[n+1] >= p.lpSum([-1*2*(X[j]-0.5)+r[j] for j in range(n)])       \n",
    "         \n",
    "#     Lp_prob += p.lpSum([2*(X[i]-0.5)*r[i] for i in range(n)]) >= X[n]*n\n",
    "    \n",
    "#     Lp_prob += p.lpSum([2*(X[i]-0.5)*r[i]*(1/pow(2,l1.index(i))) for i in l1]) >= 2*(1-(1/pow(2,math.floor(beta*size_final[0]))))\n",
    "#     Lp_prob += p.lpSum([2*(X[i]-0.5)*r[i]*(1/pow(2,l2.index(i))) for i in l2]) >= 2*(1-(1/pow(2,math.floor(beta*size_final[1]))))\n",
    "#     Lp_prob += p.lpSum([2*(X[i]-0.5)*r[i]*(1/pow(2,l3.index(i))) for i in l3]) >= 2*(1-(1/pow(2,math.floor(beta*size_final[2]))))\n",
    "#     Lp_prob += p.lpSum([2*(X[i]-0.5)*r[i]*(1/pow(2,l4.index(i))) for i in l4]) >= 2*(1-(1/pow(2,math.floor(beta*size_final[3]))))\n",
    "    \n",
    "    #n is the number of elements in sensitive attribute \n",
    "                 \n",
    "#     Lp_prob += X[n] <= 42000\n",
    "    #Lp_prob+= p.lpSum([(X[j])*cost[j] for j in range(n)])>=0\n",
    "    for i in range(m):\n",
    "        print(p.value(Y[i])) \n",
    "    for i in range(m):\n",
    "        print(p.value(X[i]))    \n",
    "    #####################################\n",
    "    status = Lp_prob.solve()   # Solver \n",
    "    print(p.LpStatus[status]) \n",
    "    print(\"objective is:\")        \n",
    "    print(p.value(Lp_prob.objective))\n",
    "    print(\"discripency is:\") \n",
    "    print(p.value(X[n]))\n",
    "    x=np.zeros(n,dtype=float)\n",
    "\n",
    "   # The solution status \n",
    "    Synth1={}\n",
    "    Synth2={}\n",
    "    # # Printing the final solution \n",
    "    for i in range(n):\n",
    "        if(p.value(X[i])==1):\n",
    "            Synth1[i]=1 \n",
    "            Synth2[i]=-1\n",
    "#             if(data1[2][i]==1):\n",
    "#                 print(\"no\")\n",
    "        else:\n",
    "            Synth1[i]=-1\n",
    "            Synth2[i]=1\n",
    "    Synthu1=Synth1  \n",
    "    Synthu2=Synth2  \n",
    "    \n",
    "              \n",
    "    return Synthu1,Synthu2   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic weighting 1\n",
    "\n",
    "\n",
    "\n",
    "####################3LP5#######################3\n",
    "\n",
    "\n",
    "\n",
    "#accu_all,DP_all,acceptance_rate,alpha_weight = main(sensitive, Y_test, Y_test_pred,e)\n",
    "#agar2\n",
    "# bilal - am_ind asian black other white female male(actual precision sequence)\n",
    "# 6         5          4          2                1              0                  3\n",
    "\n",
    "# ours -s_male, s_female  r_white, r_black, r_asian-pac-islander','r_amer-indian-eskimo','r_other\n",
    "           \n",
    "# beta=[6         5          4          2                1              0                  3]\n",
    "\n",
    "# beta=[beta[6], beta[5],beta[4],beta[2],beta[1],beta[0],beta[3]]\n",
    "\n",
    "\n",
    "#bilal -female male  am_ind asian  black other white (actual acceptance rate sequence)\n",
    "#          0      1     2       3     4      5    6   \n",
    "# ours -s_male, s_female  r_white, r_black, r_asian-pac-islander','r_amer-indian-eskimo', 'r_other\n",
    "           \n",
    "# beta=[1       0        6          4            3                   2                    5]\n",
    "\n",
    "\n",
    "#NG\n",
    "import time\n",
    "import pulp as p \n",
    "def min_max_lp_all_ng6(data1,beta_act,eps,e,beta_avg,alpha):\n",
    "    import pulp as p \n",
    "    import math\n",
    "    \n",
    "    \n",
    "    m=data1.shape[0]\n",
    "    n=data1.shape[1]\n",
    "    print('dimension of data')\n",
    "    print(m,n)\n",
    "    \n",
    "    ################ sorted result\n",
    "    h = [[] for i in range(m)]\n",
    "    key = [[] for i in range(m)]\n",
    "\n",
    "    h1=[]\n",
    "    h2=[]\n",
    "    h3=[]\n",
    "    h4=[]\n",
    "    h5=[]\n",
    "    h6=[]\n",
    "    h7=[]\n",
    "    key1=[]\n",
    "    key2=[]\n",
    "    key3=[]\n",
    "    key4=[]\n",
    "    key5=[]\n",
    "    key6=[]\n",
    "    key7=[]\n",
    "    cost=np.zeros(n,dtype=int)\n",
    "    data2=np.zeros((m,n),dtype=int)\n",
    "    \n",
    "    t = 0.5\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            if data1[j][i]==1 and e[i][1]>t:\n",
    "                data2[j][i] = 1\n",
    "            elif data1[j][i]==1 and e[i][1] <= t:\n",
    "                data2[j][i] = 2\n",
    "    \n",
    "    for j in range(n):\n",
    "        summ=0\n",
    "        for i in range(m):\n",
    "            summ=summ+data2[i][j] \n",
    "        cost[j]=summ\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            if data1[j][i]==1:           \n",
    "                h[j].append(e[i][1])\n",
    "                key[j].append(i)\n",
    "\n",
    "            \n",
    "        \n",
    "        elif data1[1][i]==1:\n",
    "            h2.append(e[i][1])\n",
    "            key2.append(i)\n",
    "            \n",
    "        elif data1[2][i]==1:\n",
    "            h3.append(e[i][1])\n",
    "            key3.append(i)\n",
    "            \n",
    "        elif data1[3][i]==1:\n",
    "            h4.append(e[i][1])\n",
    "            key4.append(i)\n",
    "        elif data1[4][i]==1:\n",
    "            h5.append(e[i][1])\n",
    "            key5.append(i)\n",
    "        elif data1[5][i]==1:\n",
    "            h6.append(e[i][1])\n",
    "            key6.append(i)\n",
    "        elif data1[6][i]==1:\n",
    "            h7.append(e[i][1])\n",
    "            key7.append(i)\n",
    "         \n",
    "#print(hc)\n",
    "#     print(key1)\n",
    "    for k in range(m):\n",
    "        for i in range(1,len(h[k])):\n",
    "            for j in range(i,0,-1):\n",
    "                var=0\n",
    "                var2=0\n",
    "                if h[k][j-1]<h[k][j]:\n",
    "                    index=j\n",
    "                    var=h[k][j]\n",
    "                    h[k][j]=h[k][j-1]\n",
    "                    h[k][j-1]=var\n",
    "                    var2=key[k][j]\n",
    "                    key[k][j]=key[k][j-1]\n",
    "                    key[k][j-1]=var2\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "    \n",
    "    for i in range(1,len(h2)):\n",
    "        for j in range(i,0,-1):\n",
    "            var=0\n",
    "            var2=0\n",
    "            if h2[j-1]<h2[j]:\n",
    "                index=j\n",
    "                var=h2[j]\n",
    "                h2[j]=h2[j-1]\n",
    "                h2[j-1]=var\n",
    "\n",
    "                var2=key2[j]\n",
    "                key2[j]=key2[j-1]\n",
    "                key2[j-1]=var2\n",
    "            else:\n",
    "                break\n",
    "    for i in range(1,len(h3)):\n",
    "        for j in range(i,0,-1):\n",
    "            var=0\n",
    "            var2=0\n",
    "            if h3[j-1]<h3[j]:\n",
    "                index=j\n",
    "                var=h3[j]\n",
    "                h3[j]=h3[j-1]\n",
    "                h3[j-1]=var\n",
    "\n",
    "                var2=key3[j]\n",
    "                key3[j]=key3[j-1]\n",
    "                key3[j-1]=var2\n",
    "            else:\n",
    "                break\n",
    "    for i in range(1,len(h4)):\n",
    "        for j in range(i,0,-1):\n",
    "            var=0\n",
    "            var2=0\n",
    "            if h4[j-1]<h4[j]:\n",
    "                index=j\n",
    "                var=h4[j]\n",
    "                h4[j]=h4[j-1]\n",
    "                h4[j-1]=var\n",
    "\n",
    "                var2=key4[j]\n",
    "                key4[j]=key4[j-1]\n",
    "                key4[j-1]=var2\n",
    "            else:\n",
    "                break\n",
    "    for i in range(1,len(h5)):\n",
    "        for j in range(i,0,-1):\n",
    "            var=0\n",
    "            var2=0\n",
    "            if h3[j-1]<h5[j]:\n",
    "                index=j\n",
    "                var=h5[j]\n",
    "                h5[j]=h5[j-1]\n",
    "                h5[j-1]=var\n",
    "\n",
    "                var2=key5[j]\n",
    "                key5[j]=key5[j-1]\n",
    "                key5[j-1]=var2\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "                \n",
    "                \n",
    "    for i in range(1,len(h6)):\n",
    "        for j in range(i,0,-1):\n",
    "            var=0\n",
    "            var2=0\n",
    "            if h6[j-1]<h6[j]:\n",
    "                index=j\n",
    "                var=h6[j]\n",
    "                h6[j]=h6[j-1]\n",
    "                h6[j-1]=var\n",
    "\n",
    "                var2=key6[j]\n",
    "                key6[j]=key6[j-1]\n",
    "                key6[j-1]=var2\n",
    "            else:\n",
    "                break        \n",
    "                \n",
    "\n",
    "    for i in range(1,len(h7)):\n",
    "        for j in range(i,0,-1):\n",
    "            var=0\n",
    "            var2=0\n",
    "            if h7[j-1]<h7[j]:\n",
    "                index=j\n",
    "                var=h7[j]\n",
    "                h7[j]=h7[j-1]\n",
    "                h7[j-1]=var\n",
    "\n",
    "                var2=key7[j]\n",
    "                key7[j]=key7[j-1]\n",
    "                key7[j-1]=var2\n",
    "            else:\n",
    "                break \n",
    "              \n",
    "    for i in range(m):\n",
    "        for j in range(len(key[i])):    \n",
    "            data2[i][key[i][j]]=j+1                \n",
    "    for j in range(len(key2)):\n",
    "         data2[1][key2[j]]=j+1\n",
    "    for j in range(len(key3)):\n",
    "         data2[2][key3[j]]=j+1\n",
    "    for j in range(len(key4)):\n",
    "         data2[3][key4[j]]=j+1 \n",
    "    for j in range(len(key5)):\n",
    "         data2[4][key5[j]]=j+1\n",
    "    for j in range(len(key6)):\n",
    "         data2[5][key6[j]]=j+1   \n",
    "    for j in range(len(key7)):\n",
    "         data2[6][key7[j]]=j+1\n",
    "    \n",
    "    \n",
    "                \n",
    "    for i in range(m):\n",
    "        for j in range(len(key1)):    \n",
    "            if h[i][j]==h[i][j-1] and j>=1:\n",
    "                data2[i][key1[j]]=data2[i][key1[j-1]]\n",
    "            else:    \n",
    "                data2[i][key1[j]]=j+1\n",
    "     \n",
    "\n",
    "    for j in range(len(key2)):\n",
    "        if h2[j]==h2[j-1] and j>=1:\n",
    "            data2[1][key2[j]]=data2[0][key2[j-1]]\n",
    "        else:    \n",
    "            data2[1][key2[j]]=j+1\n",
    "    for j in range(len(key3)):\n",
    "        if h3[j]==h3[j-1] and j>=1:\n",
    "            data2[2][key3[j]]=data2[2][key3[j-1]]\n",
    "        else:    \n",
    "            data2[2][key3[j]]=j+1\n",
    "    for j in range(len(key4)):\n",
    "        if h4[j]==h4[j-1] and j>=1:\n",
    "            data2[3][key4[j]]=data2[3][key4[j-1]]\n",
    "        else:    \n",
    "            data2[3][key4[j]]=j+1\n",
    "    for j in range(len(key5)):\n",
    "        if h5[j]==h5[j-1] and j>=1:\n",
    "            data2[4][key5[j]]=data2[4][key5[j-1]]\n",
    "        else:    \n",
    "            data2[4][key5[j]]=j+1\n",
    "    for j in range(len(key6)):\n",
    "        if h6[j]==h6[j-1] and j>=1:\n",
    "            data2[5][key6[j]]=data2[5][key6[j-1]]\n",
    "        else:    \n",
    "            data2[5][key6[j]]=j+1\n",
    "    for j in range(len(key7)):\n",
    "        if h7[j]==h7[j-1] and j>=1:\n",
    "            data2[6][key7[j]]=data2[6][key7[j-1]]\n",
    "        else:    \n",
    "            data2[6][key7[j]]=j+1 \n",
    "    '''\n",
    "    \n",
    "    alpha2=[1,1,1,1,1,1,1]\n",
    "    \n",
    "    '''\n",
    "    for j in range(len(key1)):    \n",
    "        #data2[0][key1[j]]=((j+1)/((beta[0]*len(key1))*((beta[0]*len(key1))+1)/2)*alpha[0])\n",
    "         \n",
    "        data2[0][key1[j]]=(j+1)*alpha2[0]\n",
    "    for j in range(len(key2)):\n",
    "        data2[1][key2[j]]=(j+1)*alpha2[1]\n",
    "    for j in range(len(key3)):\n",
    "        if data1[2][key3[j]]==1 and data1[0][key3[j]]==1: \n",
    "            data2[2][key3[j]]=(j+1)*(len(key1)/len(key3))*alpha2[2]\n",
    "        else:\n",
    "            data2[2][key3[j]]=(j+1)*(len(key2)/len(key3))*alpha2[2]                  \n",
    "        \n",
    "    for j in range(len(key4)):\n",
    "        if data1[3][key4[j]]==1 and data1[0][key4[j]]==1:                   \n",
    "            data2[3][key4[j]]=(j+1)*(len(key1)/len(key4))*alpha2[3]\n",
    "        else :                     \n",
    "            data2[3][key4[j]]=(j+1)*(len(key2)/len(key4))*alpha2[3]\n",
    "                             \n",
    "    for j in range(len(key5)):\n",
    "        if data1[4][key5[j]]==1 and data1[0][key5[j]]==1:                  \n",
    "            data2[4][key5[j]]=(j+1)*(len(key1)/len(key5))*alpha2[4]\n",
    "        else:      \n",
    "            data2[4][key5[j]]=(j+1)*(len(key2)/len(key5))*alpha2[4]\n",
    "    for j in range(len(key6)):\n",
    "        if data1[5][key6[j]]==1 and data1[0][key6[j]]==1:                    \n",
    "            data2[5][key6[j]]=(j+1)*(len(key1)/len(key6))*alpha2[5]\n",
    "        else:                    \n",
    "             data2[5][key6[j]]=(j+1)*(len(key2)/len(key6))*alpha2[5]\n",
    "    for j in range(len(key7)):\n",
    "        if data1[5][key7[j]]==1 and data1[0][key7[j]]==1:                    \n",
    "            data2[5][key7[j]]=(j+1)*(len(key1)/len(key7))*alpha2[5]\n",
    "        else:                    \n",
    "             data2[5][key7[j]]=(j+1)*(len(key2)/len(key7))*alpha2[5]            \n",
    "    '''            \n",
    "    \n",
    "\n",
    "        \n",
    "        \n",
    "    ################\n",
    "    \n",
    "    \n",
    "    Lp_prob = p.LpProblem('Problem', p.LpMinimize)  \n",
    "   \n",
    "    \n",
    "#     X=np.zeros(n+1,dtype=p.LpVariable)\n",
    "    X=np.zeros(n+m+1,dtype=p.LpVariable)\n",
    "    Y=np.zeros(m,dtype=p.LpVariable)\n",
    "    \n",
    "    sizes=np.zeros(m,dtype=int)\n",
    "    w=np.zeros(m,dtype=int)\n",
    "#     report_index(index,data1,e):  \n",
    "    max_size_index = 0\n",
    "    max_size=0\n",
    "    for i in range(m):\n",
    "        count=0\n",
    "        for j in range(n):\n",
    "            if data1[i][j]==1:\n",
    "                count=count+1        \n",
    "        if count>max_size:\n",
    "            max_size=count\n",
    "            max_size_index = i\n",
    "        sizes[i]=count\n",
    "    print(sizes)    \n",
    "    #############################33\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ###############################\n",
    "    \n",
    "   \n",
    "    #beta_actual = beta_act\n",
    "    #beta_actual = [0.5229226361031518, 0.49236192714453586, 0.7256857855361596, 0.2756892230576441, 0.4561891515994437, 0.520523497917906, 0.5311953352769679, 0.42627737226277373, 0.3021885521885522, 0.6963696369636964]\n",
    "        \n",
    "    \n",
    "    \n",
    "    select_sizes=np.zeros(m,dtype=int)\n",
    "   \n",
    "    size_final=np.zeros(m,dtype=int)\n",
    "\n",
    "    for i in range(m):\n",
    "        var1 = str(n+100+i)\n",
    "        Y[i]=p.LpVariable(var1,lowBound=0,upBound=1,cat='Continuous')\n",
    "    \n",
    "    for i in range(n):\n",
    "        var1=str(i)       \n",
    "        X[i]=p.LpVariable(var1,lowBound=0,upBound=1,cat='Integer')\n",
    "   \n",
    "    X[n]=p.LpVariable(str(n),lowBound=0,upBound=1,cat='Continuous')  \n",
    "#     for i in range(m):\n",
    "#         k=n+i+1\n",
    "#         var1=str(k)     \n",
    "#         alpha=(((sizes[i])*(sizes[i]+1))/2)\n",
    "#         X[i]=p.LpVariable(var1,lowBound=(((beta*sizes[i])*(beta*sizes[i]+1))/2),upBound=alpha,cat='Continuous')\n",
    "    \n",
    "        \n",
    "#     X[n]=  p.LpVariable(\"z1\",lowBound=0)\n",
    "    #X[n+1]=  p.LpVariable(\"z2\",lowBound=0)\n",
    "  \n",
    "\n",
    "    #########objective function#####################\n",
    "    \n",
    "#     Lp_prob += 2*X[n+1]+10*X[n+2]+9*X[n+3]+3*X[n+4]\n",
    "    #alpha=0.8\n",
    "    #beta_avg = 0.1\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(m):\n",
    "        w[i] = (sizes[max_size_index]/sizes[i])\n",
    "            \n",
    "        \n",
    "    \n",
    "    \n",
    "    Lp_prob+= p.lpSum([(X[j])*cost[j] for j in range(n)])  #sum(Z[j]*(sum(W[i]*data2[i][j] for i in range(m))) for j in range(n))\n",
    "    #Lp_prob+=1  \n",
    "    \n",
    "    #Lp_prob += Y[0]*sizes[0] + Y[1]*sizes[1] >= p.lpSum([Y[j]*sizes[j] for j in np.arange(2,6)])\n",
    "    #Lp_prob += Y[0]*sizes[0] + Y[1]*sizes[1] <= p.lpSum([Y[j]*sizes[j] for j in np.arange(2,6)])\n",
    "    ##############constraint#################\n",
    "    for i in range(2*m):\n",
    "        if i<m:\n",
    "\n",
    "            Lp_prob += p.lpSum([(X[j])*(data1[i][j]) for j in range(n)]) >= beta_act[i]*sizes[i]\n",
    "            Lp_prob += p.lpSum([(X[j])*(data1[i][j]) for j in range(n)]) <= (beta_act[i]+eps)*sizes[i]\n",
    "    \n",
    "    '''\n",
    "    for i in range(m):\n",
    "        if beta_actual[i] >= beta_avg:\n",
    "            Lp_prob += Y[i] >= (1-alpha)*beta_actual[i] +alpha*beta_avg\n",
    "            Lp_prob += Y[i] <= beta_actual[i]\n",
    "        else:\n",
    "            Lp_prob += Y[i] >= (1-alpha)*beta_actual[i] + alpha*beta_avg\n",
    "            Lp_prob += Y[i] <= beta_avg    \n",
    "     '''       \n",
    "           # Lp_prob += p.lpSum([(X[j])*(data1[i][j])*(sizes[i]-report_index(j,i,data1,e)+1) for j in range(n)]) <= X[n+i+1]\n",
    "\n",
    "#              Lp_prob += p.lpSum([(X[j])*(data1[i][j])*(sizes[i]-report_index(j,i,data1,e)+1)/1000 for j in range(n)]) >= X[n+i+1]\n",
    "    \n",
    "            \n",
    "#         else:        \n",
    "#             Lp_prob += X[n] >= p.lpSum([-1*2*(X[j]-0.5)*data1[i-m][j] for j in range(n)])   /((sizes[i])*(sizes[i]+1)/2)\n",
    "            #Lp_prob += X[n+1] >= p.lpSum([-1*2*(X[j]-0.5)+r[j] for j in range(n)]) \n",
    "#     Lp_prob += X[n+1] >= p.lpSum([2*(X[j]-0.5)-r[j] for j in range(n)])\n",
    "#     Lp_prob += X[n+1] >= p.lpSum([-1*2*(X[j]-0.5)+r[j] for j in range(n)])       \n",
    "         \n",
    "#     Lp_prob += p.lpSum([2*(X[i]-0.5)*r[i] for i in range(n)]) >= X[n]*n\n",
    "    \n",
    "#     Lp_prob += p.lpSum([2*(X[i]-0.5)*r[i]*(1/pow(2,l1.index(i))) for i in l1]) >= 2*(1-(1/pow(2,math.floor(beta*size_final[0]))))\n",
    "#     Lp_prob += p.lpSum([2*(X[i]-0.5)*r[i]*(1/pow(2,l2.index(i))) for i in l2]) >= 2*(1-(1/pow(2,math.floor(beta*size_final[1]))))\n",
    "#     Lp_prob += p.lpSum([2*(X[i]-0.5)*r[i]*(1/pow(2,l3.index(i))) for i in l3]) >= 2*(1-(1/pow(2,math.floor(beta*size_final[2]))))\n",
    "#     Lp_prob += p.lpSum([2*(X[i]-0.5)*r[i]*(1/pow(2,l4.index(i))) for i in l4]) >= 2*(1-(1/pow(2,math.floor(beta*size_final[3]))))\n",
    "    \n",
    "    #n is the number of elements in sensitive attribute \n",
    "                 \n",
    "#     Lp_prob += X[n] <= 42000\n",
    "    #Lp_prob+= p.lpSum([(X[j])*cost[j] for j in range(n)])>=0\n",
    "    #for i in range(m):\n",
    "    #    print(p.value(Y[i])) \n",
    "    #for i in range(m):\n",
    "    #    print(p.value(X[i]))    \n",
    "    #####################################\n",
    "    status = Lp_prob.solve()   # Solver \n",
    "    print(p.LpStatus[status]) \n",
    "    print(\"objective is:\")        \n",
    "    print(p.value(Lp_prob.objective))\n",
    "    print(\"discripency is:\") \n",
    "    print(p.value(X[n]))\n",
    "    x=np.zeros(n,dtype=float)\n",
    "\n",
    "   # The solution status \n",
    "    Synth1={}\n",
    "    Synth2={}\n",
    "    # # Printing the final solution \n",
    "    for i in range(n):\n",
    "        if(p.value(X[i])==1):\n",
    "            Synth1[i]=1 \n",
    "            Synth2[i]=-1\n",
    "#             if(data1[2][i]==1):\n",
    "#                 print(\"no\")\n",
    "        else:\n",
    "            Synth1[i]=-1\n",
    "            Synth2[i]=1\n",
    "    Synthu1=Synth1  \n",
    "    Synthu2=Synth2  \n",
    "    \n",
    "              \n",
    "    return Synthu1,Synthu2   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NG\n",
    "import time\n",
    "import pulp as p \n",
    "def min_max_lp_all_ng2(data1,gamma,eps,r,delta):\n",
    "    import pulp as p \n",
    "    \n",
    "    m=data1.shape[0]\n",
    "    n=data1.shape[1]\n",
    "    print('dimension of data')\n",
    "    print(m,n)\n",
    "    Lp_prob = p.LpProblem('Problem', p.LpMaximize)  \n",
    "   \n",
    "    \n",
    "#     X=np.zeros(n+1,dtype=p.LpVariable)\n",
    "    X=np.zeros(n+1,dtype=p.LpVariable)\n",
    "    sizes=np.zeros(m,dtype=int)\n",
    "    for i in range(m):\n",
    "        count=0\n",
    "        for j in range(n):\n",
    "            if data1[i][j]==1:\n",
    "                count=count+1               \n",
    "        sizes[i]=count\n",
    "  \n",
    "\n",
    "    for i in range(n):\n",
    "        var1=str(i)       \n",
    "        X[i]=p.LpVariable(var1,lowBound=0,upBound=1,cat='Integer')\n",
    "    \n",
    "    X[n]=p.LpVariable(str(n),lowBound=0,upBound=1,cat='Continuous')   \n",
    "        \n",
    "#     X[n]=  p.LpVariable(\"z1\",lowBound=0)\n",
    "    #X[n+1]=  p.LpVariable(\"z2\",lowBound=0)\n",
    "\n",
    "\n",
    "    #########objective function#####################\n",
    "#     Lp_prob += X[n] \n",
    "            \n",
    "    Lp_prob += X[n]\n",
    "\n",
    "\n",
    "    ##############constraint#################\n",
    "    for i in range(2*m):\n",
    "        if i<m:\n",
    "#             Lp_prob += X[n] >= p.lpSum([2*(X[j]-0.5)*data1[i][j] for j in range(n)])\n",
    "            Lp_prob += p.lpSum([2*(X[j]-0.5)*data1[i][j] for j in range(n)]) >= (2*gamma-1)*sizes[i]\n",
    "            Lp_prob += p.lpSum([2*(X[j]-0.5)*data1[i][j] for j in range(n)]) <= ((2*gamma-1)+eps)*sizes[i]\n",
    "            \n",
    "#         else:        \n",
    "#             Lp_prob += X[n] >= p.lpSum([-1*2*(X[j]-0.5)*data1[i-m][j] for j in range(n)])\n",
    "            #Lp_prob += X[n+1] >= p.lpSum([-1*2*(X[j]-0.5)+r[j] for j in range(n)]) \n",
    "#     Lp_prob += X[n+1] >= p.lpSum([2*(X[j]-0.5)-r[j] for j in range(n)])\n",
    "#     Lp_prob += X[n+1] >= p.lpSum([-1*2*(X[j]-0.5)+r[j] for j in range(n)])       \n",
    "         \n",
    "    Lp_prob += p.lpSum([2*(X[i]-0.5)*r[i] for i in range(n)]) >= X[n]*n\n",
    "    #n is the number of elements in sensitive attribute \n",
    "                 \n",
    "#     Lp_prob += X[n] <= 42000\n",
    "    \n",
    "    #####################################\n",
    "    status = Lp_prob.solve()   # Solver \n",
    "    print(p.LpStatus[status]) \n",
    "    print(\"discripency is:\")        \n",
    "    print(p.value(Lp_prob.objective))\n",
    "    \n",
    "    x=np.zeros(n,dtype=float)\n",
    "\n",
    "   # The solution status \n",
    "    Synth1={}\n",
    "    Synth2={}\n",
    "    # # Printing the final solution \n",
    "    for i in range(n):\n",
    "        if(p.value(X[i])==1):\n",
    "            Synth1[i]=1 \n",
    "            Synth2[i]=-1\n",
    "        else:\n",
    "            Synth1[i]=-1\n",
    "            Synth2[i]=1\n",
    "    Synthu1=Synth1  \n",
    "    Synthu2=Synth2  \n",
    "    \n",
    "              \n",
    "    return Synthu1,Synthu2   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main(datax,rx):\n",
    "     \n",
    "    \n",
    "#     n=datax.shape[1]\n",
    "#     s=datax.shape[0]\n",
    "        \n",
    "#     r = np.zeros(n, dtype = int) \n",
    "#     data = np.zeros((s, n), dtype = int)\n",
    "#     for i in range(n):\n",
    "#         if int(rx.iloc[i,0])==1 :\n",
    "#             r[i]=1\n",
    "#         else :\n",
    "#             r[i]= 0   \n",
    "#     ar=[]\n",
    "    \n",
    "    \n",
    "#     for j in range(s):\n",
    "#         print(\"sensitive attribute \",(j+1)) \n",
    "#         a=0\n",
    "#         b=0\n",
    "#         acc1=0\n",
    "#         acc2=0\n",
    "#         for i in range(n):\n",
    "                \n",
    "#                 data[j][i]= datax.iloc[j,i]\n",
    "#                 if data[j][i]== 1 :\n",
    "#                     a=a+1\n",
    "#                     if r[i]==1:\n",
    "#                          acc1=acc1+1 \n",
    "\n",
    "#         print(\"total ,fair accepted, aceeptance rate:\")             \n",
    "#         a1=float(acc1/a)\n",
    "\n",
    "#         print(a)\n",
    "#         print(acc1)\n",
    "#         print(a1)\n",
    "#         ar.append(a1)\n",
    "        \n",
    "#     maxi=max(ar)\n",
    "#     mini= min(ar)\n",
    "#     DP=float(maxi-mini)\n",
    "#     print(\"data acceptance rates\")\n",
    "#     print(ar)\n",
    "#     print(\"data DP\")\n",
    "#     print(DP)                       \n",
    "                    \n",
    "#     gama=[.4,.50]   \n",
    "#     epsilon=[..001,.0015,.002,]\n",
    "#     #epsilon=[.0003,.0004,.0005,.0006,.0007,.0008,.0009,.001,.0015,.002,.0025]\n",
    "    \n",
    "#     for gamma in gama:\n",
    "#         for eps in epsilon:\n",
    "#             u1,u2=min_max_lp_all(data,gamma,eps,r)\n",
    "#             #######################Disp_impact#######################  \n",
    "#             print(\"gamma-epsilon\",gamma,eps)\n",
    "#             ar=[]\n",
    "#             for j in range(s):\n",
    "#                 print(\"sensitive attribute \",(j+1)) \n",
    "#                 a=0\n",
    "#                 b=0\n",
    "#                 acc1=0\n",
    "#                 acc2=0\n",
    "#                 for i in range(n):\n",
    "#                         if data[j][i]== 1 :\n",
    "#                             a=a+1\n",
    "#                             if u1[i]==1:\n",
    "#                                  acc1=acc1+1 \n",
    "\n",
    "#                 print(\"total ,fair accepted, aceeptance rate:\")             \n",
    "#                 a1=float(acc1/a)\n",
    "                \n",
    "#                 print(a)\n",
    "#                 print(acc1)\n",
    "#                 print(a1)\n",
    "#                 ar.append(a1)\n",
    "             \n",
    "#             maxi=max(ar)\n",
    "#             mini= min(ar)\n",
    "#             DP=float(maxi-mini)\n",
    "#             print(\"acceptance rates\")\n",
    "#             print(ar)\n",
    "#             print(\"DP\")\n",
    "#             print(DP)\n",
    "\n",
    "#     return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#table1 for Adult\n",
    "\n",
    "# import time\n",
    "# import pulp as p \n",
    "def min_max_lp_all(data1,gamma,eps,r):\n",
    "    import time\n",
    "    import pulp as p \n",
    "    m=data1.shape[0]\n",
    "    n=data1.shape[1]\n",
    "    print('dimension of data')\n",
    "    print(m,n)\n",
    "    Lp_prob = p.LpProblem('Problem', p.LpMinimize)  \n",
    "   \n",
    "    \n",
    "    X=np.zeros(n+1,dtype=p.LpVariable)\n",
    "    sizes=np.zeros(m,dtype=int)\n",
    "    for i in range(m):\n",
    "        count=0\n",
    "        for j in range(n):\n",
    "            if data1[i][j]==1:\n",
    "                count=count+1\n",
    "                \n",
    "        sizes[i]=count\n",
    "  \n",
    "\n",
    "    for i in range(n):\n",
    "        var1=str(i)\n",
    "        \n",
    "        X[i]=p.LpVariable(var1,lowBound=0,upBound=1,cat='Integer')\n",
    "       \n",
    "        \n",
    "    X[n] =  p.LpVariable(\"z1\",lowBound=0)\n",
    "\n",
    "\n",
    "    #########objective function#####################\n",
    "    Lp_prob += X[n]\n",
    "\n",
    "\n",
    "    ##############constraint#################\n",
    "    for i in range(2*m):\n",
    "        if i<m:\n",
    "            Lp_prob += X[n] >= p.lpSum([2*(X[j]-0.5)*data1[i][j] for j in range(n)])\n",
    "            Lp_prob += p.lpSum([2*(X[j]-0.5)*data1[i][j] for j in range(n)]) >= (2*gamma-1)*sizes[i]\n",
    "            Lp_prob += p.lpSum([2*(X[j]-0.5)*data1[i][j] for j in range(n)]) <= ((2*gamma-1)+eps)*sizes[i]\n",
    "            \n",
    "        else:        \n",
    "            Lp_prob += X[n] >= p.lpSum([-1*2*(X[j]-0.5)*data1[i-m][j] for j in range(n)])\n",
    "            \n",
    "         \n",
    " \n",
    "    #n is the number of elements in sensitive attribute \n",
    "           \n",
    "       \n",
    "    Lp_prob += X[n] <= 42000\n",
    "    \n",
    "    #####################################\n",
    "    status = Lp_prob.solve()   # Solver \n",
    "    print(p.LpStatus[status]) \n",
    "    print(\"discripency is:\")        \n",
    "    print(p.value(Lp_prob.objective))\n",
    "    x=np.zeros(n,dtype=float)\n",
    "\n",
    "   # The solution status \n",
    "    Synth1={}\n",
    "    Synth2={}\n",
    "    # # Printing the final solution \n",
    "    for i in range(n):\n",
    "        if(p.value(X[i])==1):\n",
    "            Synth1[i]=1 \n",
    "            Synth2[i]=-1\n",
    "        else:\n",
    "            Synth1[i]=-1\n",
    "            Synth2[i]=1\n",
    "    Synthu1=Synth1  \n",
    "    Synthu2=Synth2  \n",
    "    \n",
    "              \n",
    "    return Synthu1,Synthu2   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            x1        x2        x3        x4        x5        x6        x7  \\\n",
      "0     1.392243  1.679704  1.340495  1.169002 -0.169651  1.388412  1.295138   \n",
      "1     0.281930  0.707585  2.675535 -0.119078 -2.308004  1.925658  1.426423   \n",
      "2     0.627546 -1.091224 -1.030816 -2.612045 -0.022696  0.068534 -0.092952   \n",
      "3     1.539515  1.139298  0.632225  0.576092  1.016257  0.141090 -1.288185   \n",
      "4     3.593012  2.077929  2.319466 -1.762514 -3.055532  4.126875  3.522920   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "7995 -2.126879 -3.555727  0.071769 -0.986176  2.558081  2.983460  0.846630   \n",
      "7996  2.754907 -1.817496  4.405390 -1.870844  2.225684  0.876259 -2.341649   \n",
      "7997  0.177864 -1.524149  2.712979  0.013804 -0.611786 -0.381373  1.269916   \n",
      "7998  2.455750  0.216205  4.581337 -3.531081 -1.558274  0.338572 -0.721618   \n",
      "7999  3.218231  1.572063  0.092973 -1.331167  1.232236 -0.501353  0.321379   \n",
      "\n",
      "            x8        x9       x10       x11       x12        x13        x14  \\\n",
      "0    -1.895428 -1.148357  1.614701 -1.049498  2.660067   0.037983  -4.219779   \n",
      "1    -1.184452  1.280644  1.888044 -0.990781  0.864279   6.091637  -8.104712   \n",
      "2    -4.457771 -2.416174  2.009805 -1.455803  1.670899   1.846383  -0.512428   \n",
      "3     2.764486 -0.035432  0.414372 -1.879933  1.767752  -1.977889   0.792234   \n",
      "4    -3.015858  4.153499  0.263969 -0.137930  2.148686   9.614354 -14.961108   \n",
      "...        ...       ...       ...       ...       ...        ...        ...   \n",
      "7995 -1.050211  0.588597  0.740106 -4.282749 -2.794996   3.025729   2.182613   \n",
      "7996  2.896888  1.644242  1.025150 -6.581075 -2.436871   6.117929  -3.159735   \n",
      "7997  1.446300  1.571569  1.309040 -3.710406 -0.375081   5.804239  -4.759974   \n",
      "7998  3.727277  4.616415 -1.762766 -2.002525 -2.463831  10.359781  -8.509465   \n",
      "7999  0.841131  0.745692  3.554162 -5.447069  6.053599   1.035093  -3.759005   \n",
      "\n",
      "           x15        x16        x17       x18       x19       x20  \n",
      "0    -1.514455  -0.292254  -0.073725  0.586313  3.424058 -0.627274  \n",
      "1     2.181631   3.892096  -2.103797  2.268848 -0.261387  2.047578  \n",
      "2    -1.256517  -4.991383  -3.077433 -1.348873  3.291993 -1.381279  \n",
      "3    -0.789747   0.843799   4.119767  0.438513 -0.414191  0.467554  \n",
      "4     8.028063  12.853176 -12.024976  6.678605 -0.840528  3.388474  \n",
      "...        ...        ...        ...       ...       ...       ...  \n",
      "7995  4.820896  -1.844148  -1.334843 -1.502188 -4.716004  5.190138  \n",
      "7996  4.422718   2.333759   5.759854  2.641768 -2.884294  7.506101  \n",
      "7997  2.171444   1.518408   2.995542  1.567182 -1.766561  4.972580  \n",
      "7998  7.235481  12.417189  -0.970669  8.152111 -5.739933  5.964156  \n",
      "7999 -1.677852  -1.063530   3.387187 -1.603658  0.248711  2.940528  \n",
      "\n",
      "[8000 rows x 20 columns]\n",
      "      y\n",
      "0     0\n",
      "1     0\n",
      "2     0\n",
      "3     0\n",
      "4     0\n",
      "...  ..\n",
      "7995  1\n",
      "7996  1\n",
      "7997  1\n",
      "7998  1\n",
      "7999  1\n",
      "\n",
      "[8000 rows x 1 columns]\n",
      "There are 5600 samples in the training set and 2400 samples in the test set\n",
      "y    int64\n",
      "dtype: object\n",
      "y    int64\n",
      "dtype: object\n",
      "y    int64\n",
      "dtype: object\n",
      "y    int64\n",
      "dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/subham/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the SVM classifier on training data is 1.00\n",
      "The accuracy of the SVM classifier on test data is 0.96\n",
      "####Train prediction Label###############################################\n",
      "[[9.99872581e-01 1.27418803e-04]\n",
      " [9.98531918e-01 1.46808186e-03]\n",
      " [9.16411675e-07 9.99999084e-01]\n",
      " ...\n",
      " [3.63959211e-02 9.63604079e-01]\n",
      " [1.99659556e-02 9.80034044e-01]\n",
      " [9.99999900e-01 1.00000010e-07]]\n",
      "[-0.78036573 -0.5887034   0.51103164 ...  0.17968937  0.22813585\n",
      " -1.38292185]\n",
      "            x1        x2        x3        x4        x5        x6        x7  \\\n",
      "0     3.726016  1.413642  2.273420 -0.922466 -1.824458  1.962949  1.624339   \n",
      "1     2.688174 -1.027868 -1.682078 -1.840101  1.103014 -1.896824  0.124650   \n",
      "2     1.023729 -2.973104  0.426373  0.225930 -2.585958  0.902453  2.545473   \n",
      "3    -1.808053  1.236258  3.721401 -0.016646 -2.008842  0.492476  2.205975   \n",
      "4    -0.215031  0.124284  2.116305 -3.074894 -2.958696 -0.016511  2.452041   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "2395  3.315519  0.862938  1.894183 -3.243664 -1.783574  0.940924  1.852839   \n",
      "2396  1.201637  0.780115  0.833288 -2.137964  1.745226 -0.618062 -0.108009   \n",
      "2397  2.498353 -0.012291  0.513396 -1.679410 -1.621630  0.898963  1.233787   \n",
      "2398  5.475439  2.043320  5.369473 -1.754758 -1.682912 -0.778325 -1.984432   \n",
      "2399  0.805834  0.938246  0.770220 -1.652054 -0.580682  0.038329  1.814350   \n",
      "\n",
      "            x8        x9       x10       x11        x12       x13       x14  \\\n",
      "0     0.490037  0.547875 -0.885342 -1.692963   3.168391  4.698117 -7.795435   \n",
      "1    -3.586503 -2.984660  0.327856 -1.335388   2.160897 -1.187367  1.151882   \n",
      "2     3.559631 -1.532219  1.689894 -7.023949   6.414109  4.844646 -2.448145   \n",
      "3     4.457250 -1.697281  2.870452 -6.327482   6.388491  4.590396 -3.818664   \n",
      "4     0.338043  0.124814  1.456617 -3.475118   3.996575  8.407927 -6.913455   \n",
      "...        ...       ...       ...       ...        ...       ...       ...   \n",
      "2395 -2.169884 -0.367307  0.728958 -2.818645   3.968014  6.930969 -8.504277   \n",
      "2396 -0.500619  2.754513  1.420149 -2.168208  -0.302743  2.942150 -3.360247   \n",
      "2397  1.805487 -2.192609  4.713881 -8.342469  10.009026  4.077224 -4.372858   \n",
      "2398  4.449797  0.828297  0.176962 -3.799636   2.542284  6.225360 -8.531017   \n",
      "2399  0.024413 -1.009131  1.479877 -3.514554   4.815701  2.772282 -3.309942   \n",
      "\n",
      "           x15       x16       x17       x18       x19       x20  \n",
      "0     4.017561  9.101517 -4.328087  8.033234  0.870161  0.404177  \n",
      "1    -2.383541 -5.407531 -1.744925 -0.462722  5.848033 -2.127948  \n",
      "2     3.284144  3.037343  1.079736  5.994343 -1.364255  1.522722  \n",
      "3    -2.816748  2.271956  8.413762  4.993741 -3.301473  0.878408  \n",
      "4     1.090160  4.530454 -1.103086  5.525074 -2.242880  1.229985  \n",
      "...        ...       ...       ...       ...       ...       ...  \n",
      "2395  2.225361  5.222216 -4.870262  6.321041  1.874691  0.440226  \n",
      "2396  0.578984  0.117923  0.975259 -2.284575 -2.098053  4.621331  \n",
      "2397 -0.882410  0.009383  2.961416  3.044971  0.356333  0.686769  \n",
      "2398  1.516578  8.493274  5.096081  8.542786  0.947459  2.223054  \n",
      "2399 -1.210632  1.043588  0.651040  2.721024 -0.021531  0.062070  \n",
      "\n",
      "[2400 rows x 20 columns]\n",
      "[0 0 1 ... 1 1 0]\n",
      "      y\n",
      "0     0\n",
      "1     0\n",
      "2     1\n",
      "3     0\n",
      "4     0\n",
      "...  ..\n",
      "2395  0\n",
      "2396  1\n",
      "2397  1\n",
      "2398  1\n",
      "2399  0\n",
      "\n",
      "[2400 rows x 1 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/subham/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "/home/subham/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/subham/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0     1     2     3     4     5     6     7     8     9     ...  2390  \\\n",
      "x1_0.0     0     0     0     1     1     0     0     0     0     0  ...     0   \n",
      "x1_1.0     1     1     1     0     0     1     1     1     1     1  ...     1   \n",
      "x2_0.0     0     1     1     0     0     1     1     1     0     1  ...     1   \n",
      "x2_1.0     1     0     0     1     1     0     0     0     1     0  ...     0   \n",
      "x3_0.0     0     1     0     0     0     0     0     1     0     0  ...     1   \n",
      "x3_1.0     1     0     1     1     1     1     1     0     1     1  ...     0   \n",
      "x4_0.0     1     1     0     1     1     1     0     1     1     1  ...     0   \n",
      "x4_1.0     0     0     1     0     0     0     1     0     0     0  ...     1   \n",
      "x5_0.0     1     0     1     1     1     1     1     0     1     0  ...     0   \n",
      "x5_1.0     0     1     0     0     0     0     0     1     0     1  ...     1   \n",
      "\n",
      "        2391  2392  2393  2394  2395  2396  2397  2398  2399  \n",
      "x1_0.0     0     0     1     0     0     0     0     0     0  \n",
      "x1_1.0     1     1     0     1     1     1     1     1     1  \n",
      "x2_0.0     1     1     1     1     0     0     1     0     0  \n",
      "x2_1.0     0     0     0     0     1     1     0     1     1  \n",
      "x3_0.0     0     0     1     0     0     0     0     0     0  \n",
      "x3_1.0     1     1     0     1     1     1     1     1     1  \n",
      "x4_0.0     1     0     1     1     1     1     1     1     1  \n",
      "x4_1.0     0     1     0     0     0     0     0     0     0  \n",
      "x5_0.0     1     0     0     0     1     0     1     1     1  \n",
      "x5_1.0     0     1     1     1     0     1     0     0     0  \n",
      "\n",
      "[10 rows x 2400 columns]\n",
      "sensitive attribute  1\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "698\n",
      "365\n",
      "0.5229226361031518\n",
      "sensitive attribute  2\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1702\n",
      "838\n",
      "0.49236192714453586\n",
      "sensitive attribute  3\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1203\n",
      "873\n",
      "0.7256857855361596\n",
      "sensitive attribute  4\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1197\n",
      "330\n",
      "0.2756892230576441\n",
      "sensitive attribute  5\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "719\n",
      "328\n",
      "0.4561891515994437\n",
      "sensitive attribute  6\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1681\n",
      "875\n",
      "0.520523497917906\n",
      "sensitive attribute  7\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1715\n",
      "911\n",
      "0.5311953352769679\n",
      "sensitive attribute  8\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "685\n",
      "292\n",
      "0.42627737226277373\n",
      "sensitive attribute  9\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1188\n",
      "359\n",
      "0.3021885521885522\n",
      "sensitive attribute  10\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1212\n",
      "844\n",
      "0.6963696369636964\n",
      "data acceptance rates\n",
      "[0.5229226361031518, 0.49236192714453586, 0.7256857855361596, 0.2756892230576441, 0.4561891515994437, 0.520523497917906, 0.5311953352769679, 0.42627737226277373, 0.3021885521885522, 0.6963696369636964]\n",
      "data DP\n",
      "0.4499965624785155\n",
      "sensitive attribute  1\n",
      "prec reca accuracy for each sens\n",
      "0.9971014492753624 0.9424657534246575 0.9684813753581661\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "698\n",
      "345\n",
      "0.49426934097421205\n",
      "sensitive attribute  2\n",
      "prec reca accuracy for each sens\n",
      "0.9886506935687264 0.9355608591885441 0.9629847238542891\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1702\n",
      "793\n",
      "0.46592244418331374\n",
      "sensitive attribute  3\n",
      "prec reca accuracy for each sens\n",
      "0.9964071856287425 0.9530355097365406 0.9634247714048213\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1203\n",
      "835\n",
      "0.6940980881130507\n",
      "sensitive attribute  4\n",
      "prec reca accuracy for each sens\n",
      "0.976897689768977 0.896969696969697 0.9657477025898078\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1197\n",
      "303\n",
      "0.2531328320802005\n",
      "sensitive attribute  5\n",
      "prec reca accuracy for each sens\n",
      "0.9967948717948718 0.948170731707317 0.9749652294853964\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "719\n",
      "312\n",
      "0.4339360222531293\n",
      "sensitive attribute  6\n",
      "prec reca accuracy for each sens\n",
      "0.9891041162227603 0.9337142857142857 0.9601427721594289\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1681\n",
      "826\n",
      "0.4913741820345033\n",
      "sensitive attribute  7\n",
      "prec reca accuracy for each sens\n",
      "0.9908675799086758 0.9527991218441273 0.970262390670554\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1715\n",
      "876\n",
      "0.5107871720116618\n",
      "sensitive attribute  8\n",
      "prec reca accuracy for each sens\n",
      "0.9923664122137404 0.8904109589041096 0.9503649635036496\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "685\n",
      "262\n",
      "0.38248175182481753\n",
      "sensitive attribute  9\n",
      "prec reca accuracy for each sens\n",
      "0.9783950617283951 0.883008356545961 0.9587542087542088\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1188\n",
      "324\n",
      "0.2727272727272727\n",
      "sensitive attribute  10\n",
      "prec reca accuracy for each sens\n",
      "0.9963144963144963 0.9609004739336493 0.9702970297029703\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1212\n",
      "814\n",
      "0.6716171617161716\n",
      "data acceptance rates\n",
      "[0.49426934097421205, 0.46592244418331374, 0.6940980881130507, 0.2531328320802005, 0.4339360222531293, 0.4913741820345033, 0.5107871720116618, 0.38248175182481753, 0.2727272727272727, 0.6716171617161716]\n",
      "data DP\n",
      "0.44096525603285025\n",
      "SVM accuracy--------------------------\n",
      "0.9912126537785588 0.9376558603491272 0.9645833333333333\n",
      "----------------This is for covergence at beta =  0.5  ----------------\n",
      "dimension of data\n",
      "10 2400\n",
      "[ 698 1702 1203 1197  719 1681 1715  685 1188 1212]\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Optimal\n",
      "objective is:\n",
      "None\n",
      "discripency is:\n",
      "None\n",
      "alpha, beta_avg 0 0.5\n",
      "<--------------------------------------->\n",
      "iteration t 1\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "sensitive attribute  7\n",
      "sensitive attribute  8\n",
      "sensitive attribute  9\n",
      "sensitive attribute  10\n",
      "individual acceptance rates\n",
      "[0.5229226361031518, 0.5023501762632198, 0.7356608478802993, 0.27986633249791143, 0.47983310152990266, 0.520523497917906, 0.5411078717201167, 0.42627737226277373, 0.3063973063973064, 0.7062706270627063]\n",
      "individul precision\n",
      "[0.6958904109589041, 0.7134502923976608, 0.768361581920904, 0.5492537313432836, 0.7739130434782608, 0.6822857142857143, 0.7435344827586207, 0.5958904109589042, 0.3159340659340659, 0.875]\n",
      "individual recall\n",
      "[0.6958904109589041, 0.7279236276849642, 0.7789232531500573, 0.5575757575757576, 0.8140243902439024, 0.6822857142857143, 0.7574094401756312, 0.5958904109589042, 0.3203342618384401, 0.8874407582938388]\n",
      "DP all\n",
      "0.45579451538238785\n",
      "precision all 0.7081967213114754\n",
      "recall all 0.7182044887780549\n",
      "accuracy all 0.7104166666666667\n",
      "TP,FP,TN,FN\n",
      "864 356 841 339\n",
      "dimension of data\n",
      "10 2400\n",
      "[ 698 1702 1203 1197  719 1681 1715  685 1188 1212]\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Optimal\n",
      "objective is:\n",
      "None\n",
      "discripency is:\n",
      "None\n",
      "alpha, beta_avg 0.2 0.5\n",
      "<--------------------------------------->\n",
      "iteration t 2\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "sensitive attribute  7\n",
      "sensitive attribute  8\n",
      "sensitive attribute  9\n",
      "sensitive attribute  10\n",
      "individual acceptance rates\n",
      "[0.5315186246418339, 0.5052878965922444, 0.7024106400665004, 0.32247284878863824, 0.5034770514603616, 0.5169541939321832, 0.5411078717201167, 0.44233576642335765, 0.35269360269360267, 0.66996699669967]\n",
      "individul precision\n",
      "[0.7196765498652291, 0.6011627906976744, 0.7088757396449704, 0.4792746113989637, 0.5193370165745856, 0.6858457997698504, 0.6627155172413793, 0.5577557755775577, 0.4343675417661098, 0.7413793103448276]\n",
      "individual recall\n",
      "[0.7315068493150685, 0.616945107398568, 0.6861397479954181, 0.5606060606060606, 0.573170731707317, 0.6811428571428572, 0.6750823271130626, 0.5787671232876712, 0.5069637883008357, 0.7132701421800948]\n",
      "DP all\n",
      "0.3799377912778622\n",
      "precision all 0.6368805848903331\n",
      "recall all 0.6517040731504572\n",
      "accuracy all 0.6391666666666667\n",
      "TP,FP,TN,FN\n",
      "784 447 750 419\n",
      "dimension of data\n",
      "10 2400\n",
      "[ 698 1702 1203 1197  719 1681 1715  685 1188 1212]\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Optimal\n",
      "objective is:\n",
      "None\n",
      "discripency is:\n",
      "None\n",
      "alpha, beta_avg 0.4 0.5\n",
      "<--------------------------------------->\n",
      "iteration t 3\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "sensitive attribute  7\n",
      "sensitive attribute  8\n",
      "sensitive attribute  9\n",
      "sensitive attribute  10\n",
      "individual acceptance rates\n",
      "[0.5315186246418339, 0.5017626321974148, 0.6359102244389028, 0.3842940685045948, 0.47426981919332406, 0.5258774538964902, 0.5189504373177842, 0.48905109489051096, 0.3813131313131313, 0.636963696369637]\n",
      "individul precision\n",
      "[0.628032345013477, 0.5995316159250585, 0.7241830065359477, 0.4152173913043478, 0.49560117302052786, 0.6515837104072398, 0.6696629213483146, 0.44477611940298506, 0.32891832229580575, 0.772020725388601]\n",
      "individual recall\n",
      "[0.6383561643835617, 0.6109785202863962, 0.6345933562428407, 0.5787878787878787, 0.5152439024390244, 0.6582857142857143, 0.6542261251372119, 0.5102739726027398, 0.415041782729805, 0.7061611374407583]\n",
      "DP all\n",
      "0.2556505650565057\n",
      "precision all 0.6081632653061224\n",
      "recall all 0.6192851205320034\n",
      "accuracy all 0.6091666666666666\n",
      "TP,FP,TN,FN\n",
      "745 480 717 458\n",
      "dimension of data\n",
      "10 2400\n",
      "[ 698 1702 1203 1197  719 1681 1715  685 1188 1212]\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Optimal\n",
      "objective is:\n",
      "None\n",
      "discripency is:\n",
      "None\n",
      "alpha, beta_avg 0.6 0.5\n",
      "<--------------------------------------->\n",
      "iteration t 4\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "sensitive attribute  7\n",
      "sensitive attribute  8\n",
      "sensitive attribute  9\n",
      "sensitive attribute  10\n",
      "individual acceptance rates\n",
      "[0.5315186246418339, 0.5099882491186839, 0.6209476309226932, 0.41102756892230574, 0.48400556328233657, 0.5300416418798334, 0.5189504373177842, 0.5094890510948905, 0.4208754208754209, 0.6097359735973598]\n",
      "individul precision\n",
      "[0.6495956873315364, 0.6002304147465438, 0.7456492637215528, 0.4166666666666667, 0.5344827586206896, 0.6464646464646465, 0.6707865168539325, 0.47277936962750716, 0.35, 0.7943166441136671]\n",
      "individual recall\n",
      "[0.6602739726027397, 0.6217183770883055, 0.6380297823596792, 0.6212121212121212, 0.5670731707317073, 0.6582857142857143, 0.6553238199780461, 0.565068493150685, 0.48746518105849584, 0.6954976303317536]\n",
      "DP all\n",
      "0.2099200620003875\n",
      "precision all 0.6150121065375302\n",
      "recall all 0.6334164588528678\n",
      "accuracy all 0.6175\n",
      "TP,FP,TN,FN\n",
      "762 477 720 441\n",
      "dimension of data\n",
      "10 2400\n",
      "[ 698 1702 1203 1197  719 1681 1715  685 1188 1212]\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Optimal\n",
      "objective is:\n",
      "None\n",
      "discripency is:\n",
      "None\n",
      "alpha, beta_avg 0.8 0.5\n",
      "<--------------------------------------->\n",
      "iteration t 5\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "sensitive attribute  7\n",
      "sensitive attribute  8\n",
      "sensitive attribute  9\n",
      "sensitive attribute  10\n",
      "individual acceptance rates\n",
      "[0.5315186246418339, 0.5099882491186839, 0.5461346633416458, 0.48621553884711777, 0.49235048678720444, 0.5264723378941106, 0.5253644314868805, 0.49343065693430654, 0.49242424242424243, 0.5396039603960396]\n",
      "individul precision\n",
      "[0.49326145552560646, 0.5069124423963134, 0.7275494672754946, 0.24914089347079038, 0.4378531073446328, 0.5288135593220339, 0.5904550499445061, 0.2692307692307692, 0.22393162393162394, 0.7522935779816514]\n",
      "individual recall\n",
      "[0.5013698630136987, 0.5250596658711217, 0.5475372279495991, 0.4393939393939394, 0.4725609756097561, 0.5348571428571428, 0.58397365532382, 0.3116438356164384, 0.3649025069637883, 0.5829383886255924]\n",
      "DP all\n",
      "0.059919124494528075\n",
      "precision all 0.5028248587570622\n",
      "recall all 0.5178719866999169\n",
      "accuracy all 0.5016666666666667\n",
      "TP,FP,TN,FN\n",
      "623 616 581 580\n",
      "dimension of data\n",
      "10 2400\n",
      "[ 698 1702 1203 1197  719 1681 1715  685 1188 1212]\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Optimal\n",
      "objective is:\n",
      "None\n",
      "discripency is:\n",
      "None\n",
      "alpha, beta_avg 1 0.5\n",
      "<--------------------------------------->\n",
      "iteration t 6\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "sensitive attribute  7\n",
      "sensitive attribute  8\n",
      "sensitive attribute  9\n",
      "sensitive attribute  10\n",
      "individual acceptance rates\n",
      "[0.5, 0.5094007050528789, 0.5037406483790524, 0.5096073517126148, 0.5090403337969402, 0.5056513979773944, 0.5055393586005831, 0.5094890510948905, 0.5092592592592593, 0.5041254125412541]\n",
      "individul precision\n",
      "[0.5702005730659025, 0.46482122260668973, 0.8217821782178217, 0.17049180327868851, 0.273224043715847, 0.5905882352941176, 0.5397923875432526, 0.3839541547277937, 0.2809917355371901, 0.707037643207856]\n",
      "individual recall\n",
      "[0.5452054794520548, 0.4809069212410501, 0.570446735395189, 0.3151515151515151, 0.3048780487804878, 0.5737142857142857, 0.5137211855104281, 0.4589041095890411, 0.4735376044568245, 0.5118483412322274]\n",
      "DP all\n",
      "0.009607351712614842\n",
      "precision all 0.49506578947368424\n",
      "recall all 0.5004156275976724\n",
      "accuracy all 0.49375\n",
      "TP,FP,TN,FN\n",
      "602 614 583 601\n",
      "----------------This is for covergence at beta =  0.4  ----------------\n",
      "dimension of data\n",
      "10 2400\n",
      "[ 698 1702 1203 1197  719 1681 1715  685 1188 1212]\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Optimal\n",
      "objective is:\n",
      "None\n",
      "discripency is:\n",
      "None\n",
      "alpha, beta_avg 0 0.4\n",
      "<--------------------------------------->\n",
      "iteration t 7\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "sensitive attribute  7\n",
      "sensitive attribute  8\n",
      "sensitive attribute  9\n",
      "sensitive attribute  10\n",
      "individual acceptance rates\n",
      "[0.5315186246418339, 0.5023501762632198, 0.7256857855361596, 0.29490392648287383, 0.4659248956884562, 0.5300416418798334, 0.5411078717201167, 0.43503649635036495, 0.32154882154882153, 0.6963696369636964]\n",
      "individul precision\n",
      "[0.7439353099730458, 0.6280701754385964, 0.7033218785796106, 0.5637393767705382, 0.764179104477612, 0.6251402918069585, 0.6928879310344828, 0.5704697986577181, 0.3481675392670157, 0.8056872037914692]\n",
      "individual recall\n",
      "[0.7561643835616438, 0.6408114558472554, 0.7033218785796106, 0.603030303030303, 0.7804878048780488, 0.6365714285714286, 0.7058177826564215, 0.5821917808219178, 0.37047353760445684, 0.8056872037914692]\n",
      "DP all\n",
      "0.43078185905328575\n",
      "precision all 0.6631321370309952\n",
      "recall all 0.6758104738154613\n",
      "accuracy all 0.6654166666666667\n",
      "TP,FP,TN,FN\n",
      "813 413 784 390\n",
      "dimension of data\n",
      "10 2400\n",
      "[ 698 1702 1203 1197  719 1681 1715  685 1188 1212]\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Optimal\n",
      "objective is:\n",
      "None\n",
      "discripency is:\n",
      "None\n",
      "alpha, beta_avg 0.2 0.4\n",
      "<--------------------------------------->\n",
      "iteration t 8\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "sensitive attribute  7\n",
      "sensitive attribute  8\n",
      "sensitive attribute  9\n",
      "sensitive attribute  10\n",
      "individual acceptance rates\n",
      "[0.5100286532951289, 0.5023501762632198, 0.7032418952618454, 0.30492898913951544, 0.44506258692628653, 0.5300416418798334, 0.5323615160349854, 0.43503649635036495, 0.35942760942760943, 0.6468646864686468]\n",
      "individul precision\n",
      "[0.7668539325842697, 0.6421052631578947, 0.7434988179669031, 0.5287671232876713, 0.590625, 0.7104377104377104, 0.7152245345016429, 0.5671140939597316, 0.4566744730679157, 0.7997448979591837]\n",
      "individual recall\n",
      "[0.7479452054794521, 0.6551312649164678, 0.720504009163803, 0.5848484848484848, 0.5762195121951219, 0.7234285714285714, 0.716794731064764, 0.5787671232876712, 0.5431754874651811, 0.7428909952606635]\n",
      "DP all\n",
      "0.39831290612233\n",
      "precision all 0.6787778695293146\n",
      "recall all 0.683291770573566\n",
      "accuracy all 0.6791666666666667\n",
      "TP,FP,TN,FN\n",
      "822 389 808 381\n",
      "dimension of data\n",
      "10 2400\n",
      "[ 698 1702 1203 1197  719 1681 1715  685 1188 1212]\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Optimal\n",
      "objective is:\n",
      "None\n",
      "discripency is:\n",
      "None\n",
      "alpha, beta_avg 0.4 0.4\n",
      "<--------------------------------------->\n",
      "iteration t 9\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "sensitive attribute  7\n",
      "sensitive attribute  8\n",
      "sensitive attribute  9\n",
      "sensitive attribute  10\n",
      "individual acceptance rates\n",
      "[0.47421203438395415, 0.5017626321974148, 0.6591853699085619, 0.32748538011695905, 0.43671766342141866, 0.5181439619274242, 0.5172011661807581, 0.43503649635036495, 0.4074074074074074, 0.5783828382838284]\n",
      "individul precision\n",
      "[0.6314199395770392, 0.5281030444964872, 0.7452711223203027, 0.1760204081632653, 0.6273885350318471, 0.5315729047072331, 0.5772266065388951, 0.4966442953020134, 0.29958677685950413, 0.7346647646219686]\n",
      "individual recall\n",
      "[0.5726027397260274, 0.5381861575178998, 0.6769759450171822, 0.20909090909090908, 0.600609756097561, 0.5291428571428571, 0.562019758507135, 0.5068493150684932, 0.403899721448468, 0.6101895734597157]\n",
      "DP all\n",
      "0.33169998979160287\n",
      "precision all 0.5569620253164557\n",
      "recall all 0.5486284289276808\n",
      "accuracy all 0.555\n",
      "TP,FP,TN,FN\n",
      "660 525 672 543\n",
      "dimension of data\n",
      "10 2400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 698 1702 1203 1197  719 1681 1715  685 1188 1212]\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Optimal\n",
      "objective is:\n",
      "None\n",
      "discripency is:\n",
      "None\n",
      "alpha, beta_avg 0.6 0.4\n",
      "<--------------------------------------->\n",
      "iteration t 10\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "sensitive attribute  7\n",
      "sensitive attribute  8\n",
      "sensitive attribute  9\n",
      "sensitive attribute  10\n",
      "individual acceptance rates\n",
      "[0.5315186246418339, 0.44477085781433606, 0.5303408146300914, 0.4093567251461988, 0.4422809457579972, 0.4818560380725758, 0.4897959183673469, 0.42043795620437957, 0.39646464646464646, 0.5420792079207921]\n",
      "individul precision\n",
      "[0.4123989218328841, 0.7384412153236459, 0.7899686520376176, 0.42448979591836733, 0.60062893081761, 0.6432098765432098, 0.7083333333333334, 0.40625, 0.34182590233545646, 0.8386605783866058]\n",
      "individual recall\n",
      "[0.4191780821917808, 0.6670644391408115, 0.5773195876288659, 0.6303030303030303, 0.5823170731707317, 0.5954285714285714, 0.6531284302963776, 0.4006849315068493, 0.44846796657381616, 0.6528436018957346]\n",
      "DP all\n",
      "0.14561456145614565\n",
      "precision all 0.6312056737588653\n",
      "recall all 0.5918536990856192\n",
      "accuracy all 0.6220833333333333\n",
      "TP,FP,TN,FN\n",
      "712 416 781 491\n",
      "dimension of data\n",
      "10 2400\n",
      "[ 698 1702 1203 1197  719 1681 1715  685 1188 1212]\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Optimal\n",
      "objective is:\n",
      "None\n",
      "discripency is:\n",
      "None\n",
      "alpha, beta_avg 0.8 0.4\n",
      "<--------------------------------------->\n",
      "iteration t 11\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "sensitive attribute  7\n",
      "sensitive attribute  8\n",
      "sensitive attribute  9\n",
      "sensitive attribute  10\n",
      "individual acceptance rates\n",
      "[0.42550143266475643, 0.4300822561692127, 0.47547797173732337, 0.3817878028404344, 0.43949930458970793, 0.42415229030339086, 0.4262390670553936, 0.43503649635036495, 0.39225589225589225, 0.4645214521452145]\n",
      "individul precision\n",
      "[0.43434343434343436, 0.546448087431694, 0.6013986013986014, 0.4048140043763676, 0.4873417721518987, 0.5259467040673211, 0.5567715458276333, 0.40939597315436244, 0.22317596566523606, 0.7548845470692718]\n",
      "individual recall\n",
      "[0.35342465753424657, 0.477326968973747, 0.3940435280641466, 0.5606060606060606, 0.4695121951219512, 0.42857142857142855, 0.446761800219539, 0.4178082191780822, 0.28969359331476324, 0.5035545023696683]\n",
      "DP all\n",
      "0.09369016889688897\n",
      "precision all 0.5140913508260447\n",
      "recall all 0.4397339983374896\n",
      "accuracy all 0.5108333333333334\n",
      "TP,FP,TN,FN\n",
      "529 500 697 674\n",
      "dimension of data\n",
      "10 2400\n",
      "[ 698 1702 1203 1197  719 1681 1715  685 1188 1212]\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Optimal\n",
      "objective is:\n",
      "None\n",
      "discripency is:\n",
      "None\n",
      "alpha, beta_avg 1 0.4\n",
      "<--------------------------------------->\n",
      "iteration t 12\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "sensitive attribute  7\n",
      "sensitive attribute  8\n",
      "sensitive attribute  9\n",
      "sensitive attribute  10\n",
      "individual acceptance rates\n",
      "[0.40257879656160456, 0.41304347826086957, 0.4197838736492103, 0.40016708437761067, 0.4019471488178025, 0.4134443783462225, 0.4, 0.43503649635036495, 0.4006734006734007, 0.41914191419141916]\n",
      "individul precision\n",
      "[0.5907473309608541, 0.5305832147937412, 0.9128712871287129, 0.162839248434238, 0.5432525951557093, 0.5496402877697841, 0.5728862973760933, 0.4899328859060403, 0.32983193277310924, 0.7519685039370079]\n",
      "individual recall\n",
      "[0.4547945205479452, 0.4451073985680191, 0.5280641466208477, 0.23636363636363636, 0.47865853658536583, 0.43657142857142855, 0.4313940724478595, 0.5, 0.4373259052924791, 0.45260663507109006]\n",
      "DP all\n",
      "0.03503649635036493\n",
      "precision all 0.5477642276422764\n",
      "recall all 0.4480465502909393\n",
      "accuracy all 0.5379166666666667\n",
      "TP,FP,TN,FN\n",
      "539 445 752 664\n",
      "<--------------------------------------->\n"
     ]
    }
   ],
   "source": [
    "from random import seed, shuffle\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.datasets import make_classification\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "data3 = make_classification(n_samples=8000, n_features=20, n_informative=10, n_redundant=10, n_repeated=0, \n",
    "                            n_classes=2, n_clusters_per_class=1, weights=None, flip_y=0.00, class_sep=1.0, \n",
    "                            hypercube=True, shift=0.0, scale=1.0, shuffle=False, random_state=11)\n",
    "df3 = pd.DataFrame(data3[0],columns=['x'+str(i) for i in range(1,21)])\n",
    "df3['y'] = data3[1]\n",
    "# print(df3.head())\n",
    "\n",
    "data=df3.drop(columns=['y'])\n",
    "print(data)\n",
    "r=df3[['y']]\n",
    "print(r)\n",
    "\n",
    "\n",
    "\n",
    "X_test,Y_test,Y_test_pred,e = synthetic_svm(data , r)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "# Y_test_pred.reset_index()\n",
    "Y_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(X_test)\n",
    "print(Y_test_pred)\n",
    "print(Y_test)\n",
    "# set_k=2\n",
    "p=5\n",
    "for set_k in np.arange(p,6,2):\n",
    "#for set_k in np.arange(2,11,2): \n",
    "    \n",
    "    sens=X_test[['x'+str(k) for k in range(1,set_k+1)]]\n",
    "#     print(set_k)\n",
    "#     print(sens)\n",
    "\n",
    "\n",
    "    p=sens.shape[0]\n",
    "    q=set_k\n",
    "    for i in range(0,p): \n",
    "        for j in range(0,set_k):  \n",
    "            if sens.iloc[i,j] > 0 :\n",
    "                       sens.iloc[i,j] = 1 \n",
    "            else: \n",
    "                       sens.iloc[i,j] = 0 \n",
    "#     print(sens)\n",
    "\n",
    "    sens1 = pd.get_dummies(sens, columns=['x'+str(k) for k in range(1,set_k+1)])\n",
    "    sensitive=sens1.T\n",
    "\n",
    "    print(sensitive)\n",
    "\n",
    "    # print(r.value_counts())\n",
    "    accu_all,DP_all,acceptance_rate,alpha_weight=main(sensitive, Y_test, Y_test_pred,e)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/subham/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "/home/subham/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/subham/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400 10\n",
      "         0     1     2     3     4     5     6     7     8     9     ...  \\\n",
      "x1_0.0      0     0     0     1     1     0     0     0     0     0  ...   \n",
      "x1_1.0      1     1     1     0     0     1     1     1     1     1  ...   \n",
      "x2_0.0      0     1     1     0     0     1     1     1     0     1  ...   \n",
      "x2_1.0      1     0     0     1     1     0     0     0     1     0  ...   \n",
      "x3_0.0      0     1     0     0     0     0     0     1     0     0  ...   \n",
      "x3_1.0      1     0     1     1     1     1     1     0     1     1  ...   \n",
      "x4_0.0      1     1     0     1     1     1     0     1     1     1  ...   \n",
      "x4_1.0      0     0     1     0     0     0     1     0     0     0  ...   \n",
      "x5_0.0      1     0     1     1     1     1     1     0     1     0  ...   \n",
      "x5_1.0      0     1     0     0     0     0     0     1     0     1  ...   \n",
      "x6_0.0      0     1     0     0     1     0     0     1     0     0  ...   \n",
      "x6_1.0      1     0     1     1     0     1     1     0     1     1  ...   \n",
      "x7_0.0      0     0     0     0     0     0     1     0     0     0  ...   \n",
      "x7_1.0      1     1     1     1     1     1     0     1     1     1  ...   \n",
      "x8_0.0      0     1     0     0     0     1     0     1     0     0  ...   \n",
      "x8_1.0      1     0     1     1     1     0     1     0     1     1  ...   \n",
      "x9_0.0      0     1     1     1     0     0     1     1     0     0  ...   \n",
      "x9_1.0      1     0     0     0     1     1     0     0     1     1  ...   \n",
      "x10_0.0     1     0     0     0     0     0     1     0     0     0  ...   \n",
      "x10_1.0     0     1     1     1     1     1     0     1     1     1  ...   \n",
      "\n",
      "         2390  2391  2392  2393  2394  2395  2396  2397  2398  2399  \n",
      "x1_0.0      0     0     0     1     0     0     0     0     0     0  \n",
      "x1_1.0      1     1     1     0     1     1     1     1     1     1  \n",
      "x2_0.0      1     1     1     1     1     0     0     1     0     0  \n",
      "x2_1.0      0     0     0     0     0     1     1     0     1     1  \n",
      "x3_0.0      1     0     0     1     0     0     0     0     0     0  \n",
      "x3_1.0      0     1     1     0     1     1     1     1     1     1  \n",
      "x4_0.0      0     1     0     1     1     1     1     1     1     1  \n",
      "x4_1.0      1     0     1     0     0     0     0     0     0     0  \n",
      "x5_0.0      0     1     0     0     0     1     0     1     1     1  \n",
      "x5_1.0      1     0     1     1     1     0     1     0     0     0  \n",
      "x6_0.0      0     0     0     0     0     0     1     0     1     0  \n",
      "x6_1.0      1     1     1     1     1     1     0     1     0     1  \n",
      "x7_0.0      0     0     0     0     0     0     1     0     1     0  \n",
      "x7_1.0      1     1     1     1     1     1     0     1     0     1  \n",
      "x8_0.0      0     1     0     1     0     1     1     0     0     0  \n",
      "x8_1.0      1     0     1     0     1     0     0     1     1     1  \n",
      "x9_0.0      0     0     0     0     0     1     0     1     0     1  \n",
      "x9_1.0      1     1     1     1     1     0     1     0     1     0  \n",
      "x10_0.0     0     0     1     0     1     0     0     0     0     0  \n",
      "x10_1.0     1     1     0     1     0     1     1     1     1     1  \n",
      "\n",
      "[20 rows x 2400 columns]\n",
      "sensitive attribute  1\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "698\n",
      "365\n",
      "0.5229226361031518\n",
      "sensitive attribute  2\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1702\n",
      "838\n",
      "0.49236192714453586\n",
      "sensitive attribute  3\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1203\n",
      "873\n",
      "0.7256857855361596\n",
      "sensitive attribute  4\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1197\n",
      "330\n",
      "0.2756892230576441\n",
      "sensitive attribute  5\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "719\n",
      "328\n",
      "0.4561891515994437\n",
      "sensitive attribute  6\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1681\n",
      "875\n",
      "0.520523497917906\n",
      "sensitive attribute  7\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1715\n",
      "911\n",
      "0.5311953352769679\n",
      "sensitive attribute  8\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "685\n",
      "292\n",
      "0.42627737226277373\n",
      "sensitive attribute  9\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1188\n",
      "359\n",
      "0.3021885521885522\n",
      "sensitive attribute  10\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1212\n",
      "844\n",
      "0.6963696369636964\n",
      "sensitive attribute  11\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "592\n",
      "282\n",
      "0.47635135135135137\n",
      "sensitive attribute  12\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1808\n",
      "921\n",
      "0.5094026548672567\n",
      "sensitive attribute  13\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "668\n",
      "357\n",
      "0.5344311377245509\n",
      "sensitive attribute  14\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1732\n",
      "846\n",
      "0.4884526558891455\n",
      "sensitive attribute  15\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1155\n",
      "314\n",
      "0.27186147186147186\n",
      "sensitive attribute  16\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1245\n",
      "889\n",
      "0.7140562248995984\n",
      "sensitive attribute  17\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1141\n",
      "325\n",
      "0.2848378615249781\n",
      "sensitive attribute  18\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1259\n",
      "878\n",
      "0.6973788721207307\n",
      "sensitive attribute  19\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "667\n",
      "387\n",
      "0.5802098950524738\n",
      "sensitive attribute  20\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1733\n",
      "816\n",
      "0.4708597807270629\n",
      "data acceptance rates\n",
      "[0.5229226361031518, 0.49236192714453586, 0.7256857855361596, 0.2756892230576441, 0.4561891515994437, 0.520523497917906, 0.5311953352769679, 0.42627737226277373, 0.3021885521885522, 0.6963696369636964, 0.47635135135135137, 0.5094026548672567, 0.5344311377245509, 0.4884526558891455, 0.27186147186147186, 0.7140562248995984, 0.2848378615249781, 0.6973788721207307, 0.5802098950524738, 0.4708597807270629]\n",
      "data DP\n",
      "0.45382431367468773\n",
      "sensitive attribute  1\n",
      "prec reca accuracy for each sens\n",
      "0.9971014492753624 0.9424657534246575 0.9684813753581661\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "698\n",
      "345\n",
      "0.49426934097421205\n",
      "sensitive attribute  2\n",
      "prec reca accuracy for each sens\n",
      "0.9886506935687264 0.9355608591885441 0.9629847238542891\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1702\n",
      "793\n",
      "0.46592244418331374\n",
      "sensitive attribute  3\n",
      "prec reca accuracy for each sens\n",
      "0.9964071856287425 0.9530355097365406 0.9634247714048213\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1203\n",
      "835\n",
      "0.6940980881130507\n",
      "sensitive attribute  4\n",
      "prec reca accuracy for each sens\n",
      "0.976897689768977 0.896969696969697 0.9657477025898078\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1197\n",
      "303\n",
      "0.2531328320802005\n",
      "sensitive attribute  5\n",
      "prec reca accuracy for each sens\n",
      "0.9967948717948718 0.948170731707317 0.9749652294853964\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "719\n",
      "312\n",
      "0.4339360222531293\n",
      "sensitive attribute  6\n",
      "prec reca accuracy for each sens\n",
      "0.9891041162227603 0.9337142857142857 0.9601427721594289\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1681\n",
      "826\n",
      "0.4913741820345033\n",
      "sensitive attribute  7\n",
      "prec reca accuracy for each sens\n",
      "0.9908675799086758 0.9527991218441273 0.970262390670554\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1715\n",
      "876\n",
      "0.5107871720116618\n",
      "sensitive attribute  8\n",
      "prec reca accuracy for each sens\n",
      "0.9923664122137404 0.8904109589041096 0.9503649635036496\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "685\n",
      "262\n",
      "0.38248175182481753\n",
      "sensitive attribute  9\n",
      "prec reca accuracy for each sens\n",
      "0.9783950617283951 0.883008356545961 0.9587542087542088\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1188\n",
      "324\n",
      "0.2727272727272727\n",
      "sensitive attribute  10\n",
      "prec reca accuracy for each sens\n",
      "0.9963144963144963 0.9609004739336493 0.9702970297029703\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1212\n",
      "814\n",
      "0.6716171617161716\n",
      "sensitive attribute  11\n",
      "prec reca accuracy for each sens\n",
      "0.9961685823754789 0.9219858156028369 0.9611486486486487\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "592\n",
      "261\n",
      "0.4408783783783784\n",
      "sensitive attribute  12\n",
      "prec reca accuracy for each sens\n",
      "0.9897377423033067 0.9424538545059717 0.9657079646017699\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1808\n",
      "877\n",
      "0.4850663716814159\n",
      "sensitive attribute  13\n",
      "prec reca accuracy for each sens\n",
      "0.981651376146789 0.8991596638655462 0.937125748502994\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "668\n",
      "327\n",
      "0.48952095808383234\n",
      "sensitive attribute  14\n",
      "prec reca accuracy for each sens\n",
      "0.9950678175092479 0.9539007092198581 0.9751732101616628\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1732\n",
      "811\n",
      "0.4682448036951501\n",
      "sensitive attribute  15\n",
      "prec reca accuracy for each sens\n",
      "0.9931972789115646 0.9299363057324841 0.9792207792207792\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1155\n",
      "294\n",
      "0.2545454545454545\n",
      "sensitive attribute  16\n",
      "prec reca accuracy for each sens\n",
      "0.990521327014218 0.9403824521934758 0.951004016064257\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1245\n",
      "844\n",
      "0.6779116465863454\n",
      "sensitive attribute  17\n",
      "prec reca accuracy for each sens\n",
      "0.9865771812080537 0.9046153846153846 0.9693251533742331\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1141\n",
      "298\n",
      "0.26117440841367223\n",
      "sensitive attribute  18\n",
      "prec reca accuracy for each sens\n",
      "0.9928571428571429 0.9498861047835991 0.960285941223193\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1259\n",
      "840\n",
      "0.6671961874503575\n",
      "sensitive attribute  19\n",
      "prec reca accuracy for each sens\n",
      "0.9972144846796658 0.9250645994832042 0.9550224887556222\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "667\n",
      "359\n",
      "0.5382308845577212\n",
      "sensitive attribute  20\n",
      "prec reca accuracy for each sens\n",
      "0.9884467265725289 0.9436274509803921 0.9682631275245239\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1733\n",
      "779\n",
      "0.44950952106174263\n",
      "data acceptance rates\n",
      "[0.49426934097421205, 0.46592244418331374, 0.6940980881130507, 0.2531328320802005, 0.4339360222531293, 0.4913741820345033, 0.5107871720116618, 0.38248175182481753, 0.2727272727272727, 0.6716171617161716, 0.4408783783783784, 0.4850663716814159, 0.48952095808383234, 0.4682448036951501, 0.2545454545454545, 0.6779116465863454, 0.26117440841367223, 0.6671961874503575, 0.5382308845577212, 0.44950952106174263]\n",
      "data DP\n",
      "0.44096525603285025\n",
      "SVM accuracy--------------------------\n",
      "0.9912126537785588 0.9376558603491272 0.9645833333333333\n",
      "----------------This is for covergence at beta =  0.5  ----------------\n",
      "dimension of data\n",
      "20 2400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 698 1702 1203 1197  719 1681 1715  685 1188 1212  592 1808  668 1732\n",
      " 1155 1245 1141 1259  667 1733]\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Optimal\n",
      "objective is:\n",
      "None\n",
      "discripency is:\n",
      "None\n",
      "alpha, beta_avg 0 0.5\n",
      "<--------------------------------------->\n",
      "iteration t 1\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "sensitive attribute  7\n",
      "sensitive attribute  8\n",
      "sensitive attribute  9\n",
      "sensitive attribute  10\n",
      "sensitive attribute  11\n",
      "sensitive attribute  12\n",
      "sensitive attribute  13\n",
      "sensitive attribute  14\n",
      "sensitive attribute  15\n",
      "sensitive attribute  16\n",
      "sensitive attribute  17\n",
      "sensitive attribute  18\n",
      "sensitive attribute  19\n",
      "sensitive attribute  20\n",
      "individual acceptance rates\n",
      "[0.5229226361031518, 0.5017626321974148, 0.7256857855361596, 0.28905597326649957, 0.4561891515994437, 0.5300416418798334, 0.5399416909620991, 0.42773722627737226, 0.3055555555555556, 0.7062706270627063, 0.4864864864864865, 0.514933628318584, 0.5434131736526946, 0.4942263279445728, 0.2857142857142857, 0.7140562248995984, 0.2900964066608238, 0.7053216838760922, 0.5892053973013494, 0.4766301211771495]\n",
      "individul precision\n",
      "[0.7972602739726027, 0.7693208430913349, 0.852233676975945, 0.5895953757225434, 0.7439024390243902, 0.7901234567901234, 0.796976241900648, 0.7167235494880546, 0.5757575757575758, 0.8633177570093458, 0.6770833333333334, 0.8088077336197637, 0.7272727272727273, 0.7990654205607477, 0.6272727272727273, 0.8335208098987626, 0.6193353474320241, 0.8367117117117117, 0.8396946564885496, 0.7481840193704601]\n",
      "individual recall\n",
      "[0.7972602739726027, 0.7840095465393795, 0.852233676975945, 0.6181818181818182, 0.7439024390243902, 0.8045714285714286, 0.8100987925356751, 0.7191780821917808, 0.5821727019498607, 0.8755924170616114, 0.6914893617021277, 0.8175895765472313, 0.7394957983193278, 0.8085106382978723, 0.6592356687898089, 0.8335208098987626, 0.6307692307692307, 0.8462414578587699, 0.8527131782945736, 0.7573529411764706]\n",
      "DP all\n",
      "0.4399714998218739\n",
      "precision all 0.7776866283839212\n",
      "recall all 0.7880299251870324\n",
      "accuracy all 0.7808333333333334\n",
      "TP,FP,TN,FN\n",
      "948 271 926 255\n",
      "dimension of data\n",
      "20 2400\n",
      "[ 698 1702 1203 1197  719 1681 1715  685 1188 1212  592 1808  668 1732\n",
      " 1155 1245 1141 1259  667 1733]\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Optimal\n",
      "objective is:\n",
      "None\n",
      "discripency is:\n",
      "None\n",
      "alpha, beta_avg 0.2 0.5\n",
      "<--------------------------------------->\n",
      "iteration t 2\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "sensitive attribute  7\n",
      "sensitive attribute  8\n",
      "sensitive attribute  9\n",
      "sensitive attribute  10\n",
      "sensitive attribute  11\n",
      "sensitive attribute  12\n",
      "sensitive attribute  13\n",
      "sensitive attribute  14\n",
      "sensitive attribute  15\n",
      "sensitive attribute  16\n",
      "sensitive attribute  17\n",
      "sensitive attribute  18\n",
      "sensitive attribute  19\n",
      "sensitive attribute  20\n",
      "individual acceptance rates\n",
      "[0.5186246418338109, 0.5011750881316098, 0.6807980049875312, 0.3308270676691729, 0.4659248956884562, 0.5234979179060083, 0.5253644314868805, 0.4583941605839416, 0.3434343434343434, 0.6658415841584159, 0.4814189189189189, 0.5143805309734514, 0.5434131736526946, 0.49191685912240185, 0.3281385281385281, 0.6714859437751004, 0.32865907099035935, 0.6671961874503575, 0.5652173913043478, 0.4835545297172533]\n",
      "individul precision\n",
      "[0.6933701657458563, 0.6729191090269636, 0.8107448107448108, 0.4065656565656566, 0.6537313432835821, 0.6886363636363636, 0.709211986681465, 0.5923566878980892, 0.4068627450980392, 0.8166047087980174, 0.6912280701754386, 0.6752688172043011, 0.6225895316804407, 0.7030516431924883, 0.40105540897097625, 0.8050239234449761, 0.4533333333333333, 0.7797619047619048, 0.8090185676392573, 0.6205250596658711]\n",
      "individual recall\n",
      "[0.6876712328767123, 0.684964200477327, 0.7605956471935853, 0.48787878787878786, 0.6676829268292683, 0.6925714285714286, 0.7014270032930845, 0.636986301369863, 0.4623955431754875, 0.7808056872037915, 0.6985815602836879, 0.6818675352877307, 0.6330532212885154, 0.7080378250591016, 0.4840764331210191, 0.7570303712035995, 0.5230769230769231, 0.7460136674259681, 0.7881136950904393, 0.6372549019607843]\n",
      "DP all\n",
      "0.35265947684900306\n",
      "precision all 0.6790123456790124\n",
      "recall all 0.685785536159601\n",
      "accuracy all 0.68\n",
      "TP,FP,TN,FN\n",
      "825 390 807 378\n",
      "dimension of data\n",
      "20 2400\n",
      "[ 698 1702 1203 1197  719 1681 1715  685 1188 1212  592 1808  668 1732\n",
      " 1155 1245 1141 1259  667 1733]\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Optimal\n",
      "objective is:\n",
      "None\n",
      "discripency is:\n",
      "None\n",
      "alpha, beta_avg 0.4 0.5\n",
      "<--------------------------------------->\n",
      "iteration t 3\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "sensitive attribute  7\n",
      "sensitive attribute  8\n",
      "sensitive attribute  9\n",
      "sensitive attribute  10\n",
      "sensitive attribute  11\n",
      "sensitive attribute  12\n",
      "sensitive attribute  13\n",
      "sensitive attribute  14\n",
      "sensitive attribute  15\n",
      "sensitive attribute  16\n",
      "sensitive attribute  17\n",
      "sensitive attribute  18\n",
      "sensitive attribute  19\n",
      "sensitive attribute  20\n",
      "individual acceptance rates\n",
      "[0.5186246418338109, 0.5099882491186839, 0.6533665835411472, 0.37092731829573933, 0.49791376912378305, 0.5187388459250446, 0.5346938775510204, 0.4569343065693431, 0.38215488215488214, 0.6402640264026402, 0.49155405405405406, 0.519358407079646, 0.5434131736526946, 0.5005773672055427, 0.36363636363636365, 0.6506024096385542, 0.37949167397020156, 0.6330420969023034, 0.5892053973013494, 0.4829774956722447]\n",
      "individul precision\n",
      "[0.6795580110497238, 0.6232718894009217, 0.8002544529262087, 0.35585585585585583, 0.6033519553072626, 0.6548165137614679, 0.6684841875681571, 0.5559105431309904, 0.381057268722467, 0.7912371134020618, 0.6288659793814433, 0.6432374866879659, 0.5977961432506887, 0.657439446366782, 0.34285714285714286, 0.7938271604938272, 0.43187066974595845, 0.7528230865746549, 0.72264631043257, 0.6009557945041816]\n",
      "individual recall\n",
      "[0.673972602739726, 0.6455847255369929, 0.720504009163803, 0.47878787878787876, 0.6585365853658537, 0.6525714285714286, 0.672886937431394, 0.5958904109589042, 0.4818941504178273, 0.7274881516587678, 0.648936170212766, 0.6558089033659066, 0.6078431372549019, 0.6737588652482269, 0.4585987261146497, 0.7232845894263217, 0.5753846153846154, 0.683371298405467, 0.7338501291989664, 0.616421568627451]\n",
      "DP all\n",
      "0.28973021990478354\n",
      "precision all 0.6398373983739838\n",
      "recall all 0.6541978387364921\n",
      "accuracy all 0.6420833333333333\n",
      "TP,FP,TN,FN\n",
      "787 443 754 416\n",
      "dimension of data\n",
      "20 2400\n",
      "[ 698 1702 1203 1197  719 1681 1715  685 1188 1212  592 1808  668 1732\n",
      " 1155 1245 1141 1259  667 1733]\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Optimal\n",
      "objective is:\n",
      "None\n",
      "discripency is:\n",
      "None\n",
      "alpha, beta_avg 0.6 0.5\n",
      "<--------------------------------------->\n",
      "iteration t 4\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "sensitive attribute  7\n",
      "sensitive attribute  8\n",
      "sensitive attribute  9\n",
      "sensitive attribute  10\n",
      "sensitive attribute  11\n",
      "sensitive attribute  12\n",
      "sensitive attribute  13\n",
      "sensitive attribute  14\n",
      "sensitive attribute  15\n",
      "sensitive attribute  16\n",
      "sensitive attribute  17\n",
      "sensitive attribute  18\n",
      "sensitive attribute  19\n",
      "sensitive attribute  20\n",
      "individual acceptance rates\n",
      "[0.5171919770773639, 0.5094007050528789, 0.6118038237738986, 0.41102756892230574, 0.49791376912378305, 0.5175490779298036, 0.5276967930029155, 0.47153284671532847, 0.42508417508417506, 0.5965346534653465, 0.49155405405405406, 0.5182522123893806, 0.5164670658682635, 0.5098152424942263, 0.41385281385281386, 0.6024096385542169, 0.42769500438212094, 0.5877680698967435, 0.5622188905547226, 0.49221004039238314]\n",
      "individul precision\n",
      "[0.6260387811634349, 0.5674740484429066, 0.7730978260869565, 0.30284552845528456, 0.5726256983240223, 0.5896551724137931, 0.611049723756906, 0.5108359133126935, 0.31485148514851485, 0.7731673582295989, 0.6048109965635738, 0.5784418356456777, 0.5362318840579711, 0.6036240090600227, 0.2719665271966527, 0.784, 0.3176229508196721, 0.7608108108108108, 0.712, 0.5287221570926143]\n",
      "individual recall\n",
      "[0.6191780821917808, 0.5871121718377088, 0.6517754868270332, 0.45151515151515154, 0.625, 0.5862857142857143, 0.6070252469813392, 0.565068493150685, 0.4428969359331476, 0.6623222748815166, 0.624113475177305, 0.5884907709011944, 0.5182072829131653, 0.6300236406619385, 0.4140127388535032, 0.6614173228346457, 0.47692307692307695, 0.6412300683371298, 0.689922480620155, 0.5526960784313726]\n",
      "DP all\n",
      "0.2007762548515929\n",
      "precision all 0.5846905537459284\n",
      "recall all 0.5968412302576891\n",
      "accuracy all 0.5854166666666667\n",
      "TP,FP,TN,FN\n",
      "718 510 687 485\n",
      "dimension of data\n",
      "20 2400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 698 1702 1203 1197  719 1681 1715  685 1188 1212  592 1808  668 1732\n",
      " 1155 1245 1141 1259  667 1733]\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Optimal\n",
      "objective is:\n",
      "None\n",
      "discripency is:\n",
      "None\n",
      "alpha, beta_avg 0.8 0.5\n",
      "<--------------------------------------->\n",
      "iteration t 5\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "sensitive attribute  7\n",
      "sensitive attribute  8\n",
      "sensitive attribute  9\n",
      "sensitive attribute  10\n",
      "sensitive attribute  11\n",
      "sensitive attribute  12\n",
      "sensitive attribute  13\n",
      "sensitive attribute  14\n",
      "sensitive attribute  15\n",
      "sensitive attribute  16\n",
      "sensitive attribute  17\n",
      "sensitive attribute  18\n",
      "sensitive attribute  19\n",
      "sensitive attribute  20\n",
      "individual acceptance rates\n",
      "[0.5315186246418339, 0.5035252643948296, 0.5536159600997507, 0.46950710108604843, 0.49235048678720444, 0.5199286139202856, 0.5166180758017492, 0.4992700729927007, 0.4831649831649832, 0.5396039603960396, 0.4966216216216216, 0.5165929203539823, 0.5164670658682635, 0.5098152424942263, 0.47445887445887447, 0.5461847389558233, 0.4802804557405784, 0.5401111993645751, 0.5172413793103449, 0.5095210617426428]\n",
      "individul precision\n",
      "[0.6064690026954178, 0.5379229871645275, 0.7672672672672672, 0.3113879003558719, 0.5621468926553672, 0.5572082379862701, 0.5665914221218962, 0.5380116959064327, 0.343205574912892, 0.7477064220183486, 0.673469387755102, 0.5224839400428265, 0.6086956521739131, 0.5390713476783692, 0.29927007299270075, 0.7676470588235295, 0.36678832116788324, 0.7132352941176471, 0.6666666666666666, 0.5164212910532276]\n",
      "individual recall\n",
      "[0.6164383561643836, 0.5501193317422435, 0.5853379152348225, 0.5303030303030303, 0.6067073170731707, 0.5565714285714286, 0.5510428100987925, 0.6301369863013698, 0.5487465181058496, 0.5793838862559242, 0.7021276595744681, 0.5298588490770901, 0.5882352941176471, 0.5626477541371159, 0.5222929936305732, 0.5871766029246345, 0.6184615384615385, 0.5523917995444191, 0.5943152454780362, 0.5588235294117647]\n",
      "DP all\n",
      "0.08410885901370224\n",
      "precision all 0.5586319218241043\n",
      "recall all 0.5702410640066501\n",
      "accuracy all 0.55875\n",
      "TP,FP,TN,FN\n",
      "686 542 655 517\n",
      "dimension of data\n",
      "20 2400\n",
      "[ 698 1702 1203 1197  719 1681 1715  685 1188 1212  592 1808  668 1732\n",
      " 1155 1245 1141 1259  667 1733]\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Optimal\n",
      "objective is:\n",
      "None\n",
      "discripency is:\n",
      "None\n",
      "alpha, beta_avg 1 0.5\n",
      "<--------------------------------------->\n",
      "iteration t 6\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "sensitive attribute  7\n",
      "sensitive attribute  8\n",
      "sensitive attribute  9\n",
      "sensitive attribute  10\n",
      "sensitive attribute  11\n",
      "sensitive attribute  12\n",
      "sensitive attribute  13\n",
      "sensitive attribute  14\n",
      "sensitive attribute  15\n",
      "sensitive attribute  16\n",
      "sensitive attribute  17\n",
      "sensitive attribute  18\n",
      "sensitive attribute  19\n",
      "sensitive attribute  20\n",
      "individual acceptance rates\n",
      "[0.5, 0.5076380728554641, 0.5020781379883624, 0.5087719298245614, 0.5006954102920723, 0.5074360499702558, 0.5037900874635568, 0.5094890510948905, 0.5025252525252525, 0.5082508250825083, 0.5084459459459459, 0.504424778761062, 0.5, 0.5075057736720554, 0.5099567099567099, 0.5012048192771085, 0.5004382120946538, 0.5099285146942018, 0.5007496251874063, 0.5072129255626082]\n",
      "individul precision\n",
      "[0.49570200573065903, 0.5046296296296297, 0.7682119205298014, 0.23809523809523808, 0.28888888888888886, 0.5920281359906213, 0.5775462962962963, 0.3151862464183381, 0.3500837520938023, 0.6493506493506493, 0.3920265780730897, 0.5383771929824561, 0.5688622754491018, 0.47667804323094426, 0.2071307300509338, 0.780448717948718, 0.22416812609457093, 0.7492211838006231, 0.5598802395209581, 0.4800910125142207]\n",
      "individual recall\n",
      "[0.473972602739726, 0.5202863961813843, 0.5315005727376861, 0.4393939393939394, 0.3170731707317073, 0.5771428571428572, 0.5477497255762898, 0.3767123287671233, 0.5821727019498607, 0.47393364928909953, 0.41843971631205673, 0.5331161780673181, 0.5322128851540616, 0.49527186761229314, 0.3885350318471338, 0.547806524184477, 0.39384615384615385, 0.5478359908883826, 0.48320413436692505, 0.5171568627450981]\n",
      "DP all\n",
      "0.009956709956709942\n",
      "precision all 0.5020610057708161\n",
      "recall all 0.5062344139650873\n",
      "accuracy all 0.5008333333333334\n",
      "TP,FP,TN,FN\n",
      "609 604 593 594\n",
      "----------------This is for covergence at beta =  0.4  ----------------\n",
      "dimension of data\n",
      "20 2400\n",
      "[ 698 1702 1203 1197  719 1681 1715  685 1188 1212  592 1808  668 1732\n",
      " 1155 1245 1141 1259  667 1733]\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Optimal\n",
      "objective is:\n",
      "None\n",
      "discripency is:\n",
      "None\n",
      "alpha, beta_avg 0 0.4\n",
      "<--------------------------------------->\n",
      "iteration t 7\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "sensitive attribute  7\n",
      "sensitive attribute  8\n",
      "sensitive attribute  9\n",
      "sensitive attribute  10\n",
      "sensitive attribute  11\n",
      "sensitive attribute  12\n",
      "sensitive attribute  13\n",
      "sensitive attribute  14\n",
      "sensitive attribute  15\n",
      "sensitive attribute  16\n",
      "sensitive attribute  17\n",
      "sensitive attribute  18\n",
      "sensitive attribute  19\n",
      "sensitive attribute  20\n",
      "individual acceptance rates\n",
      "[0.5257879656160458, 0.5023501762632198, 0.7256857855361596, 0.29156223893065997, 0.46453407510431155, 0.528256989886972, 0.5393586005830904, 0.4335766423357664, 0.31734006734006737, 0.6971947194719472, 0.4847972972972973, 0.5171460176991151, 0.5374251497005988, 0.4982678983833718, 0.2883116883116883, 0.7140562248995984, 0.30148992112182293, 0.6973788721207307, 0.5832083958020989, 0.48066935949221]\n",
      "individul precision\n",
      "[0.784741144414169, 0.7368421052631579, 0.845360824742268, 0.5157593123209169, 0.7754491017964071, 0.7421171171171171, 0.7664864864864865, 0.7037037037037037, 0.4350132625994695, 0.8923076923076924, 0.7177700348432056, 0.7614973262032085, 0.6629526462395543, 0.7879490150637312, 0.5855855855855856, 0.813273340832396, 0.48546511627906974, 0.8553530751708428, 0.8406169665809768, 0.709483793517407]\n",
      "individual recall\n",
      "[0.7890410958904109, 0.7517899761336515, 0.845360824742268, 0.5454545454545454, 0.7896341463414634, 0.7531428571428571, 0.7782656421514819, 0.7157534246575342, 0.4568245125348189, 0.8933649289099526, 0.7304964539007093, 0.7730727470141151, 0.6666666666666666, 0.8037825059101655, 0.6210191082802548, 0.813273340832396, 0.5138461538461538, 0.8553530751708428, 0.8449612403100775, 0.7242647058823529]\n",
      "DP all\n",
      "0.4373740972244713\n",
      "precision all 0.7512274959083469\n",
      "recall all 0.7630922693266833\n",
      "accuracy all 0.7545833333333334\n",
      "TP,FP,TN,FN\n",
      "918 304 893 285\n",
      "dimension of data\n",
      "20 2400\n",
      "[ 698 1702 1203 1197  719 1681 1715  685 1188 1212  592 1808  668 1732\n",
      " 1155 1245 1141 1259  667 1733]\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Optimal\n",
      "objective is:\n",
      "None\n",
      "discripency is:\n",
      "None\n",
      "alpha, beta_avg 0.2 0.4\n",
      "<--------------------------------------->\n",
      "iteration t 8\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "sensitive attribute  7\n",
      "sensitive attribute  8\n",
      "sensitive attribute  9\n",
      "sensitive attribute  10\n",
      "sensitive attribute  11\n",
      "sensitive attribute  12\n",
      "sensitive attribute  13\n",
      "sensitive attribute  14\n",
      "sensitive attribute  15\n",
      "sensitive attribute  16\n",
      "sensitive attribute  17\n",
      "sensitive attribute  18\n",
      "sensitive attribute  19\n",
      "sensitive attribute  20\n",
      "individual acceptance rates\n",
      "[0.504297994269341, 0.4770857814336075, 0.6650041562759768, 0.30409356725146197, 0.4575799721835883, 0.49672813801308746, 0.5102040816326531, 0.4218978102189781, 0.3223905723905724, 0.6443894389438944, 0.46452702702702703, 0.49170353982300885, 0.5209580838323353, 0.47113163972286376, 0.3038961038961039, 0.653012048192771, 0.30850131463628394, 0.6449563145353455, 0.5442278860569715, 0.46220427005193304]\n",
      "individul precision\n",
      "[0.7045454545454546, 0.6995073891625616, 0.80625, 0.4697802197802198, 0.6899696048632219, 0.7053892215568862, 0.7291428571428571, 0.615916955017301, 0.4308093994778068, 0.8335467349551856, 0.7090909090909091, 0.6985376827896513, 0.6810344827586207, 0.7095588235294118, 0.4700854700854701, 0.8007380073800738, 0.4318181818181818, 0.8177339901477833, 0.790633608815427, 0.6604244694132334]\n",
      "individual recall\n",
      "[0.6794520547945205, 0.6778042959427207, 0.738831615120275, 0.5181818181818182, 0.6920731707317073, 0.6731428571428572, 0.7003293084522503, 0.6095890410958904, 0.4596100278551532, 0.7713270142180095, 0.6914893617021277, 0.6742671009771987, 0.6638655462184874, 0.6843971631205674, 0.5254777070063694, 0.7322834645669292, 0.4676923076923077, 0.7562642369020501, 0.7416020671834626, 0.6482843137254902]\n",
      "DP all\n",
      "0.36110805237987287\n",
      "precision all 0.7010309278350515\n",
      "recall all 0.6783042394014963\n",
      "accuracy all 0.69375\n",
      "TP,FP,TN,FN\n",
      "816 348 849 387\n",
      "dimension of data\n",
      "20 2400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 698 1702 1203 1197  719 1681 1715  685 1188 1212  592 1808  668 1732\n",
      " 1155 1245 1141 1259  667 1733]\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Optimal\n",
      "objective is:\n",
      "None\n",
      "discripency is:\n",
      "None\n",
      "alpha, beta_avg 0.4 0.4\n",
      "<--------------------------------------->\n",
      "iteration t 9\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "sensitive attribute  7\n",
      "sensitive attribute  8\n",
      "sensitive attribute  9\n",
      "sensitive attribute  10\n",
      "sensitive attribute  11\n",
      "sensitive attribute  12\n",
      "sensitive attribute  13\n",
      "sensitive attribute  14\n",
      "sensitive attribute  15\n",
      "sensitive attribute  16\n",
      "sensitive attribute  17\n",
      "sensitive attribute  18\n",
      "sensitive attribute  19\n",
      "sensitive attribute  20\n",
      "individual acceptance rates\n",
      "[0.49140401146131807, 0.45593419506462984, 0.5960099750623441, 0.3358395989974937, 0.4339360222531293, 0.48007138607971445, 0.4787172011661808, 0.43503649635036495, 0.3501683501683502, 0.58003300330033, 0.46790540540540543, 0.4657079646017699, 0.4820359281437126, 0.46016166281755194, 0.3238095238095238, 0.5983935742971888, 0.3312883435582822, 0.5885623510722796, 0.5232383808095952, 0.44431621465666477]\n",
      "individul precision\n",
      "[0.6559766763848397, 0.5979381443298969, 0.8158995815899581, 0.25870646766169153, 0.5128205128205128, 0.6555142503097894, 0.6248477466504263, 0.5906040268456376, 0.33413461538461536, 0.7823613086770982, 0.5595667870036101, 0.6342042755344418, 0.5838509316770186, 0.6286072772898369, 0.31283422459893045, 0.7677852348993288, 0.38095238095238093, 0.7354925775978407, 0.7249283667621776, 0.5662337662337662]\n",
      "individual recall\n",
      "[0.6164383561643836, 0.5536992840095465, 0.6701030927835051, 0.3151515151515151, 0.4878048780487805, 0.6045714285714285, 0.5631174533479693, 0.6027397260273972, 0.3871866295264624, 0.6516587677725119, 0.549645390070922, 0.5798045602605864, 0.5266106442577031, 0.5921985815602837, 0.37261146496815284, 0.6434195725534309, 0.4430769230769231, 0.6207289293849658, 0.6537467700258398, 0.5343137254901961]\n",
      "DP all\n",
      "0.27458405048766493\n",
      "precision all 0.6157283288650581\n",
      "recall all 0.572734829592685\n",
      "accuracy all 0.6066666666666667\n",
      "TP,FP,TN,FN\n",
      "689 430 767 514\n",
      "dimension of data\n",
      "20 2400\n",
      "[ 698 1702 1203 1197  719 1681 1715  685 1188 1212  592 1808  668 1732\n",
      " 1155 1245 1141 1259  667 1733]\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Optimal\n",
      "objective is:\n",
      "None\n",
      "discripency is:\n",
      "None\n",
      "alpha, beta_avg 0.6 0.4\n",
      "<--------------------------------------->\n",
      "iteration t 10\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "sensitive attribute  7\n",
      "sensitive attribute  8\n",
      "sensitive attribute  9\n",
      "sensitive attribute  10\n",
      "sensitive attribute  11\n",
      "sensitive attribute  12\n",
      "sensitive attribute  13\n",
      "sensitive attribute  14\n",
      "sensitive attribute  15\n",
      "sensitive attribute  16\n",
      "sensitive attribute  17\n",
      "sensitive attribute  18\n",
      "sensitive attribute  19\n",
      "sensitive attribute  20\n",
      "individual acceptance rates\n",
      "[0.4498567335243553, 0.44770857814336074, 0.545303408146301, 0.3508771929824561, 0.4228094575799722, 0.4592504461629982, 0.4536443148688047, 0.43503649635036495, 0.3627946127946128, 0.5321782178217822, 0.43243243243243246, 0.45353982300884954, 0.4565868263473054, 0.44515011547344113, 0.3541125541125541, 0.5357429718875502, 0.3698510078878177, 0.5194598888006354, 0.4992503748125937, 0.42873629544143105]\n",
      "individul precision\n",
      "[0.6942675159235668, 0.5669291338582677, 0.7850609756097561, 0.32142857142857145, 0.5493421052631579, 0.6256476683937824, 0.6246786632390745, 0.5503355704697986, 0.35730858468677495, 0.7689922480620155, 0.62890625, 0.5963414634146341, 0.5934426229508196, 0.6083009079118028, 0.33496332518337407, 0.7691154422788605, 0.3412322274881517, 0.7737003058103975, 0.7117117117117117, 0.5558546433378196]\n",
      "individual recall\n",
      "[0.5972602739726027, 0.5155131264916468, 0.5899198167239404, 0.4090909090909091, 0.5091463414634146, 0.552, 0.5334796926454446, 0.5616438356164384, 0.42896935933147634, 0.5876777251184834, 0.5709219858156028, 0.5309446254071661, 0.5070028011204482, 0.5543735224586288, 0.43630573248407645, 0.5770528683914511, 0.4430769230769231, 0.5763097949886105, 0.6124031007751938, 0.5061274509803921]\n",
      "DP all\n",
      "0.19442621516384484\n",
      "precision all 0.604089219330855\n",
      "recall all 0.5403158769742311\n",
      "accuracy all 0.5920833333333333\n",
      "TP,FP,TN,FN\n",
      "650 426 771 553\n",
      "dimension of data\n",
      "20 2400\n",
      "[ 698 1702 1203 1197  719 1681 1715  685 1188 1212  592 1808  668 1732\n",
      " 1155 1245 1141 1259  667 1733]\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Optimal\n",
      "objective is:\n",
      "None\n",
      "discripency is:\n",
      "None\n",
      "alpha, beta_avg 0.8 0.4\n",
      "<--------------------------------------->\n",
      "iteration t 11\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "sensitive attribute  7\n",
      "sensitive attribute  8\n",
      "sensitive attribute  9\n",
      "sensitive attribute  10\n",
      "sensitive attribute  11\n",
      "sensitive attribute  12\n",
      "sensitive attribute  13\n",
      "sensitive attribute  14\n",
      "sensitive attribute  15\n",
      "sensitive attribute  16\n",
      "sensitive attribute  17\n",
      "sensitive attribute  18\n",
      "sensitive attribute  19\n",
      "sensitive attribute  20\n",
      "individual acceptance rates\n",
      "[0.4355300859598854, 0.4189189189189189, 0.4713216957605985, 0.37593984962406013, 0.41585535465924894, 0.4271267102914932, 0.43090379008746355, 0.4058394160583942, 0.3813131313131313, 0.46534653465346537, 0.42905405405405406, 0.42201327433628316, 0.4281437125748503, 0.4220554272517321, 0.37922077922077924, 0.4650602409638554, 0.37773882559158634, 0.4654487688641779, 0.4362818590704648, 0.4189267166762839]\n",
      "individul precision\n",
      "[0.6743421052631579, 0.48807854137447404, 0.7336860670194003, 0.30444444444444446, 0.4782608695652174, 0.5710306406685237, 0.5317997293640054, 0.5755395683453237, 0.293598233995585, 0.7446808510638298, 0.610236220472441, 0.5216251638269986, 0.534965034965035, 0.5471956224350205, 0.2968036529680365, 0.7305699481865285, 0.32018561484918795, 0.7081911262798635, 0.6391752577319587, 0.5055096418732782]\n",
      "individual recall\n",
      "[0.5616438356164384, 0.4152744630071599, 0.4765177548682703, 0.41515151515151516, 0.43597560975609756, 0.4685714285714286, 0.4313940724478595, 0.547945205479452, 0.37047353760445684, 0.4976303317535545, 0.549645390070922, 0.43213897937024975, 0.42857142857142855, 0.4728132387706856, 0.4140127388535032, 0.47581552305961755, 0.4246153846153846, 0.47266514806378135, 0.4806201550387597, 0.4497549019607843]\n",
      "DP all\n",
      "0.09538184613653838\n",
      "precision all 0.543756145526057\n",
      "recall all 0.4596841230257689\n",
      "accuracy all 0.5358333333333334\n",
      "TP,FP,TN,FN\n",
      "553 464 733 650\n",
      "dimension of data\n",
      "20 2400\n",
      "[ 698 1702 1203 1197  719 1681 1715  685 1188 1212  592 1808  668 1732\n",
      " 1155 1245 1141 1259  667 1733]\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Optimal\n",
      "objective is:\n",
      "None\n",
      "discripency is:\n",
      "None\n",
      "alpha, beta_avg 1 0.4\n",
      "<--------------------------------------->\n",
      "iteration t 12\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "sensitive attribute  7\n",
      "sensitive attribute  8\n",
      "sensitive attribute  9\n",
      "sensitive attribute  10\n",
      "sensitive attribute  11\n",
      "sensitive attribute  12\n",
      "sensitive attribute  13\n",
      "sensitive attribute  14\n",
      "sensitive attribute  15\n",
      "sensitive attribute  16\n",
      "sensitive attribute  17\n",
      "sensitive attribute  18\n",
      "sensitive attribute  19\n",
      "sensitive attribute  20\n",
      "individual acceptance rates\n",
      "[0.40114613180515757, 0.40658049353701525, 0.400665004156276, 0.4093567251461988, 0.40055632823365783, 0.40690065437239736, 0.40699708454810496, 0.4, 0.4099326599326599, 0.40016501650165015, 0.40033783783783783, 0.4065265486725664, 0.405688622754491, 0.40473441108545033, 0.4095238095238095, 0.40080321285140563, 0.4092900964066608, 0.4011119936457506, 0.4002998500749625, 0.4068090017311021]\n",
      "individul precision\n",
      "[0.3142857142857143, 0.5144508670520231, 0.8402489626556017, 0.07959183673469387, 0.3541666666666667, 0.5, 0.5128939828080229, 0.31386861313868614, 0.15811088295687886, 0.756701030927835, 0.3924050632911392, 0.4775510204081633, 0.33948339483394835, 0.5021398002853067, 0.04439746300211417, 0.8476953907815631, 0.139186295503212, 0.7504950495049505, 0.5955056179775281, 0.40425531914893614]\n",
      "individual recall\n",
      "[0.2410958904109589, 0.42482100238663484, 0.4639175257731959, 0.11818181818181818, 0.31097560975609756, 0.39085714285714285, 0.3929747530186608, 0.2945205479452055, 0.21448467966573817, 0.43483412322274884, 0.32978723404255317, 0.3811074918566775, 0.25770308123249297, 0.4160756501182033, 0.06687898089171974, 0.47581552305961755, 0.2, 0.4316628701594533, 0.4108527131782946, 0.3492647058823529]\n",
      "DP all\n",
      "0.009932659932659904\n",
      "precision all 0.4567901234567901\n",
      "recall all 0.3690773067331671\n",
      "accuracy all 0.46375\n",
      "TP,FP,TN,FN\n",
      "444 528 669 759\n",
      "<--------------------------------------->\n"
     ]
    }
   ],
   "source": [
    "p=10\n",
    "for set_k in np.arange(p,11,2):\n",
    "#for set_k in np.arange(2,11,2): \n",
    "    \n",
    "    sens=X_test[['x'+str(k) for k in range(1,set_k+1)]]\n",
    "#     print(set_k)\n",
    "#     print(sens)\n",
    "\n",
    "\n",
    "    p=sens.shape[0]\n",
    "    q=set_k\n",
    "    for i in range(0,p): \n",
    "        for j in range(0,set_k):  \n",
    "            if sens.iloc[i,j] > 0 :\n",
    "                       sens.iloc[i,j] = 1 \n",
    "            else: \n",
    "                       sens.iloc[i,j] = 0 \n",
    "#     print(sens)\n",
    "\n",
    "    sens1 = pd.get_dummies(sens, columns=['x'+str(k) for k in range(1,set_k+1)])\n",
    "    sensitive=sens1.T\n",
    "    print(sens.shape[0],sens.shape[1])\n",
    "    print(sensitive)\n",
    "\n",
    "    # print(r.value_counts())\n",
    "    accu_all,DP_all,acceptance_rate,alpha_weight=main(sensitive, Y_test, Y_test_pred,e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensitive attribute  1\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "698\n",
      "365\n",
      "0.5229226361031518\n",
      "sensitive attribute  2\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1702\n",
      "838\n",
      "0.49236192714453586\n",
      "sensitive attribute  3\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1203\n",
      "873\n",
      "0.7256857855361596\n",
      "sensitive attribute  4\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1197\n",
      "330\n",
      "0.2756892230576441\n",
      "sensitive attribute  5\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "719\n",
      "328\n",
      "0.4561891515994437\n",
      "sensitive attribute  6\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1681\n",
      "875\n",
      "0.520523497917906\n",
      "sensitive attribute  7\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1715\n",
      "911\n",
      "0.5311953352769679\n",
      "sensitive attribute  8\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "685\n",
      "292\n",
      "0.42627737226277373\n",
      "sensitive attribute  9\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1188\n",
      "359\n",
      "0.3021885521885522\n",
      "sensitive attribute  10\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1212\n",
      "844\n",
      "0.6963696369636964\n",
      "data acceptance rates\n",
      "[0.5229226361031518, 0.49236192714453586, 0.7256857855361596, 0.2756892230576441, 0.4561891515994437, 0.520523497917906, 0.5311953352769679, 0.42627737226277373, 0.3021885521885522, 0.6963696369636964]\n",
      "data DP\n",
      "0.4499965624785155\n",
      "sensitive attribute  1\n",
      "prec reca accuracy for each sens\n",
      "0.9971014492753624 0.9424657534246575 0.9684813753581661\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "698\n",
      "345\n",
      "0.49426934097421205\n",
      "sensitive attribute  2\n",
      "prec reca accuracy for each sens\n",
      "0.9886506935687264 0.9355608591885441 0.9629847238542891\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1702\n",
      "793\n",
      "0.46592244418331374\n",
      "sensitive attribute  3\n",
      "prec reca accuracy for each sens\n",
      "0.9964071856287425 0.9530355097365406 0.9634247714048213\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1203\n",
      "835\n",
      "0.6940980881130507\n",
      "sensitive attribute  4\n",
      "prec reca accuracy for each sens\n",
      "0.976897689768977 0.896969696969697 0.9657477025898078\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1197\n",
      "303\n",
      "0.2531328320802005\n",
      "sensitive attribute  5\n",
      "prec reca accuracy for each sens\n",
      "0.9967948717948718 0.948170731707317 0.9749652294853964\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "719\n",
      "312\n",
      "0.4339360222531293\n",
      "sensitive attribute  6\n",
      "prec reca accuracy for each sens\n",
      "0.9891041162227603 0.9337142857142857 0.9601427721594289\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1681\n",
      "826\n",
      "0.4913741820345033\n",
      "sensitive attribute  7\n",
      "prec reca accuracy for each sens\n",
      "0.9908675799086758 0.9527991218441273 0.970262390670554\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1715\n",
      "876\n",
      "0.5107871720116618\n",
      "sensitive attribute  8\n",
      "prec reca accuracy for each sens\n",
      "0.9923664122137404 0.8904109589041096 0.9503649635036496\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "685\n",
      "262\n",
      "0.38248175182481753\n",
      "sensitive attribute  9\n",
      "prec reca accuracy for each sens\n",
      "0.9783950617283951 0.883008356545961 0.9587542087542088\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1188\n",
      "324\n",
      "0.2727272727272727\n",
      "sensitive attribute  10\n",
      "prec reca accuracy for each sens\n",
      "0.9963144963144963 0.9609004739336493 0.9702970297029703\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1212\n",
      "814\n",
      "0.6716171617161716\n",
      "data acceptance rates\n",
      "[0.49426934097421205, 0.46592244418331374, 0.6940980881130507, 0.2531328320802005, 0.4339360222531293, 0.4913741820345033, 0.5107871720116618, 0.38248175182481753, 0.2727272727272727, 0.6716171617161716]\n",
      "data DP\n",
      "0.44096525603285025\n",
      "SVM accuracy--------------------------\n",
      "0.9912126537785588 0.9376558603491272 0.9645833333333333\n",
      "----------------This is for covergence at beta =  0.5  ----------------\n",
      "dimension of data\n",
      "10 2400\n",
      "[ 698 1702 1203 1197  719 1681 1715  685 1188 1212]\n",
      "Optimal\n",
      "objective is:\n",
      "6340.0\n",
      "discripency is:\n",
      "None\n",
      " beta_avg 0.5\n",
      "<--------------------------------------->\n",
      "iteration t 1\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "sensitive attribute  7\n",
      "sensitive attribute  8\n",
      "sensitive attribute  9\n",
      "sensitive attribute  10\n",
      "individual acceptance rates\n",
      "[0.5229226361031518, 0.5017626321974148, 0.7256857855361596, 0.28905597326649957, 0.4561891515994437, 0.5300416418798334, 0.5399416909620991, 0.42773722627737226, 0.3055555555555556, 0.7062706270627063]\n",
      "individul precision\n",
      "[0.9726027397260274, 0.949648711943794, 0.9782359679266895, 0.9017341040462428, 0.9664634146341463, 0.9528619528619529, 0.9622030237580994, 0.9385665529010239, 0.931129476584022, 0.9672897196261683]\n",
      "individual recall\n",
      "[0.9726027397260274, 0.9677804295942721, 0.9782359679266895, 0.9454545454545454, 0.9664634146341463, 0.9702857142857143, 0.9780461031833151, 0.9417808219178082, 0.9415041782729805, 0.981042654028436]\n",
      "DP all\n",
      "0.43662981226966\n",
      "precision all 0.9565217391304348\n",
      "recall all 0.969243557772236\n",
      "accuracy all 0.9625\n",
      "TP,FP,TN,FN\n",
      "1166 53 1144 37\n",
      "----------------This is for covergence at beta =  0.4  ----------------\n",
      "dimension of data\n",
      "10 2400\n",
      "[ 698 1702 1203 1197  719 1681 1715  685 1188 1212]\n",
      "Optimal\n",
      "objective is:\n",
      "6340.0\n",
      "discripency is:\n",
      "None\n",
      " beta_avg 0.4\n",
      "<--------------------------------------->\n",
      "iteration t 2\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "sensitive attribute  7\n",
      "sensitive attribute  8\n",
      "sensitive attribute  9\n",
      "sensitive attribute  10\n",
      "individual acceptance rates\n",
      "[0.5229226361031518, 0.5017626321974148, 0.7256857855361596, 0.28905597326649957, 0.4561891515994437, 0.5300416418798334, 0.5399416909620991, 0.42773722627737226, 0.3055555555555556, 0.7062706270627063]\n",
      "individul precision\n",
      "[0.9726027397260274, 0.949648711943794, 0.9782359679266895, 0.9017341040462428, 0.9664634146341463, 0.9528619528619529, 0.9622030237580994, 0.9385665529010239, 0.931129476584022, 0.9672897196261683]\n",
      "individual recall\n",
      "[0.9726027397260274, 0.9677804295942721, 0.9782359679266895, 0.9454545454545454, 0.9664634146341463, 0.9702857142857143, 0.9780461031833151, 0.9417808219178082, 0.9415041782729805, 0.981042654028436]\n",
      "DP all\n",
      "0.43662981226966\n",
      "precision all 0.9565217391304348\n",
      "recall all 0.969243557772236\n",
      "accuracy all 0.9625\n",
      "TP,FP,TN,FN\n",
      "1166 53 1144 37\n",
      "<--------------------------------------->\n"
     ]
    }
   ],
   "source": [
    " accu_all,DP_all,acceptance_rate,alpha_weight=main2(sensitive, Y_test, Y_test_pred,e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            x1        x2        x3        x4        x5        x6        x7  \\\n",
      "0     1.392243  1.679704  1.340495  1.169002 -0.169651  1.388412  1.295138   \n",
      "1     0.281930  0.707585  2.675535 -0.119078 -2.308004  1.925658  1.426423   \n",
      "2     0.627546 -1.091224 -1.030816 -2.612045 -0.022696  0.068534 -0.092952   \n",
      "3     1.539515  1.139298  0.632225  0.576092  1.016257  0.141090 -1.288185   \n",
      "4     3.593012  2.077929  2.319466 -1.762514 -3.055532  4.126875  3.522920   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "7995 -2.126879 -3.555727  0.071769 -0.986176  2.558081  2.983460  0.846630   \n",
      "7996  2.754907 -1.817496  4.405390 -1.870844  2.225684  0.876259 -2.341649   \n",
      "7997  0.177864 -1.524149  2.712979  0.013804 -0.611786 -0.381373  1.269916   \n",
      "7998  2.455750  0.216205  4.581337 -3.531081 -1.558274  0.338572 -0.721618   \n",
      "7999  3.218231  1.572063  0.092973 -1.331167  1.232236 -0.501353  0.321379   \n",
      "\n",
      "            x8        x9       x10       x11       x12        x13        x14  \\\n",
      "0    -1.895428 -1.148357  1.614701 -1.049498  2.660067   0.037983  -4.219779   \n",
      "1    -1.184452  1.280644  1.888044 -0.990781  0.864279   6.091637  -8.104712   \n",
      "2    -4.457771 -2.416174  2.009805 -1.455803  1.670899   1.846383  -0.512428   \n",
      "3     2.764486 -0.035432  0.414372 -1.879933  1.767752  -1.977889   0.792234   \n",
      "4    -3.015858  4.153499  0.263969 -0.137930  2.148686   9.614354 -14.961108   \n",
      "...        ...       ...       ...       ...       ...        ...        ...   \n",
      "7995 -1.050211  0.588597  0.740106 -4.282749 -2.794996   3.025729   2.182613   \n",
      "7996  2.896888  1.644242  1.025150 -6.581075 -2.436871   6.117929  -3.159735   \n",
      "7997  1.446300  1.571569  1.309040 -3.710406 -0.375081   5.804239  -4.759974   \n",
      "7998  3.727277  4.616415 -1.762766 -2.002525 -2.463831  10.359781  -8.509465   \n",
      "7999  0.841131  0.745692  3.554162 -5.447069  6.053599   1.035093  -3.759005   \n",
      "\n",
      "           x15        x16        x17       x18       x19       x20  \n",
      "0    -1.514455  -0.292254  -0.073725  0.586313  3.424058 -0.627274  \n",
      "1     2.181631   3.892096  -2.103797  2.268848 -0.261387  2.047578  \n",
      "2    -1.256517  -4.991383  -3.077433 -1.348873  3.291993 -1.381279  \n",
      "3    -0.789747   0.843799   4.119767  0.438513 -0.414191  0.467554  \n",
      "4     8.028063  12.853176 -12.024976  6.678605 -0.840528  3.388474  \n",
      "...        ...        ...        ...       ...       ...       ...  \n",
      "7995  4.820896  -1.844148  -1.334843 -1.502188 -4.716004  5.190138  \n",
      "7996  4.422718   2.333759   5.759854  2.641768 -2.884294  7.506101  \n",
      "7997  2.171444   1.518408   2.995542  1.567182 -1.766561  4.972580  \n",
      "7998  7.235481  12.417189  -0.970669  8.152111 -5.739933  5.964156  \n",
      "7999 -1.677852  -1.063530   3.387187 -1.603658  0.248711  2.940528  \n",
      "\n",
      "[8000 rows x 20 columns]\n",
      "      y\n",
      "0     0\n",
      "1     0\n",
      "2     0\n",
      "3     0\n",
      "4     0\n",
      "...  ..\n",
      "7995  1\n",
      "7996  1\n",
      "7997  1\n",
      "7998  1\n",
      "7999  1\n",
      "\n",
      "[8000 rows x 1 columns]\n",
      "There are 5600 samples in the training set and 2400 samples in the test set\n",
      "y    int64\n",
      "dtype: object\n",
      "y    int64\n",
      "dtype: object\n",
      "y    int64\n",
      "dtype: object\n",
      "y    int64\n",
      "dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/subham/anaconda3/envs/tensorflow/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the SVM classifier on training data is 1.00\n",
      "The accuracy of the SVM classifier on test data is 0.96\n",
      "####Train prediction Label###############################################\n",
      "[[9.99872581e-01 1.27418803e-04]\n",
      " [9.98531918e-01 1.46808186e-03]\n",
      " [9.16411675e-07 9.99999084e-01]\n",
      " ...\n",
      " [3.63959211e-02 9.63604079e-01]\n",
      " [1.99659556e-02 9.80034044e-01]\n",
      " [9.99999900e-01 1.00000010e-07]]\n",
      "[-0.78036573 -0.5887034   0.51103164 ...  0.17968937  0.22813585\n",
      " -1.38292185]\n",
      "            x1        x2        x3        x4        x5        x6        x7  \\\n",
      "0     3.726016  1.413642  2.273420 -0.922466 -1.824458  1.962949  1.624339   \n",
      "1     2.688174 -1.027868 -1.682078 -1.840101  1.103014 -1.896824  0.124650   \n",
      "2     1.023729 -2.973104  0.426373  0.225930 -2.585958  0.902453  2.545473   \n",
      "3    -1.808053  1.236258  3.721401 -0.016646 -2.008842  0.492476  2.205975   \n",
      "4    -0.215031  0.124284  2.116305 -3.074894 -2.958696 -0.016511  2.452041   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "2395  3.315519  0.862938  1.894183 -3.243664 -1.783574  0.940924  1.852839   \n",
      "2396  1.201637  0.780115  0.833288 -2.137964  1.745226 -0.618062 -0.108009   \n",
      "2397  2.498353 -0.012291  0.513396 -1.679410 -1.621630  0.898963  1.233787   \n",
      "2398  5.475439  2.043320  5.369473 -1.754758 -1.682912 -0.778325 -1.984432   \n",
      "2399  0.805834  0.938246  0.770220 -1.652054 -0.580682  0.038329  1.814350   \n",
      "\n",
      "            x8        x9       x10       x11        x12       x13       x14  \\\n",
      "0     0.490037  0.547875 -0.885342 -1.692963   3.168391  4.698117 -7.795435   \n",
      "1    -3.586503 -2.984660  0.327856 -1.335388   2.160897 -1.187367  1.151882   \n",
      "2     3.559631 -1.532219  1.689894 -7.023949   6.414109  4.844646 -2.448145   \n",
      "3     4.457250 -1.697281  2.870452 -6.327482   6.388491  4.590396 -3.818664   \n",
      "4     0.338043  0.124814  1.456617 -3.475118   3.996575  8.407927 -6.913455   \n",
      "...        ...       ...       ...       ...        ...       ...       ...   \n",
      "2395 -2.169884 -0.367307  0.728958 -2.818645   3.968014  6.930969 -8.504277   \n",
      "2396 -0.500619  2.754513  1.420149 -2.168208  -0.302743  2.942150 -3.360247   \n",
      "2397  1.805487 -2.192609  4.713881 -8.342469  10.009026  4.077224 -4.372858   \n",
      "2398  4.449797  0.828297  0.176962 -3.799636   2.542284  6.225360 -8.531017   \n",
      "2399  0.024413 -1.009131  1.479877 -3.514554   4.815701  2.772282 -3.309942   \n",
      "\n",
      "           x15       x16       x17       x18       x19       x20  \n",
      "0     4.017561  9.101517 -4.328087  8.033234  0.870161  0.404177  \n",
      "1    -2.383541 -5.407531 -1.744925 -0.462722  5.848033 -2.127948  \n",
      "2     3.284144  3.037343  1.079736  5.994343 -1.364255  1.522722  \n",
      "3    -2.816748  2.271956  8.413762  4.993741 -3.301473  0.878408  \n",
      "4     1.090160  4.530454 -1.103086  5.525074 -2.242880  1.229985  \n",
      "...        ...       ...       ...       ...       ...       ...  \n",
      "2395  2.225361  5.222216 -4.870262  6.321041  1.874691  0.440226  \n",
      "2396  0.578984  0.117923  0.975259 -2.284575 -2.098053  4.621331  \n",
      "2397 -0.882410  0.009383  2.961416  3.044971  0.356333  0.686769  \n",
      "2398  1.516578  8.493274  5.096081  8.542786  0.947459  2.223054  \n",
      "2399 -1.210632  1.043588  0.651040  2.721024 -0.021531  0.062070  \n",
      "\n",
      "[2400 rows x 20 columns]\n",
      "[0 0 1 ... 1 1 0]\n",
      "      y\n",
      "0     0\n",
      "1     0\n",
      "2     1\n",
      "3     0\n",
      "4     0\n",
      "...  ..\n",
      "2395  0\n",
      "2396  1\n",
      "2397  1\n",
      "2398  1\n",
      "2399  0\n",
      "\n",
      "[2400 rows x 1 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/subham/anaconda3/envs/tensorflow/lib/python3.6/site-packages/pandas/core/indexing.py:671: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "/home/subham/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/subham/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0     1     2     3     4     5     6     7     8     9     ...  2390  \\\n",
      "x1_0.0     0     0     0     1     1     0     0     0     0     0  ...     0   \n",
      "x1_1.0     1     1     1     0     0     1     1     1     1     1  ...     1   \n",
      "x2_0.0     0     1     1     0     0     1     1     1     0     1  ...     1   \n",
      "x2_1.0     1     0     0     1     1     0     0     0     1     0  ...     0   \n",
      "\n",
      "        2391  2392  2393  2394  2395  2396  2397  2398  2399  \n",
      "x1_0.0     0     0     1     0     0     0     0     0     0  \n",
      "x1_1.0     1     1     0     1     1     1     1     1     1  \n",
      "x2_0.0     1     1     1     1     0     0     1     0     0  \n",
      "x2_1.0     0     0     0     0     1     1     0     1     1  \n",
      "\n",
      "[4 rows x 2400 columns]\n",
      "sensitive attribute  1\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "698\n",
      "365\n",
      "0.5229226361031518\n",
      "sensitive attribute  2\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1702\n",
      "838\n",
      "0.49236192714453586\n",
      "sensitive attribute  3\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1203\n",
      "873\n",
      "0.7256857855361596\n",
      "sensitive attribute  4\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1197\n",
      "330\n",
      "0.2756892230576441\n",
      "data acceptance rates\n",
      "[0.5229226361031518, 0.49236192714453586, 0.7256857855361596, 0.2756892230576441]\n",
      "data DP\n",
      "0.4499965624785155\n",
      "sensitive attribute  1\n",
      "prec reca accuracy for each sens\n",
      "0.9971014492753624 0.9424657534246575 0.9684813753581661\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "698\n",
      "345\n",
      "0.49426934097421205\n",
      "sensitive attribute  2\n",
      "prec reca accuracy for each sens\n",
      "0.9886506935687264 0.9355608591885441 0.9629847238542891\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1702\n",
      "793\n",
      "0.46592244418331374\n",
      "sensitive attribute  3\n",
      "prec reca accuracy for each sens\n",
      "0.9964071856287425 0.9530355097365406 0.9634247714048213\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1203\n",
      "835\n",
      "0.6940980881130507\n",
      "sensitive attribute  4\n",
      "prec reca accuracy for each sens\n",
      "0.976897689768977 0.896969696969697 0.9657477025898078\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1197\n",
      "303\n",
      "0.2531328320802005\n",
      "data acceptance rates\n",
      "[0.49426934097421205, 0.46592244418331374, 0.6940980881130507, 0.2531328320802005]\n",
      "data DP\n",
      "0.44096525603285025\n",
      "SVM accuracy--------------------------\n",
      "0.9912126537785588 0.9376558603491272 0.9645833333333333\n",
      "precision recall accuracy\n",
      "dimension of data\n",
      "4 2400\n",
      "Optimal\n",
      "discripency is:\n",
      "0.56833333\n",
      "gamma-epsilon-delta 0.35 0.02 1\n",
      "################################\n",
      "sensitive attribute  1\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.5265306122448979 0.35342465753424657 0.49570200573065903\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "698 245 0.3510028653295129\n",
      "sensitive attribute  2\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.985172981878089 0.7136038186157518 0.8537015276145711\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1702 607 0.35663924794359575\n",
      "sensitive attribute  3\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9930715935334873 0.4925544100801833 0.6292601828761429\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1203 433 0.35993349958437243\n",
      "sensitive attribute  4\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7088305489260143 0.9 0.8705096073517126\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1197 419 0.35004177109440265\n",
      "data acceptance rates\n",
      "[0.3510028653295129, 0.35663924794359575, 0.35993349958437243, 0.35004177109440265]\n",
      "data DP\n",
      "0.009891728489969775\n",
      "total accepted \n",
      "finalprecision 0.8532863849765259\n",
      "finalrecall 0.6043225270157938\n",
      "finalaccuracy 0.7495833333333334\n",
      "dimension of data\n",
      "4 2400\n",
      "Optimal\n",
      "discripency is:\n",
      "0.57833333\n",
      "gamma-epsilon-delta 0.35 0.04 1\n",
      "################################\n",
      "sensitive attribute  1\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.5265306122448979 0.35342465753424657 0.49570200573065903\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "698 245 0.3510028653295129\n",
      "sensitive attribute  2\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9854604200323102 0.7279236276849642 0.8607520564042304\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1702 619 0.363689776733255\n",
      "sensitive attribute  3\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9932584269662922 0.5063001145475372 0.6392352452202826\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1203 445 0.36990856192851207\n",
      "sensitive attribute  4\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7088305489260143 0.9 0.8705096073517126\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1197 419 0.35004177109440265\n",
      "data acceptance rates\n",
      "[0.3510028653295129, 0.363689776733255, 0.36990856192851207, 0.35004177109440265]\n",
      "data DP\n",
      "0.019866790834109416\n",
      "total accepted \n",
      "finalprecision 0.8553240740740741\n",
      "finalrecall 0.6142975893599335\n",
      "finalaccuracy 0.7545833333333334\n",
      "dimension of data\n",
      "4 2400\n",
      "Optimal\n",
      "discripency is:\n",
      "0.60833333\n",
      "gamma-epsilon-delta 0.35 0.1 1\n",
      "################################\n",
      "sensitive attribute  1\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.5265306122448979 0.35342465753424657 0.49570200573065903\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "698 245 0.3510028653295129\n",
      "sensitive attribute  2\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9862595419847329 0.7708830548926014 0.881903642773208\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1702 655 0.38484136310223266\n",
      "sensitive attribute  3\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9937629937629938 0.5475372279495991 0.6691604322527016\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1203 481 0.399833748960931\n",
      "sensitive attribute  4\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7088305489260143 0.9 0.8705096073517126\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1197 419 0.35004177109440265\n",
      "data acceptance rates\n",
      "[0.3510028653295129, 0.38484136310223266, 0.399833748960931, 0.35004177109440265]\n",
      "data DP\n",
      "0.049791977866528336\n",
      "total accepted \n",
      "finalprecision 0.8611111111111112\n",
      "finalrecall 0.6442227763923525\n",
      "finalaccuracy 0.7695833333333333\n",
      "dimension of data\n",
      "4 2400\n",
      "Optimal\n",
      "discripency is:\n",
      "0.63333333\n",
      "gamma-epsilon-delta 0.35 0.15 1\n",
      "################################\n",
      "sensitive attribute  1\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.5265306122448979 0.35342465753424657 0.49570200573065903\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "698 245 0.3510028653295129\n",
      "sensitive attribute  2\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9868613138686131 0.8066825775656324 0.899529964747356\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1702 685 0.40246768507638075\n",
      "sensitive attribute  3\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9941291585127201 0.581901489117984 0.6940980881130507\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1203 511 0.4247714048212801\n",
      "sensitive attribute  4\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7088305489260143 0.9 0.8705096073517126\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1197 419 0.35004177109440265\n",
      "data acceptance rates\n",
      "[0.3510028653295129, 0.40246768507638075, 0.4247714048212801, 0.35004177109440265]\n",
      "data DP\n",
      "0.07472963372687746\n",
      "total accepted \n",
      "finalprecision 0.8655913978494624\n",
      "finalrecall 0.6691604322527016\n",
      "finalaccuracy 0.7820833333333334\n",
      "dimension of data\n",
      "4 2400\n",
      "Optimal\n",
      "discripency is:\n",
      "0.65833333\n",
      "gamma-epsilon-delta 0.35 0.2 1\n",
      "################################\n",
      "sensitive attribute  1\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.5265306122448979 0.35342465753424657 0.49570200573065903\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "698 245 0.3510028653295129\n",
      "sensitive attribute  2\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9874125874125874 0.8424821002386634 0.9171562867215041\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1702 715 0.4200940070505288\n",
      "sensitive attribute  3\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9944547134935305 0.6162657502863689 0.7190357439733999\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1203 541 0.44970906068162925\n",
      "sensitive attribute  4\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7088305489260143 0.9 0.8705096073517126\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1197 419 0.35004177109440265\n",
      "data acceptance rates\n",
      "[0.3510028653295129, 0.4200940070505288, 0.44970906068162925, 0.35004177109440265]\n",
      "data DP\n",
      "0.09966728958722659\n",
      "total accepted \n",
      "finalprecision 0.8697916666666666\n",
      "finalrecall 0.6940980881130507\n",
      "finalaccuracy 0.7945833333333333\n",
      "dimension of data\n",
      "4 2400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal\n",
      "discripency is:\n",
      "0.7075\n",
      "gamma-epsilon-delta 0.35 0.3 1\n",
      "################################\n",
      "sensitive attribute  1\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.5224489795918368 0.3506849315068493 0.49283667621776506\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "698 245 0.3510028653295129\n",
      "sensitive attribute  2\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9884020618556701 0.9152744630071599 0.9529964747356052\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1702 776 0.45593419506462984\n",
      "sensitive attribute  3\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9950083194675541 0.6849942726231386 0.7689110556940981\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1203 601 0.4995843724023275\n",
      "sensitive attribute  4\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7071428571428572 0.9 0.8696741854636592\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1197 420 0.3508771929824561\n",
      "data acceptance rates\n",
      "[0.3510028653295129, 0.45593419506462984, 0.4995843724023275, 0.3508771929824561]\n",
      "data DP\n",
      "0.14870717941987138\n",
      "total accepted \n",
      "finalprecision 0.8765915768854065\n",
      "finalrecall 0.743973399833749\n",
      "finalaccuracy 0.8191666666666667\n",
      "dimension of data\n",
      "4 2400\n",
      "Optimal\n",
      "discripency is:\n",
      "0.75833333\n",
      "gamma-epsilon-delta 0.35 0.4 1\n",
      "################################\n",
      "sensitive attribute  1\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.5958188153310104 0.4684931506849315 0.5558739255014327\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "698 287 0.41117478510028654\n",
      "sensitive attribute  2\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9886506935687264 0.9355608591885441 0.9629847238542891\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1702 793 0.46592244418331374\n",
      "sensitive attribute  3\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9954614220877458 0.7537227949599083 0.8187863674147964\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1203 661 0.5494596841230258\n",
      "sensitive attribute  4\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7088305489260143 0.9 0.8705096073517126\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1197 419 0.35004177109440265\n",
      "data acceptance rates\n",
      "[0.41117478510028654, 0.46592244418331374, 0.5494596841230258, 0.35004177109440265]\n",
      "data DP\n",
      "0.19941791302862316\n",
      "total accepted \n",
      "finalprecision 0.8842592592592593\n",
      "finalrecall 0.7938487115544473\n",
      "finalaccuracy 0.8445833333333334\n",
      "dimension of data\n",
      "4 2400\n",
      "Optimal\n",
      "discripency is:\n",
      "0.80833333\n",
      "gamma-epsilon-delta 0.35 0.5 1\n",
      "################################\n",
      "sensitive attribute  1\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.6657060518731989 0.6328767123287671 0.6418338108882522\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "698 347 0.497134670487106\n",
      "sensitive attribute  2\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9886506935687264 0.9355608591885441 0.9629847238542891\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1702 793 0.46592244418331374\n",
      "sensitive attribute  3\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9958391123439667 0.8224513172966781 0.8686616791354946\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1203 721 0.5993349958437241\n",
      "sensitive attribute  4\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7088305489260143 0.9 0.8705096073517126\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1197 419 0.35004177109440265\n",
      "data acceptance rates\n",
      "[0.497134670487106, 0.46592244418331374, 0.5993349958437241, 0.35004177109440265]\n",
      "data DP\n",
      "0.24929322474932142\n",
      "total accepted \n",
      "finalprecision 0.8903508771929824\n",
      "finalrecall 0.8437240232751455\n",
      "finalaccuracy 0.8695833333333334\n",
      "dimension of data\n",
      "4 2400\n",
      "Optimal\n",
      "discripency is:\n",
      "0.85833333\n",
      "gamma-epsilon-delta 0.35 0.6 1\n",
      "################################\n",
      "sensitive attribute  1\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7439293598233996 0.9232876712328767 0.7936962750716332\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "698 453 0.6489971346704871\n",
      "sensitive attribute  2\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9879518072289156 0.8806682577565632 0.935957696827262\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1702 747 0.4388954171562867\n",
      "sensitive attribute  3\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9961587708066582 0.8911798396334479 0.9185369908561929\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1203 781 0.6492103075644223\n",
      "sensitive attribute  4\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7088305489260143 0.9 0.8705096073517126\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1197 419 0.35004177109440265\n",
      "data acceptance rates\n",
      "[0.6489971346704871, 0.4388954171562867, 0.6492103075644223, 0.35004177109440265]\n",
      "data DP\n",
      "0.2991685364700197\n",
      "total accepted \n",
      "finalprecision 0.8958333333333334\n",
      "finalrecall 0.8935993349958438\n",
      "finalaccuracy 0.8945833333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/subham/anaconda3/envs/tensorflow/lib/python3.6/site-packages/pandas/core/indexing.py:671: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "/home/subham/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/subham/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0     1     2     3     4     5     6     7     8     9     ...  2390  \\\n",
      "x1_0.0     0     0     0     1     1     0     0     0     0     0  ...     0   \n",
      "x1_1.0     1     1     1     0     0     1     1     1     1     1  ...     1   \n",
      "x2_0.0     0     1     1     0     0     1     1     1     0     1  ...     1   \n",
      "x2_1.0     1     0     0     1     1     0     0     0     1     0  ...     0   \n",
      "x3_0.0     0     1     0     0     0     0     0     1     0     0  ...     1   \n",
      "x3_1.0     1     0     1     1     1     1     1     0     1     1  ...     0   \n",
      "x4_0.0     1     1     0     1     1     1     0     1     1     1  ...     0   \n",
      "x4_1.0     0     0     1     0     0     0     1     0     0     0  ...     1   \n",
      "\n",
      "        2391  2392  2393  2394  2395  2396  2397  2398  2399  \n",
      "x1_0.0     0     0     1     0     0     0     0     0     0  \n",
      "x1_1.0     1     1     0     1     1     1     1     1     1  \n",
      "x2_0.0     1     1     1     1     0     0     1     0     0  \n",
      "x2_1.0     0     0     0     0     1     1     0     1     1  \n",
      "x3_0.0     0     0     1     0     0     0     0     0     0  \n",
      "x3_1.0     1     1     0     1     1     1     1     1     1  \n",
      "x4_0.0     1     0     1     1     1     1     1     1     1  \n",
      "x4_1.0     0     1     0     0     0     0     0     0     0  \n",
      "\n",
      "[8 rows x 2400 columns]\n",
      "sensitive attribute  1\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "698\n",
      "365\n",
      "0.5229226361031518\n",
      "sensitive attribute  2\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1702\n",
      "838\n",
      "0.49236192714453586\n",
      "sensitive attribute  3\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1203\n",
      "873\n",
      "0.7256857855361596\n",
      "sensitive attribute  4\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1197\n",
      "330\n",
      "0.2756892230576441\n",
      "sensitive attribute  5\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "719\n",
      "328\n",
      "0.4561891515994437\n",
      "sensitive attribute  6\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1681\n",
      "875\n",
      "0.520523497917906\n",
      "sensitive attribute  7\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1715\n",
      "911\n",
      "0.5311953352769679\n",
      "sensitive attribute  8\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "685\n",
      "292\n",
      "0.42627737226277373\n",
      "data acceptance rates\n",
      "[0.5229226361031518, 0.49236192714453586, 0.7256857855361596, 0.2756892230576441, 0.4561891515994437, 0.520523497917906, 0.5311953352769679, 0.42627737226277373]\n",
      "data DP\n",
      "0.4499965624785155\n",
      "sensitive attribute  1\n",
      "prec reca accuracy for each sens\n",
      "0.9971014492753624 0.9424657534246575 0.9684813753581661\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "698\n",
      "345\n",
      "0.49426934097421205\n",
      "sensitive attribute  2\n",
      "prec reca accuracy for each sens\n",
      "0.9886506935687264 0.9355608591885441 0.9629847238542891\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1702\n",
      "793\n",
      "0.46592244418331374\n",
      "sensitive attribute  3\n",
      "prec reca accuracy for each sens\n",
      "0.9964071856287425 0.9530355097365406 0.9634247714048213\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1203\n",
      "835\n",
      "0.6940980881130507\n",
      "sensitive attribute  4\n",
      "prec reca accuracy for each sens\n",
      "0.976897689768977 0.896969696969697 0.9657477025898078\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1197\n",
      "303\n",
      "0.2531328320802005\n",
      "sensitive attribute  5\n",
      "prec reca accuracy for each sens\n",
      "0.9967948717948718 0.948170731707317 0.9749652294853964\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "719\n",
      "312\n",
      "0.4339360222531293\n",
      "sensitive attribute  6\n",
      "prec reca accuracy for each sens\n",
      "0.9891041162227603 0.9337142857142857 0.9601427721594289\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1681\n",
      "826\n",
      "0.4913741820345033\n",
      "sensitive attribute  7\n",
      "prec reca accuracy for each sens\n",
      "0.9908675799086758 0.9527991218441273 0.970262390670554\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1715\n",
      "876\n",
      "0.5107871720116618\n",
      "sensitive attribute  8\n",
      "prec reca accuracy for each sens\n",
      "0.9923664122137404 0.8904109589041096 0.9503649635036496\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "685\n",
      "262\n",
      "0.38248175182481753\n",
      "data acceptance rates\n",
      "[0.49426934097421205, 0.46592244418331374, 0.6940980881130507, 0.2531328320802005, 0.4339360222531293, 0.4913741820345033, 0.5107871720116618, 0.38248175182481753]\n",
      "data DP\n",
      "0.44096525603285025\n",
      "SVM accuracy--------------------------\n",
      "0.9912126537785588 0.9376558603491272 0.9645833333333333\n",
      "precision recall accuracy\n",
      "dimension of data\n",
      "8 2400\n",
      "Optimal\n",
      "discripency is:\n",
      "0.56833333\n",
      "gamma-epsilon-delta 0.35 0.02 1\n",
      "################################\n",
      "sensitive attribute  1\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.991869918699187 0.6684931506849315 0.8237822349570201\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "698 246 0.3524355300859599\n",
      "sensitive attribute  2\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8085808580858086 0.5847255369928401 0.72737955346651\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1702 606 0.35605170387779084\n",
      "sensitive attribute  3\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9953810623556582 0.4936998854524628 0.6309226932668329\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1203 433 0.35993349958437243\n",
      "sensitive attribute  4\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7231503579952268 0.9181818181818182 0.8805346700083542\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1197 419 0.35004177109440265\n",
      "sensitive attribute  5\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9224806201550387 0.725609756097561 0.847009735744089\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "719 258 0.3588317107093185\n",
      "sensitive attribute  6\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.835016835016835 0.5668571428571428 0.7162403331350387\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1681 594 0.3533610945865556\n",
      "sensitive attribute  7\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9558823529411765 0.6421514818880352 0.7941690962099125\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1715 612 0.3568513119533528\n",
      "sensitive attribute  8\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.6208333333333333 0.5102739726027398 0.6583941605839416\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "685 240 0.35036496350364965\n",
      "data acceptance rates\n",
      "[0.3524355300859599, 0.35605170387779084, 0.35993349958437243, 0.35004177109440265, 0.3588317107093185, 0.3533610945865556, 0.3568513119533528, 0.35036496350364965]\n",
      "data DP\n",
      "0.009891728489969775\n",
      "total accepted \n",
      "finalprecision 0.8615023474178404\n",
      "finalrecall 0.6101413133832086\n",
      "finalaccuracy 0.7554166666666666\n",
      "dimension of data\n",
      "8 2400\n",
      "Optimal\n",
      "discripency is:\n",
      "0.57833333\n",
      "gamma-epsilon-delta 0.35 0.04 1\n",
      "################################\n",
      "sensitive attribute  1\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9961240310077519 0.7041095890410959 0.8438395415472779\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "698 258 0.36962750716332377\n",
      "sensitive attribute  2\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8085808580858086 0.5847255369928401 0.72737955346651\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1702 606 0.35605170387779084\n",
      "sensitive attribute  3\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9955056179775281 0.5074455899198167 0.6408977556109726\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1203 445 0.36990856192851207\n",
      "sensitive attribute  4\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7255369928400954 0.9212121212121213 0.8822055137844611\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1197 419 0.35004177109440265\n",
      "sensitive attribute  5\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9603174603174603 0.7378048780487805 0.866481223922114\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "719 252 0.35048678720445064\n",
      "sensitive attribute  6\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8251633986928104 0.5771428571428572 0.7162403331350387\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1681 612 0.364069006543724\n",
      "sensitive attribute  7\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9738134206219312 0.6531284302963776 0.8064139941690962\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1715 611 0.356268221574344\n",
      "sensitive attribute  8\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.6007905138339921 0.5205479452054794 0.6481751824817519\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "685 253 0.36934306569343067\n",
      "data acceptance rates\n",
      "[0.36962750716332377, 0.35605170387779084, 0.36990856192851207, 0.35004177109440265, 0.35048678720445064, 0.364069006543724, 0.356268221574344, 0.36934306569343067]\n",
      "data DP\n",
      "0.019866790834109416\n",
      "total accepted \n",
      "finalprecision 0.8645833333333334\n",
      "finalrecall 0.6209476309226932\n",
      "finalaccuracy 0.76125\n",
      "dimension of data\n",
      "8 2400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal\n",
      "discripency is:\n",
      "0.60833333\n",
      "gamma-epsilon-delta 0.35 0.1 1\n",
      "################################\n",
      "sensitive attribute  1\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9959183673469387 0.6684931506849315 0.8252148997134671\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "698 245 0.3510028653295129\n",
      "sensitive attribute  2\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8213740458015267 0.6420047732696897 0.754994124559342\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1702 655 0.38484136310223266\n",
      "sensitive attribute  3\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9958419958419958 0.5486827033218786 0.6708229426433915\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1203 481 0.399833748960931\n",
      "sensitive attribute  4\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7231503579952268 0.9181818181818182 0.8805346700083542\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1197 419 0.35004177109440265\n",
      "sensitive attribute  5\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8780487804878049 0.7682926829268293 0.8456189151599444\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "719 287 0.3991655076495132\n",
      "sensitive attribute  6\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8646003262642741 0.6057142857142858 0.7453896490184414\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1681 613 0.36466389054134446\n",
      "sensitive attribute  7\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9378787878787879 0.6794731064763996 0.8058309037900875\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1715 660 0.3848396501457726\n",
      "sensitive attribute  8\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.6791666666666667 0.5582191780821918 0.6992700729927007\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "685 240 0.35036496350364965\n",
      "data acceptance rates\n",
      "[0.3510028653295129, 0.38484136310223266, 0.399833748960931, 0.35004177109440265, 0.3991655076495132, 0.36466389054134446, 0.3848396501457726, 0.35036496350364965]\n",
      "data DP\n",
      "0.049791977866528336\n",
      "total accepted \n",
      "finalprecision 0.8688888888888889\n",
      "finalrecall 0.6500415627597672\n",
      "finalaccuracy 0.7754166666666666\n",
      "dimension of data\n",
      "8 2400\n",
      "Optimal\n",
      "discripency is:\n",
      "0.63333333\n",
      "gamma-epsilon-delta 0.35 0.15 1\n",
      "################################\n",
      "sensitive attribute  1\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9966216216216216 0.8082191780821918 0.8982808022922636\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "698 296 0.42406876790830944\n",
      "sensitive attribute  2\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8138801261829653 0.6157517899761337 0.7414806110458284\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1702 634 0.372502937720329\n",
      "sensitive attribute  3\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9960861056751468 0.5830469644902635 0.6957605985037406\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1203 511 0.4247714048212801\n",
      "sensitive attribute  4\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.720763723150358 0.9151515151515152 0.8788638262322472\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1197 419 0.35004177109440265\n",
      "sensitive attribute  5\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8704318936877077 0.7987804878048781 0.8539638386648123\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "719 301 0.41863699582753827\n",
      "sensitive attribute  6\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8728139904610492 0.6274285714285714 0.7584770969660916\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1681 629 0.37418203450327187\n",
      "sensitive attribute  7\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9295774647887324 0.6520307354555434 0.7889212827988338\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1715 639 0.37259475218658894\n",
      "sensitive attribute  8\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7457044673539519 0.7431506849315068 0.7824817518248175\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "685 291 0.4248175182481752\n",
      "data acceptance rates\n",
      "[0.42406876790830944, 0.372502937720329, 0.4247714048212801, 0.35004177109440265, 0.41863699582753827, 0.37418203450327187, 0.37259475218658894, 0.4248175182481752]\n",
      "data DP\n",
      "0.07477574715377255\n",
      "total accepted \n",
      "finalprecision 0.8720430107526882\n",
      "finalrecall 0.6741479634247715\n",
      "finalaccuracy 0.7870833333333334\n",
      "dimension of data\n",
      "8 2400\n",
      "Optimal\n",
      "discripency is:\n",
      "0.65833333\n",
      "gamma-epsilon-delta 0.35 0.2 1\n",
      "################################\n",
      "sensitive attribute  1\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9968152866242038 0.8575342465753425 0.9240687679083095\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "698 314 0.4498567335243553\n",
      "sensitive attribute  2\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8188854489164087 0.6312649164677804 0.7497062279670975\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1702 646 0.3795534665099882\n",
      "sensitive attribute  3\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9981515711645101 0.6185567010309279 0.7223607647547797\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1203 541 0.44970906068162925\n",
      "sensitive attribute  4\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.720763723150358 0.9151515151515152 0.8788638262322472\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1197 419 0.35004177109440265\n",
      "sensitive attribute  5\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8703071672354948 0.7774390243902439 0.8456189151599444\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "719 293 0.40751043115438107\n",
      "sensitive attribute  6\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8800599700149925 0.6708571428571428 0.7810826888756692\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1681 667 0.3967876264128495\n",
      "sensitive attribute  7\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9321533923303835 0.6937431394072447 0.8104956268221575\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1715 678 0.39533527696793\n",
      "sensitive attribute  8\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7446808510638298 0.7191780821917808 0.7751824817518248\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "685 282 0.4116788321167883\n",
      "data acceptance rates\n",
      "[0.4498567335243553, 0.3795534665099882, 0.44970906068162925, 0.35004177109440265, 0.40751043115438107, 0.3967876264128495, 0.39533527696793, 0.4116788321167883]\n",
      "data DP\n",
      "0.09981496242995264\n",
      "total accepted \n",
      "finalprecision 0.8770833333333333\n",
      "finalrecall 0.6999168744804655\n",
      "finalaccuracy 0.8004166666666667\n",
      "dimension of data\n",
      "8 2400\n",
      "Optimal\n",
      "discripency is:\n",
      "0.70833333\n",
      "gamma-epsilon-delta 0.35 0.3 1\n",
      "################################\n",
      "sensitive attribute  1\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9856733524355301 0.9424657534246575 0.9627507163323782\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "698 349 0.5\n",
      "sensitive attribute  2\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8286140089418778 0.6634844868735084 0.7667450058754407\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1702 671 0.39424206815511165\n",
      "sensitive attribute  3\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9966722129783694 0.6861397479954181 0.770573566084788\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1203 601 0.4995843724023275\n",
      "sensitive attribute  4\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7183770883054893 0.9121212121212121 0.8771929824561403\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1197 419 0.35004177109440265\n",
      "sensitive attribute  5\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8835227272727273 0.948170731707317 0.9193324061196105\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "719 352 0.48956884561891517\n",
      "sensitive attribute  6\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8817365269461078 0.6731428571428572 0.7828673408685306\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1681 668 0.39738251041046996\n",
      "sensitive attribute  7\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9365781710914455 0.6970362239297475 0.8139941690962099\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1715 678 0.39533527696793\n",
      "sensitive attribute  8\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7748538011695907 0.9075342465753424 0.8481751824817518\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "685 342 0.4992700729927007\n",
      "data acceptance rates\n",
      "[0.5, 0.39424206815511165, 0.4995843724023275, 0.35004177109440265, 0.48956884561891517, 0.39738251041046996, 0.39533527696793, 0.4992700729927007]\n",
      "data DP\n",
      "0.14995822890559735\n",
      "total accepted \n",
      "finalprecision 0.8823529411764706\n",
      "finalrecall 0.7481296758104738\n",
      "finalaccuracy 0.82375\n",
      "dimension of data\n",
      "8 2400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal\n",
      "discripency is:\n",
      "0.75833333\n",
      "gamma-epsilon-delta 0.35 0.4 1\n",
      "################################\n",
      "sensitive attribute  1\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8981723237597912 0.9424657534246575 0.9140401146131805\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "698 383 0.5487106017191977\n",
      "sensitive attribute  2\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8837876614060258 0.7350835322195705 0.8219741480611046\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1702 697 0.4095182138660399\n",
      "sensitive attribute  3\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9969742813918305 0.7548682703321878 0.8204488778054863\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1203 661 0.5494596841230258\n",
      "sensitive attribute  4\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7183770883054893 0.9121212121212121 0.8771929824561403\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1197 419 0.35004177109440265\n",
      "sensitive attribute  5\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8835227272727273 0.948170731707317 0.9193324061196105\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "719 352 0.48956884561891517\n",
      "sensitive attribute  6\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8914835164835165 0.7417142857142857 0.8185603807257584\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1681 728 0.4330755502676978\n",
      "sensitive attribute  7\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9872159090909091 0.7628979143798024 0.8688046647230321\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1715 704 0.41049562682215746\n",
      "sensitive attribute  8\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7047872340425532 0.9075342465753424 0.7985401459854015\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "685 376 0.5489051094890511\n",
      "data acceptance rates\n",
      "[0.5487106017191977, 0.4095182138660399, 0.5494596841230258, 0.35004177109440265, 0.48956884561891517, 0.4330755502676978, 0.41049562682215746, 0.5489051094890511]\n",
      "data DP\n",
      "0.19941791302862316\n",
      "total accepted \n",
      "finalprecision 0.8888888888888888\n",
      "finalrecall 0.7980049875311721\n",
      "finalaccuracy 0.84875\n",
      "dimension of data\n",
      "8 2400\n",
      "Optimal\n",
      "discripency is:\n",
      "0.80833333\n",
      "gamma-epsilon-delta 0.35 0.5 1\n",
      "################################\n",
      "sensitive attribute  1\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8333333333333334 0.6301369863013698 0.7406876790830945\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "698 276 0.3954154727793696\n",
      "sensitive attribute  2\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9120370370370371 0.9403341288782816 0.9259694477085781\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1702 864 0.5076380728554641\n",
      "sensitive attribute  3\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9958391123439667 0.8224513172966781 0.8686616791354946\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1203 721 0.5993349958437241\n",
      "sensitive attribute  4\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7159904534606205 0.9090909090909091 0.8755221386800334\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1197 419 0.35004177109440265\n",
      "sensitive attribute  5\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.823076923076923 0.6524390243902439 0.7774687065368567\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "719 260 0.3616133518776078\n",
      "sensitive attribute  6\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9136363636363637 0.9188571428571428 0.9125520523497918\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1681 880 0.5234979179060083\n",
      "sensitive attribute  7\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.989501312335958 0.827661909989023 0.9037900874635568\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1715 762 0.4443148688046647\n",
      "sensitive attribute  8\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.6984126984126984 0.9041095890410958 0.7927007299270074\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "685 378 0.5518248175182482\n",
      "data acceptance rates\n",
      "[0.3954154727793696, 0.5076380728554641, 0.5993349958437241, 0.35004177109440265, 0.3616133518776078, 0.5234979179060083, 0.4443148688046647, 0.5518248175182482]\n",
      "data DP\n",
      "0.24929322474932142\n",
      "total accepted \n",
      "finalprecision 0.8929824561403509\n",
      "finalrecall 0.8462177888611804\n",
      "finalaccuracy 0.8720833333333333\n",
      "dimension of data\n",
      "8 2400\n",
      "Optimal\n",
      "discripency is:\n",
      "0.85833333\n",
      "gamma-epsilon-delta 0.35 0.6 1\n",
      "################################\n",
      "sensitive attribute  1\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8021978021978022 0.8 0.7922636103151862\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "698 364 0.5214899713467048\n",
      "sensitive attribute  2\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9401913875598086 0.9379474940334129 0.9400705052878966\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1702 836 0.491186839012926\n",
      "sensitive attribute  3\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9961587708066582 0.8911798396334479 0.9185369908561929\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1203 781 0.6492103075644223\n",
      "sensitive attribute  4\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7159904534606205 0.9090909090909091 0.8755221386800334\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1197 419 0.35004177109440265\n",
      "sensitive attribute  5\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7824773413897281 0.7896341463414634 0.803894297635605\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "719 331 0.4603616133518776\n",
      "sensitive attribute  6\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9424626006904487 0.936 0.9369422962522308\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1681 869 0.5169541939321832\n",
      "sensitive attribute  7\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9599528857479388 0.8946212952799122 0.924198250728863\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1715 849 0.49504373177842564\n",
      "sensitive attribute  8\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7492877492877493 0.9006849315068494 0.8291970802919708\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "685 351 0.5124087591240876\n",
      "data acceptance rates\n",
      "[0.5214899713467048, 0.491186839012926, 0.6492103075644223, 0.35004177109440265, 0.4603616133518776, 0.5169541939321832, 0.49504373177842564, 0.5124087591240876]\n",
      "data DP\n",
      "0.2991685364700197\n",
      "total accepted \n",
      "finalprecision 0.8983333333333333\n",
      "finalrecall 0.8960931005818786\n",
      "finalaccuracy 0.8970833333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/subham/anaconda3/envs/tensorflow/lib/python3.6/site-packages/pandas/core/indexing.py:671: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "/home/subham/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/subham/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0     1     2     3     4     5     6     7     8     9     ...  2390  \\\n",
      "x1_0.0     0     0     0     1     1     0     0     0     0     0  ...     0   \n",
      "x1_1.0     1     1     1     0     0     1     1     1     1     1  ...     1   \n",
      "x2_0.0     0     1     1     0     0     1     1     1     0     1  ...     1   \n",
      "x2_1.0     1     0     0     1     1     0     0     0     1     0  ...     0   \n",
      "x3_0.0     0     1     0     0     0     0     0     1     0     0  ...     1   \n",
      "x3_1.0     1     0     1     1     1     1     1     0     1     1  ...     0   \n",
      "x4_0.0     1     1     0     1     1     1     0     1     1     1  ...     0   \n",
      "x4_1.0     0     0     1     0     0     0     1     0     0     0  ...     1   \n",
      "x5_0.0     1     0     1     1     1     1     1     0     1     0  ...     0   \n",
      "x5_1.0     0     1     0     0     0     0     0     1     0     1  ...     1   \n",
      "x6_0.0     0     1     0     0     1     0     0     1     0     0  ...     0   \n",
      "x6_1.0     1     0     1     1     0     1     1     0     1     1  ...     1   \n",
      "\n",
      "        2391  2392  2393  2394  2395  2396  2397  2398  2399  \n",
      "x1_0.0     0     0     1     0     0     0     0     0     0  \n",
      "x1_1.0     1     1     0     1     1     1     1     1     1  \n",
      "x2_0.0     1     1     1     1     0     0     1     0     0  \n",
      "x2_1.0     0     0     0     0     1     1     0     1     1  \n",
      "x3_0.0     0     0     1     0     0     0     0     0     0  \n",
      "x3_1.0     1     1     0     1     1     1     1     1     1  \n",
      "x4_0.0     1     0     1     1     1     1     1     1     1  \n",
      "x4_1.0     0     1     0     0     0     0     0     0     0  \n",
      "x5_0.0     1     0     0     0     1     0     1     1     1  \n",
      "x5_1.0     0     1     1     1     0     1     0     0     0  \n",
      "x6_0.0     0     0     0     0     0     1     0     1     0  \n",
      "x6_1.0     1     1     1     1     1     0     1     0     1  \n",
      "\n",
      "[12 rows x 2400 columns]\n",
      "sensitive attribute  1\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "698\n",
      "365\n",
      "0.5229226361031518\n",
      "sensitive attribute  2\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1702\n",
      "838\n",
      "0.49236192714453586\n",
      "sensitive attribute  3\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1203\n",
      "873\n",
      "0.7256857855361596\n",
      "sensitive attribute  4\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1197\n",
      "330\n",
      "0.2756892230576441\n",
      "sensitive attribute  5\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "719\n",
      "328\n",
      "0.4561891515994437\n",
      "sensitive attribute  6\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1681\n",
      "875\n",
      "0.520523497917906\n",
      "sensitive attribute  7\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1715\n",
      "911\n",
      "0.5311953352769679\n",
      "sensitive attribute  8\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "685\n",
      "292\n",
      "0.42627737226277373\n",
      "sensitive attribute  9\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1188\n",
      "359\n",
      "0.3021885521885522\n",
      "sensitive attribute  10\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1212\n",
      "844\n",
      "0.6963696369636964\n",
      "sensitive attribute  11\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "592\n",
      "282\n",
      "0.47635135135135137\n",
      "sensitive attribute  12\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1808\n",
      "921\n",
      "0.5094026548672567\n",
      "data acceptance rates\n",
      "[0.5229226361031518, 0.49236192714453586, 0.7256857855361596, 0.2756892230576441, 0.4561891515994437, 0.520523497917906, 0.5311953352769679, 0.42627737226277373, 0.3021885521885522, 0.6963696369636964, 0.47635135135135137, 0.5094026548672567]\n",
      "data DP\n",
      "0.4499965624785155\n",
      "sensitive attribute  1\n",
      "prec reca accuracy for each sens\n",
      "0.9971014492753624 0.9424657534246575 0.9684813753581661\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "698\n",
      "345\n",
      "0.49426934097421205\n",
      "sensitive attribute  2\n",
      "prec reca accuracy for each sens\n",
      "0.9886506935687264 0.9355608591885441 0.9629847238542891\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1702\n",
      "793\n",
      "0.46592244418331374\n",
      "sensitive attribute  3\n",
      "prec reca accuracy for each sens\n",
      "0.9964071856287425 0.9530355097365406 0.9634247714048213\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1203\n",
      "835\n",
      "0.6940980881130507\n",
      "sensitive attribute  4\n",
      "prec reca accuracy for each sens\n",
      "0.976897689768977 0.896969696969697 0.9657477025898078\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1197\n",
      "303\n",
      "0.2531328320802005\n",
      "sensitive attribute  5\n",
      "prec reca accuracy for each sens\n",
      "0.9967948717948718 0.948170731707317 0.9749652294853964\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "719\n",
      "312\n",
      "0.4339360222531293\n",
      "sensitive attribute  6\n",
      "prec reca accuracy for each sens\n",
      "0.9891041162227603 0.9337142857142857 0.9601427721594289\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1681\n",
      "826\n",
      "0.4913741820345033\n",
      "sensitive attribute  7\n",
      "prec reca accuracy for each sens\n",
      "0.9908675799086758 0.9527991218441273 0.970262390670554\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1715\n",
      "876\n",
      "0.5107871720116618\n",
      "sensitive attribute  8\n",
      "prec reca accuracy for each sens\n",
      "0.9923664122137404 0.8904109589041096 0.9503649635036496\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "685\n",
      "262\n",
      "0.38248175182481753\n",
      "sensitive attribute  9\n",
      "prec reca accuracy for each sens\n",
      "0.9783950617283951 0.883008356545961 0.9587542087542088\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1188\n",
      "324\n",
      "0.2727272727272727\n",
      "sensitive attribute  10\n",
      "prec reca accuracy for each sens\n",
      "0.9963144963144963 0.9609004739336493 0.9702970297029703\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1212\n",
      "814\n",
      "0.6716171617161716\n",
      "sensitive attribute  11\n",
      "prec reca accuracy for each sens\n",
      "0.9961685823754789 0.9219858156028369 0.9611486486486487\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "592\n",
      "261\n",
      "0.4408783783783784\n",
      "sensitive attribute  12\n",
      "prec reca accuracy for each sens\n",
      "0.9897377423033067 0.9424538545059717 0.9657079646017699\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1808\n",
      "877\n",
      "0.4850663716814159\n",
      "data acceptance rates\n",
      "[0.49426934097421205, 0.46592244418331374, 0.6940980881130507, 0.2531328320802005, 0.4339360222531293, 0.4913741820345033, 0.5107871720116618, 0.38248175182481753, 0.2727272727272727, 0.6716171617161716, 0.4408783783783784, 0.4850663716814159]\n",
      "data DP\n",
      "0.44096525603285025\n",
      "SVM accuracy--------------------------\n",
      "0.9912126537785588 0.9376558603491272 0.9645833333333333\n",
      "precision recall accuracy\n",
      "dimension of data\n",
      "12 2400\n",
      "Optimal\n",
      "discripency is:\n",
      "0.56833333\n",
      "gamma-epsilon-delta 0.35 0.02 1\n",
      "################################\n",
      "sensitive attribute  1\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7529880478087649 0.5178082191780822 0.6590257879656161\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "698 251 0.35959885386819485\n",
      "sensitive attribute  2\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9184692179700499 0.6587112171837709 0.8031727379553466\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1702 601 0.35311398354876616\n",
      "sensitive attribute  3\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9976905311778291 0.4948453608247423 0.6325852036575229\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1203 433 0.35993349958437243\n",
      "sensitive attribute  4\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7374701670644391 0.9363636363636364 0.8905597326649958\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1197 419 0.35004177109440265\n",
      "sensitive attribute  5\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.933852140077821 0.7317073170731707 0.8539638386648123\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "719 257 0.3574408901251738\n",
      "sensitive attribute  6\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8420168067226891 0.5725714285714286 0.7215942891136229\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1681 595 0.3539559785841761\n",
      "sensitive attribute  7\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9297385620915033 0.6245883644346871 0.7755102040816326\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1715 612 0.3568513119533528\n",
      "sensitive attribute  8\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7166666666666667 0.589041095890411 0.7255474452554744\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "685 240 0.35036496350364965\n",
      "sensitive attribute  9\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.75 0.8690807799442897 0.872895622895623\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1188 416 0.3501683501683502\n",
      "sensitive attribute  10\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9839449541284404 0.5082938388625592 0.6518151815181518\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1212 436 0.35973597359735976\n",
      "sensitive attribute  11\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9375 0.6914893617021277 0.831081081081081\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "592 208 0.35135135135135137\n",
      "sensitive attribute  12\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8478260869565217 0.5928338762214984 0.7383849557522124\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1808 644 0.3561946902654867\n",
      "data acceptance rates\n",
      "[0.35959885386819485, 0.35311398354876616, 0.35993349958437243, 0.35004177109440265, 0.3574408901251738, 0.3539559785841761, 0.3568513119533528, 0.35036496350364965, 0.3501683501683502, 0.35973597359735976, 0.35135135135135137, 0.3561946902654867]\n",
      "data DP\n",
      "0.009891728489969775\n",
      "total accepted \n",
      "finalprecision 0.8697183098591549\n",
      "finalrecall 0.6159600997506235\n",
      "finalaccuracy 0.76125\n",
      "dimension of data\n",
      "12 2400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal\n",
      "discripency is:\n",
      "0.57833333\n",
      "gamma-epsilon-delta 0.35 0.04 1\n",
      "################################\n",
      "sensitive attribute  1\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8410852713178295 0.5945205479452055 0.7292263610315186\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "698 258 0.36962750716332377\n",
      "sensitive attribute  2\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8729372937293729 0.6312649164677804 0.7732079905992949\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1702 606 0.35605170387779084\n",
      "sensitive attribute  3\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9955056179775281 0.5074455899198167 0.6408977556109726\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1203 445 0.36990856192851207\n",
      "sensitive attribute  4\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7231503579952268 0.9181818181818182 0.8805346700083542\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1197 419 0.35004177109440265\n",
      "sensitive attribute  5\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7518796992481203 0.6097560975609756 0.7301808066759388\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "719 266 0.36995827538247567\n",
      "sensitive attribute  6\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9130434782608695 0.624 0.7733491969066032\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1681 598 0.3557406305770375\n",
      "sensitive attribute  7\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8685897435897436 0.5949506037321625 0.7370262390670554\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1715 624 0.3638483965014577\n",
      "sensitive attribute  8\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.85 0.6986301369863014 0.8189781021897811\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "685 240 0.35036496350364965\n",
      "sensitive attribute  9\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7548076923076923 0.8746518105849582 0.8762626262626263\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1188 416 0.3501683501683502\n",
      "sensitive attribute  10\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9642857142857143 0.5118483412322274 0.6468646864686468\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1212 448 0.3696369636963696\n",
      "sensitive attribute  11\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8447488584474886 0.6560283687943262 0.7787162162162162\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "592 219 0.36993243243243246\n",
      "sensitive attribute  12\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8697674418604651 0.6091205211726385 0.754424778761062\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1808 645 0.3567477876106195\n",
      "data acceptance rates\n",
      "[0.36962750716332377, 0.35605170387779084, 0.36990856192851207, 0.35004177109440265, 0.36995827538247567, 0.3557406305770375, 0.3638483965014577, 0.35036496350364965, 0.3501683501683502, 0.3696369636963696, 0.36993243243243246, 0.3567477876106195]\n",
      "data DP\n",
      "0.01991650428807301\n",
      "total accepted \n",
      "finalprecision 0.8634259259259259\n",
      "finalrecall 0.6201163757273483\n",
      "finalaccuracy 0.7604166666666666\n",
      "dimension of data\n",
      "12 2400\n",
      "Optimal\n",
      "discripency is:\n",
      "0.60833333\n",
      "gamma-epsilon-delta 0.35 0.1 1\n",
      "################################\n",
      "sensitive attribute  1\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8040816326530612 0.5397260273972603 0.6905444126074498\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "698 245 0.3510028653295129\n",
      "sensitive attribute  2\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8900763358778626 0.6957040572792362 0.8078730904817861\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1702 655 0.38484136310223266\n",
      "sensitive attribute  3\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.997920997920998 0.5498281786941581 0.6724854530340815\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1203 481 0.399833748960931\n",
      "sensitive attribute  4\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7159904534606205 0.9090909090909091 0.8755221386800334\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1197 419 0.35004177109440265\n",
      "sensitive attribute  5\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7470355731225297 0.5762195121951219 0.717663421418637\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "719 253 0.3518776077885953\n",
      "sensitive attribute  6\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9134466769706336 0.6754285714285714 0.7977394408090422\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1681 647 0.3848899464604402\n",
      "sensitive attribute  7\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9330218068535826 0.6575192096597146 0.793002915451895\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1715 642 0.37434402332361516\n",
      "sensitive attribute  8\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7015503875968992 0.6198630136986302 0.7255474452554744\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "685 258 0.37664233576642336\n",
      "sensitive attribute  9\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7548076923076923 0.8746518105849582 0.8762626262626263\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1188 416 0.3501683501683502\n",
      "sensitive attribute  10\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9628099173553719 0.5521327014218009 0.6732673267326733\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1212 484 0.39933993399339934\n",
      "sensitive attribute  11\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9134615384615384 0.6737588652482269 0.8141891891891891\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "592 208 0.35135135135135137\n",
      "sensitive attribute  12\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8526011560693642 0.6406080347448425 0.7605088495575221\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1808 692 0.3827433628318584\n",
      "data acceptance rates\n",
      "[0.3510028653295129, 0.38484136310223266, 0.399833748960931, 0.35004177109440265, 0.3518776077885953, 0.3848899464604402, 0.37434402332361516, 0.37664233576642336, 0.3501683501683502, 0.39933993399339934, 0.35135135135135137, 0.3827433628318584]\n",
      "data DP\n",
      "0.049791977866528336\n",
      "total accepted \n",
      "finalprecision 0.8666666666666667\n",
      "finalrecall 0.6483790523690773\n",
      "finalaccuracy 0.77375\n",
      "dimension of data\n",
      "12 2400\n",
      "Optimal\n",
      "discripency is:\n",
      "0.63333333\n",
      "gamma-epsilon-delta 0.35 0.15 1\n",
      "################################\n",
      "sensitive attribute  1\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7210884353741497 0.5808219178082191 0.663323782234957\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "698 294 0.42120343839541546\n",
      "sensitive attribute  2\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9512578616352201 0.7219570405727923 0.8448883666274971\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1702 636 0.3736780258519389\n",
      "sensitive attribute  3\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9960861056751468 0.5830469644902635 0.6957605985037406\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1203 511 0.4247714048212801\n",
      "sensitive attribute  4\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7350835322195705 0.9333333333333333 0.8888888888888888\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1197 419 0.35004177109440265\n",
      "sensitive attribute  5\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9049180327868852 0.8414634146341463 0.8873435326842837\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "719 305 0.4242002781641168\n",
      "sensitive attribute  6\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8656 0.6182857142857143 0.7513384889946461\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1681 625 0.37180249851279\n",
      "sensitive attribute  7\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9530516431924883 0.6684961580680571 0.8064139941690962\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1715 639 0.37259475218658894\n",
      "sensitive attribute  8\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7147766323024055 0.7123287671232876 0.7562043795620438\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "685 291 0.4248175182481752\n",
      "sensitive attribute  9\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7475961538461539 0.8662952646239555 0.8712121212121212\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1188 416 0.3501683501683502\n",
      "sensitive attribute  10\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9844357976653697 0.5995260663507109 0.7145214521452146\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1212 514 0.4240924092409241\n",
      "sensitive attribute  11\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9369369369369369 0.7375886524822695 0.8513513513513513\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "592 222 0.375\n",
      "sensitive attribute  12\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8601694915254238 0.6612377850162866 0.7726769911504425\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1808 708 0.3915929203539823\n",
      "data acceptance rates\n",
      "[0.42120343839541546, 0.3736780258519389, 0.4247714048212801, 0.35004177109440265, 0.4242002781641168, 0.37180249851279, 0.37259475218658894, 0.4248175182481752, 0.3501683501683502, 0.4240924092409241, 0.375, 0.3915929203539823]\n",
      "data DP\n",
      "0.07477574715377255\n",
      "total accepted \n",
      "finalprecision 0.878494623655914\n",
      "finalrecall 0.6791354945968412\n",
      "finalaccuracy 0.7920833333333334\n",
      "dimension of data\n",
      "12 2400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal\n",
      "discripency is:\n",
      "0.65833333\n",
      "gamma-epsilon-delta 0.35 0.2 1\n",
      "################################\n",
      "sensitive attribute  1\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9745222929936306 0.8383561643835616 0.9040114613180515\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "698 314 0.4498567335243553\n",
      "sensitive attribute  2\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8297213622291022 0.639618138424821 0.7579318448883666\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1702 646 0.3795534665099882\n",
      "sensitive attribute  3\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9963031423290203 0.6174112256586484 0.7206982543640897\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1203 541 0.44970906068162925\n",
      "sensitive attribute  4\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7231503579952268 0.9181818181818182 0.8805346700083542\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1197 419 0.35004177109440265\n",
      "sensitive attribute  5\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7142857142857143 0.6859756097560976 0.7315716272600834\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "719 315 0.4381084840055633\n",
      "sensitive attribute  6\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9565891472868217 0.7051428571428572 0.8298631766805473\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1681 645 0.38370017846519927\n",
      "sensitive attribute  7\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.875 0.6684961580680571 0.7731778425655976\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1715 696 0.40583090379008746\n",
      "sensitive attribute  8\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8825757575757576 0.797945205479452 0.8686131386861314\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "685 264 0.3854014598540146\n",
      "sensitive attribute  9\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7637231503579952 0.8913649025069638 0.8838383838383839\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1188 419 0.35269360269360267\n",
      "sensitive attribute  10\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9648798521256932 0.6184834123222749 0.7186468646864687\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1212 541 0.44636963696369636\n",
      "sensitive attribute  11\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8266129032258065 0.7269503546099291 0.7972972972972973\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "592 248 0.4189189189189189\n",
      "sensitive attribute  12\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8946629213483146 0.6916395222584147 0.8014380530973452\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1808 712 0.3938053097345133\n",
      "data acceptance rates\n",
      "[0.4498567335243553, 0.3795534665099882, 0.44970906068162925, 0.35004177109440265, 0.4381084840055633, 0.38370017846519927, 0.40583090379008746, 0.3854014598540146, 0.35269360269360267, 0.44636963696369636, 0.4189189189189189, 0.3938053097345133]\n",
      "data DP\n",
      "0.09981496242995264\n",
      "total accepted \n",
      "finalprecision 0.8770833333333333\n",
      "finalrecall 0.6999168744804655\n",
      "finalaccuracy 0.8004166666666667\n",
      "dimension of data\n",
      "12 2400\n",
      "Optimal\n",
      "discripency is:\n",
      "0.70833333\n",
      "gamma-epsilon-delta 0.35 0.3 1\n",
      "################################\n",
      "sensitive attribute  1\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8710601719197708 0.8328767123287671 0.8481375358166189\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "698 349 0.5\n",
      "sensitive attribute  2\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8852459016393442 0.7088305489260143 0.8113983548766157\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1702 671 0.39424206815511165\n",
      "sensitive attribute  3\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9950083194675541 0.6849942726231386 0.7689110556940981\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1203 601 0.4995843724023275\n",
      "sensitive attribute  4\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7159904534606205 0.9090909090909091 0.8755221386800334\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1197 419 0.35004177109440265\n",
      "sensitive attribute  5\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7632311977715878 0.8353658536585366 0.8066759388038943\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "719 359 0.4993045897079277\n",
      "sensitive attribute  6\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9440242057488654 0.7131428571428572 0.8286734086853064\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1681 661 0.39321832242712673\n",
      "sensitive attribute  7\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9216255442670537 0.6970362239297475 0.8075801749271136\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1715 689 0.40174927113702624\n",
      "sensitive attribute  8\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7945619335347432 0.9006849315068494 0.8583941605839416\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "685 331 0.4832116788321168\n",
      "sensitive attribute  9\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7548076923076923 0.8746518105849582 0.8762626262626263\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1188 416 0.3501683501683502\n",
      "sensitive attribute  10\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9668874172185431 0.6919431279620853 0.768976897689769\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1212 604 0.49834983498349833\n",
      "sensitive attribute  11\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8695652173913043 0.851063829787234 0.8682432432432432\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "592 276 0.46621621621621623\n",
      "sensitive attribute  12\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8844086021505376 0.7144408251900108 0.8069690265486725\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1808 744 0.41150442477876104\n",
      "data acceptance rates\n",
      "[0.5, 0.39424206815511165, 0.4995843724023275, 0.35004177109440265, 0.4993045897079277, 0.39321832242712673, 0.40174927113702624, 0.4832116788321168, 0.3501683501683502, 0.49834983498349833, 0.46621621621621623, 0.41150442477876104]\n",
      "data DP\n",
      "0.14995822890559735\n",
      "total accepted \n",
      "finalprecision 0.8803921568627451\n",
      "finalrecall 0.7464671654197839\n",
      "finalaccuracy 0.8220833333333334\n",
      "dimension of data\n",
      "12 2400\n",
      "Optimal\n",
      "discripency is:\n",
      "0.75833333\n",
      "gamma-epsilon-delta 0.35 0.4 1\n",
      "################################\n",
      "sensitive attribute  1\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8120104438642297 0.852054794520548 0.8194842406876791\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "698 383 0.5487106017191977\n",
      "sensitive attribute  2\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9296987087517934 0.7732696897374701 0.8595769682726204\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1702 697 0.4095182138660399\n",
      "sensitive attribute  3\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9969742813918305 0.7548682703321878 0.8204488778054863\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1203 661 0.5494596841230258\n",
      "sensitive attribute  4\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7159904534606205 0.9090909090909091 0.8755221386800334\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1197 419 0.35004177109440265\n",
      "sensitive attribute  5\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8885714285714286 0.948170731707317 0.9221140472878998\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "719 350 0.48678720445062584\n",
      "sensitive attribute  6\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8876712328767123 0.7405714285714285 0.8161808447352766\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1681 730 0.43426531826293874\n",
      "sensitive attribute  7\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9744623655913979 0.7958287596048299 0.880466472303207\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1715 744 0.43381924198250726\n",
      "sensitive attribute  8\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.6964285714285714 0.8013698630136986 0.7664233576642335\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "685 336 0.4905109489051095\n",
      "sensitive attribute  9\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7588652482269503 0.8941504178272981 0.8821548821548821\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1188 423 0.3560606060606061\n",
      "sensitive attribute  10\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9710806697108066 0.7559241706161137 0.8143564356435643\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1212 657 0.5420792079207921\n",
      "sensitive attribute  11\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8782608695652174 0.7163120567375887 0.8175675675675675\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "592 230 0.3885135135135135\n",
      "sensitive attribute  12\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8905882352941177 0.8219326818675353 0.8578539823008849\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1808 850 0.47013274336283184\n",
      "data acceptance rates\n",
      "[0.5487106017191977, 0.4095182138660399, 0.5494596841230258, 0.35004177109440265, 0.48678720445062584, 0.43426531826293874, 0.43381924198250726, 0.4905109489051095, 0.3560606060606061, 0.5420792079207921, 0.3885135135135135, 0.47013274336283184]\n",
      "data DP\n",
      "0.19941791302862316\n",
      "total accepted \n",
      "finalprecision 0.887962962962963\n",
      "finalrecall 0.7971737323358271\n",
      "finalaccuracy 0.8479166666666667\n",
      "dimension of data\n",
      "12 2400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal\n",
      "discripency is:\n",
      "0.80833333\n",
      "gamma-epsilon-delta 0.35 0.5 1\n",
      "################################\n",
      "sensitive attribute  1\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8086124401913876 0.9260273972602739 0.8467048710601719\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "698 418 0.5988538681948424\n",
      "sensitive attribute  2\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9376731301939059 0.8078758949880668 0.8789659224441834\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1702 722 0.42420681551116335\n",
      "sensitive attribute  3\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9958391123439667 0.8224513172966781 0.8686616791354946\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1203 721 0.5993349958437241\n",
      "sensitive attribute  4\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7088305489260143 0.9 0.8705096073517126\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1197 419 0.35004177109440265\n",
      "sensitive attribute  5\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8466898954703833 0.7408536585365854 0.8205841446453408\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "719 287 0.3991655076495132\n",
      "sensitive attribute  6\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9050410316529894 0.8822857142857143 0.8905413444378346\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1681 853 0.5074360499702558\n",
      "sensitive attribute  7\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9696233292831106 0.8759604829857299 0.919533527696793\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1715 823 0.47988338192419827\n",
      "sensitive attribute  8\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.6845425867507886 0.7431506849315068 0.7445255474452555\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "685 317 0.46277372262773725\n",
      "sensitive attribute  9\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7475961538461539 0.8662952646239555 0.8712121212121212\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1188 416 0.3501683501683502\n",
      "sensitive attribute  10\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9723756906077348 0.8341232227488151 0.8679867986798679\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1212 724 0.5973597359735974\n",
      "sensitive attribute  11\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8767605633802817 0.8829787234042553 0.8851351351351351\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "592 284 0.4797297297297297\n",
      "sensitive attribute  12\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8948598130841121 0.8317046688382194 0.8644911504424779\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1808 856 0.47345132743362833\n",
      "data acceptance rates\n",
      "[0.5988538681948424, 0.42420681551116335, 0.5993349958437241, 0.35004177109440265, 0.3991655076495132, 0.5074360499702558, 0.47988338192419827, 0.46277372262773725, 0.3501683501683502, 0.5973597359735974, 0.4797297297297297, 0.47345132743362833]\n",
      "data DP\n",
      "0.24929322474932142\n",
      "total accepted \n",
      "finalprecision 0.8903508771929824\n",
      "finalrecall 0.8437240232751455\n",
      "finalaccuracy 0.8695833333333334\n",
      "dimension of data\n",
      "12 2400\n",
      "Optimal\n",
      "discripency is:\n",
      "0.85833333\n",
      "gamma-epsilon-delta 0.35 0.6 1\n",
      "################################\n",
      "sensitive attribute  1\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7598152424942263 0.9013698630136986 0.7994269340974212\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "698 433 0.6203438395415473\n",
      "sensitive attribute  2\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9726205997392438 0.8902147971360382 0.9336075205640423\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1702 767 0.4506462984723854\n",
      "sensitive attribute  3\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9961587708066582 0.8911798396334479 0.9185369908561929\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1203 781 0.6492103075644223\n",
      "sensitive attribute  4\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7088305489260143 0.9 0.8705096073517126\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1197 419 0.35004177109440265\n",
      "sensitive attribute  5\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9107142857142857 0.9329268292682927 0.9276773296244785\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "719 336 0.4673157162726008\n",
      "sensitive attribute  6\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8900462962962963 0.8788571428571429 0.8804283164782868\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1681 864 0.5139797739440809\n",
      "sensitive attribute  7\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9714937286202965 0.9352360043907794 0.9510204081632653\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1715 877 0.5113702623906705\n",
      "sensitive attribute  8\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.6904024767801857 0.7636986301369864 0.7532846715328467\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "685 323 0.47153284671532847\n",
      "sensitive attribute  9\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7227272727272728 0.8857938718662952 0.8627946127946128\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1188 440 0.37037037037037035\n",
      "sensitive attribute  10\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9960526315789474 0.8969194312796208 0.9257425742574258\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1212 760 0.6270627062706271\n",
      "sensitive attribute  11\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.901060070671378 0.9042553191489362 0.9070945945945946\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "592 283 0.4780405405405405\n",
      "sensitive attribute  12\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8942202835332607 0.8903365906623235 0.8904867256637168\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1808 917 0.5071902654867256\n",
      "data acceptance rates\n",
      "[0.6203438395415473, 0.4506462984723854, 0.6492103075644223, 0.35004177109440265, 0.4673157162726008, 0.5139797739440809, 0.5113702623906705, 0.47153284671532847, 0.37037037037037035, 0.6270627062706271, 0.4780405405405405, 0.5071902654867256]\n",
      "data DP\n",
      "0.2991685364700197\n",
      "total accepted \n",
      "finalprecision 0.8958333333333334\n",
      "finalrecall 0.8935993349958438\n",
      "finalaccuracy 0.8945833333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/subham/anaconda3/envs/tensorflow/lib/python3.6/site-packages/pandas/core/indexing.py:671: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "/home/subham/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/subham/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0     1     2     3     4     5     6     7     8     9     ...  2390  \\\n",
      "x1_0.0     0     0     0     1     1     0     0     0     0     0  ...     0   \n",
      "x1_1.0     1     1     1     0     0     1     1     1     1     1  ...     1   \n",
      "x2_0.0     0     1     1     0     0     1     1     1     0     1  ...     1   \n",
      "x2_1.0     1     0     0     1     1     0     0     0     1     0  ...     0   \n",
      "x3_0.0     0     1     0     0     0     0     0     1     0     0  ...     1   \n",
      "x3_1.0     1     0     1     1     1     1     1     0     1     1  ...     0   \n",
      "x4_0.0     1     1     0     1     1     1     0     1     1     1  ...     0   \n",
      "x4_1.0     0     0     1     0     0     0     1     0     0     0  ...     1   \n",
      "x5_0.0     1     0     1     1     1     1     1     0     1     0  ...     0   \n",
      "x5_1.0     0     1     0     0     0     0     0     1     0     1  ...     1   \n",
      "x6_0.0     0     1     0     0     1     0     0     1     0     0  ...     0   \n",
      "x6_1.0     1     0     1     1     0     1     1     0     1     1  ...     1   \n",
      "x7_0.0     0     0     0     0     0     0     1     0     0     0  ...     0   \n",
      "x7_1.0     1     1     1     1     1     1     0     1     1     1  ...     1   \n",
      "x8_0.0     0     1     0     0     0     1     0     1     0     0  ...     0   \n",
      "x8_1.0     1     0     1     1     1     0     1     0     1     1  ...     1   \n",
      "\n",
      "        2391  2392  2393  2394  2395  2396  2397  2398  2399  \n",
      "x1_0.0     0     0     1     0     0     0     0     0     0  \n",
      "x1_1.0     1     1     0     1     1     1     1     1     1  \n",
      "x2_0.0     1     1     1     1     0     0     1     0     0  \n",
      "x2_1.0     0     0     0     0     1     1     0     1     1  \n",
      "x3_0.0     0     0     1     0     0     0     0     0     0  \n",
      "x3_1.0     1     1     0     1     1     1     1     1     1  \n",
      "x4_0.0     1     0     1     1     1     1     1     1     1  \n",
      "x4_1.0     0     1     0     0     0     0     0     0     0  \n",
      "x5_0.0     1     0     0     0     1     0     1     1     1  \n",
      "x5_1.0     0     1     1     1     0     1     0     0     0  \n",
      "x6_0.0     0     0     0     0     0     1     0     1     0  \n",
      "x6_1.0     1     1     1     1     1     0     1     0     1  \n",
      "x7_0.0     0     0     0     0     0     1     0     1     0  \n",
      "x7_1.0     1     1     1     1     1     0     1     0     1  \n",
      "x8_0.0     1     0     1     0     1     1     0     0     0  \n",
      "x8_1.0     0     1     0     1     0     0     1     1     1  \n",
      "\n",
      "[16 rows x 2400 columns]\n",
      "sensitive attribute  1\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "698\n",
      "365\n",
      "0.5229226361031518\n",
      "sensitive attribute  2\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1702\n",
      "838\n",
      "0.49236192714453586\n",
      "sensitive attribute  3\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1203\n",
      "873\n",
      "0.7256857855361596\n",
      "sensitive attribute  4\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1197\n",
      "330\n",
      "0.2756892230576441\n",
      "sensitive attribute  5\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "719\n",
      "328\n",
      "0.4561891515994437\n",
      "sensitive attribute  6\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1681\n",
      "875\n",
      "0.520523497917906\n",
      "sensitive attribute  7\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1715\n",
      "911\n",
      "0.5311953352769679\n",
      "sensitive attribute  8\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "685\n",
      "292\n",
      "0.42627737226277373\n",
      "sensitive attribute  9\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1188\n",
      "359\n",
      "0.3021885521885522\n",
      "sensitive attribute  10\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1212\n",
      "844\n",
      "0.6963696369636964\n",
      "sensitive attribute  11\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "592\n",
      "282\n",
      "0.47635135135135137\n",
      "sensitive attribute  12\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1808\n",
      "921\n",
      "0.5094026548672567\n",
      "sensitive attribute  13\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "668\n",
      "357\n",
      "0.5344311377245509\n",
      "sensitive attribute  14\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1732\n",
      "846\n",
      "0.4884526558891455\n",
      "sensitive attribute  15\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1155\n",
      "314\n",
      "0.27186147186147186\n",
      "sensitive attribute  16\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1245\n",
      "889\n",
      "0.7140562248995984\n",
      "data acceptance rates\n",
      "[0.5229226361031518, 0.49236192714453586, 0.7256857855361596, 0.2756892230576441, 0.4561891515994437, 0.520523497917906, 0.5311953352769679, 0.42627737226277373, 0.3021885521885522, 0.6963696369636964, 0.47635135135135137, 0.5094026548672567, 0.5344311377245509, 0.4884526558891455, 0.27186147186147186, 0.7140562248995984]\n",
      "data DP\n",
      "0.45382431367468773\n",
      "sensitive attribute  1\n",
      "prec reca accuracy for each sens\n",
      "0.9971014492753624 0.9424657534246575 0.9684813753581661\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "698\n",
      "345\n",
      "0.49426934097421205\n",
      "sensitive attribute  2\n",
      "prec reca accuracy for each sens\n",
      "0.9886506935687264 0.9355608591885441 0.9629847238542891\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1702\n",
      "793\n",
      "0.46592244418331374\n",
      "sensitive attribute  3\n",
      "prec reca accuracy for each sens\n",
      "0.9964071856287425 0.9530355097365406 0.9634247714048213\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1203\n",
      "835\n",
      "0.6940980881130507\n",
      "sensitive attribute  4\n",
      "prec reca accuracy for each sens\n",
      "0.976897689768977 0.896969696969697 0.9657477025898078\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1197\n",
      "303\n",
      "0.2531328320802005\n",
      "sensitive attribute  5\n",
      "prec reca accuracy for each sens\n",
      "0.9967948717948718 0.948170731707317 0.9749652294853964\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "719\n",
      "312\n",
      "0.4339360222531293\n",
      "sensitive attribute  6\n",
      "prec reca accuracy for each sens\n",
      "0.9891041162227603 0.9337142857142857 0.9601427721594289\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1681\n",
      "826\n",
      "0.4913741820345033\n",
      "sensitive attribute  7\n",
      "prec reca accuracy for each sens\n",
      "0.9908675799086758 0.9527991218441273 0.970262390670554\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1715\n",
      "876\n",
      "0.5107871720116618\n",
      "sensitive attribute  8\n",
      "prec reca accuracy for each sens\n",
      "0.9923664122137404 0.8904109589041096 0.9503649635036496\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "685\n",
      "262\n",
      "0.38248175182481753\n",
      "sensitive attribute  9\n",
      "prec reca accuracy for each sens\n",
      "0.9783950617283951 0.883008356545961 0.9587542087542088\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1188\n",
      "324\n",
      "0.2727272727272727\n",
      "sensitive attribute  10\n",
      "prec reca accuracy for each sens\n",
      "0.9963144963144963 0.9609004739336493 0.9702970297029703\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1212\n",
      "814\n",
      "0.6716171617161716\n",
      "sensitive attribute  11\n",
      "prec reca accuracy for each sens\n",
      "0.9961685823754789 0.9219858156028369 0.9611486486486487\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "592\n",
      "261\n",
      "0.4408783783783784\n",
      "sensitive attribute  12\n",
      "prec reca accuracy for each sens\n",
      "0.9897377423033067 0.9424538545059717 0.9657079646017699\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1808\n",
      "877\n",
      "0.4850663716814159\n",
      "sensitive attribute  13\n",
      "prec reca accuracy for each sens\n",
      "0.981651376146789 0.8991596638655462 0.937125748502994\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "668\n",
      "327\n",
      "0.48952095808383234\n",
      "sensitive attribute  14\n",
      "prec reca accuracy for each sens\n",
      "0.9950678175092479 0.9539007092198581 0.9751732101616628\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1732\n",
      "811\n",
      "0.4682448036951501\n",
      "sensitive attribute  15\n",
      "prec reca accuracy for each sens\n",
      "0.9931972789115646 0.9299363057324841 0.9792207792207792\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1155\n",
      "294\n",
      "0.2545454545454545\n",
      "sensitive attribute  16\n",
      "prec reca accuracy for each sens\n",
      "0.990521327014218 0.9403824521934758 0.951004016064257\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1245\n",
      "844\n",
      "0.6779116465863454\n",
      "data acceptance rates\n",
      "[0.49426934097421205, 0.46592244418331374, 0.6940980881130507, 0.2531328320802005, 0.4339360222531293, 0.4913741820345033, 0.5107871720116618, 0.38248175182481753, 0.2727272727272727, 0.6716171617161716, 0.4408783783783784, 0.4850663716814159, 0.48952095808383234, 0.4682448036951501, 0.2545454545454545, 0.6779116465863454]\n",
      "data DP\n",
      "0.44096525603285025\n",
      "SVM accuracy--------------------------\n",
      "0.9912126537785588 0.9376558603491272 0.9645833333333333\n",
      "precision recall accuracy\n",
      "dimension of data\n",
      "16 2400\n",
      "Optimal\n",
      "discripency is:\n",
      "0.555\n",
      "gamma-epsilon-delta 0.35 0.02 1\n",
      "################################\n",
      "sensitive attribute  1\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9233870967741935 0.6273972602739726 0.7779369627507163\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "698 248 0.3553008595988539\n",
      "sensitive attribute  2\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8112582781456954 0.5847255369928401 0.7285546415981199\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1702 604 0.35487661574618096\n",
      "sensitive attribute  3\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9953703703703703 0.4925544100801833 0.6300914380714879\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1203 432 0.35910224438902744\n",
      "sensitive attribute  4\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.6880952380952381 0.8757575757575757 0.8563074352548037\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1197 420 0.3508771929824561\n",
      "sensitive attribute  5\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.753968253968254 0.5792682926829268 0.721835883171071\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "719 252 0.35048678720445064\n",
      "sensitive attribute  6\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8816666666666667 0.6045714285714285 0.7519333729922665\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1681 600 0.35693039857227843\n",
      "sensitive attribute  7\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8531353135313532 0.5675082327113062 0.7183673469387755\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1715 606 0.3533527696793003\n",
      "sensitive attribute  8\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8211382113821138 0.6917808219178082 0.8043795620437956\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "685 246 0.35912408759124087\n",
      "sensitive attribute  9\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.6850961538461539 0.7938718662952646 0.8274410774410774\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1188 416 0.3501683501683502\n",
      "sensitive attribute  10\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9954128440366973 0.514218009478673 0.6600660066006601\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1212 436 0.35973597359735976\n",
      "sensitive attribute  11\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8957345971563981 0.6702127659574468 0.8057432432432432\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "592 211 0.3564189189189189\n",
      "sensitive attribute  12\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8268330733229329 0.5754614549402823 0.7223451327433629\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1808 641 0.3545353982300885\n",
      "sensitive attribute  13\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.975 0.6554621848739496 0.8068862275449101\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "668 240 0.3592814371257485\n",
      "sensitive attribute  14\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7924836601307189 0.5732860520094563 0.7182448036951501\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1732 612 0.3533487297921478\n",
      "sensitive attribute  15\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.6888888888888889 0.8885350318471338 0.8606060606060606\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1155 405 0.35064935064935066\n",
      "sensitive attribute  16\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9843400447427293 0.4949381327334083 0.6337349397590362\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1245 447 0.35903614457831323\n",
      "data acceptance rates\n",
      "[0.3553008595988539, 0.35487661574618096, 0.35910224438902744, 0.3508771929824561, 0.35048678720445064, 0.35693039857227843, 0.3533527696793003, 0.35912408759124087, 0.3501683501683502, 0.35973597359735976, 0.3564189189189189, 0.3545353982300885, 0.3592814371257485, 0.3533487297921478, 0.35064935064935066, 0.35903614457831323]\n",
      "data DP\n",
      "0.009567623429009575\n",
      "total accepted \n",
      "finalprecision 0.8438967136150235\n",
      "finalrecall 0.5976724854530341\n",
      "finalaccuracy 0.7429166666666667\n",
      "dimension of data\n",
      "16 2400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal\n",
      "discripency is:\n",
      "0.56916667\n",
      "gamma-epsilon-delta 0.35 0.04 1\n",
      "################################\n",
      "sensitive attribute  1\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9534883720930233 0.673972602739726 0.8123209169054442\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "698 258 0.36962750716332377\n",
      "sensitive attribute  2\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8039538714991763 0.5823389021479713 0.7244418331374853\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1702 607 0.35663924794359575\n",
      "sensitive attribute  3\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9955056179775281 0.5074455899198167 0.6408977556109726\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1203 445 0.36990856192851207\n",
      "sensitive attribute  4\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.6928571428571428 0.8818181818181818 0.8596491228070176\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1197 420 0.3508771929824561\n",
      "sensitive attribute  5\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7549407114624506 0.5823170731707317 0.7232267037552156\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "719 253 0.3518776077885953\n",
      "sensitive attribute  6\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8872549019607843 0.6205714285714286 0.7614515169541939\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1681 612 0.364069006543724\n",
      "sensitive attribute  7\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8725490196078431 0.5861690450054885 0.7346938775510204\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1715 612 0.3568513119533528\n",
      "sensitive attribute  8\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7905138339920948 0.684931506849315 0.7883211678832117\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "685 253 0.36934306569343067\n",
      "sensitive attribute  9\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.6906474820143885 0.8022284122562674 0.8316498316498316\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1188 417 0.351010101010101\n",
      "sensitive attribute  10\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9955357142857143 0.5284360189573459 0.66996699669967\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1212 448 0.3696369636963696\n",
      "sensitive attribute  11\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9315068493150684 0.723404255319149 0.8429054054054054\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "592 219 0.36993243243243246\n",
      "sensitive attribute  12\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8204334365325078 0.5754614549402823 0.7195796460176991\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1808 646 0.3573008849557522\n",
      "sensitive attribute  13\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9757085020242915 0.6750700280112045 0.8173652694610778\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "668 247 0.36976047904191617\n",
      "sensitive attribute  14\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7977346278317152 0.58274231678487 0.7240184757505773\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1732 618 0.3568129330254042\n",
      "sensitive attribute  15\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.6938271604938272 0.8949044585987261 0.8640692640692641\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1155 405 0.35064935064935066\n",
      "sensitive attribute  16\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9847826086956522 0.5095613048368954 0.6441767068273092\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1245 460 0.36947791164658633\n",
      "data acceptance rates\n",
      "[0.36962750716332377, 0.35663924794359575, 0.36990856192851207, 0.3508771929824561, 0.3518776077885953, 0.364069006543724, 0.3568513119533528, 0.36934306569343067, 0.351010101010101, 0.3696369636963696, 0.36993243243243246, 0.3573008849557522, 0.36976047904191617, 0.3568129330254042, 0.35064935064935066, 0.36947791164658633]\n",
      "data DP\n",
      "0.0192830817830818\n",
      "total accepted \n",
      "finalprecision 0.8485549132947977\n",
      "finalrecall 0.6101413133832086\n",
      "finalaccuracy 0.75\n",
      "dimension of data\n",
      "16 2400\n",
      "Optimal\n",
      "discripency is:\n",
      "0.60833333\n",
      "gamma-epsilon-delta 0.35 0.1 1\n",
      "################################\n",
      "sensitive attribute  1\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8960573476702509 0.684931506849315 0.7936962750716332\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "698 279 0.3997134670487106\n",
      "sensitive attribute  2\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8470209339774557 0.6276849642004774 0.7608695652173914\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1702 621 0.36486486486486486\n",
      "sensitive attribute  3\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9958419958419958 0.5486827033218786 0.6708229426433915\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1203 481 0.399833748960931\n",
      "sensitive attribute  4\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7088305489260143 0.9 0.8705096073517126\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1197 419 0.35004177109440265\n",
      "sensitive attribute  5\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.794425087108014 0.6951219512195121 0.7788595271210014\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "719 287 0.3991655076495132\n",
      "sensitive attribute  6\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8939641109298532 0.6262857142857143 0.7668054729327781\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1681 613 0.36466389054134446\n",
      "sensitive attribute  7\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.896969696969697 0.6498353457738749 0.7743440233236152\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1715 660 0.3848396501457726\n",
      "sensitive attribute  8\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7666666666666667 0.6301369863013698 0.7605839416058394\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "685 240 0.35036496350364965\n",
      "sensitive attribute  9\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7067307692307693 0.8189415041782729 0.8425925925925926\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1188 416 0.3501683501683502\n",
      "sensitive attribute  10\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9958677685950413 0.5710900473933649 0.6996699669966997\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1212 484 0.39933993399339934\n",
      "sensitive attribute  11\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9429824561403509 0.7624113475177305 0.8648648648648649\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "592 228 0.38513513513513514\n",
      "sensitive attribute  12\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8348214285714286 0.6091205211726385 0.7394911504424779\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1808 672 0.37168141592920356\n",
      "sensitive attribute  13\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9775280898876404 0.7310924369747899 0.8473053892215568\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "668 267 0.3997005988023952\n",
      "sensitive attribute  14\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8135860979462876 0.6087470449172577 0.7407621247113164\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1732 633 0.365473441108545\n",
      "sensitive attribute  15\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7111111111111111 0.9171974522292994 0.8761904761904762\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1155 405 0.35064935064935066\n",
      "sensitive attribute  16\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9858585858585859 0.5489313835770528 0.672289156626506\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1245 495 0.39759036144578314\n",
      "data acceptance rates\n",
      "[0.3997134670487106, 0.36486486486486486, 0.399833748960931, 0.35004177109440265, 0.3991655076495132, 0.36466389054134446, 0.3848396501457726, 0.35036496350364965, 0.3501683501683502, 0.39933993399339934, 0.38513513513513514, 0.37168141592920356, 0.3997005988023952, 0.365473441108545, 0.35064935064935066, 0.39759036144578314]\n",
      "data DP\n",
      "0.049791977866528336\n",
      "total accepted \n",
      "finalprecision 0.8622222222222222\n",
      "finalrecall 0.6450540315876975\n",
      "finalaccuracy 0.7704166666666666\n",
      "dimension of data\n",
      "16 2400\n",
      "Optimal\n",
      "discripency is:\n",
      "0.63333333\n",
      "gamma-epsilon-delta 0.35 0.15 1\n",
      "################################\n",
      "sensitive attribute  1\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9358108108108109 0.7589041095890411 0.8467048710601719\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "698 296 0.42406876790830944\n",
      "sensitive attribute  2\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.831230283911672 0.6288782816229117 0.754406580493537\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1702 634 0.372502937720329\n",
      "sensitive attribute  3\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9941291585127201 0.581901489117984 0.6940980881130507\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1203 511 0.4247714048212801\n",
      "sensitive attribute  4\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7064439140811456 0.896969696969697 0.8688387635756056\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1197 419 0.35004177109440265\n",
      "sensitive attribute  5\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7631578947368421 0.7073170731707317 0.7663421418636995\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "719 304 0.4228094575799722\n",
      "sensitive attribute  6\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9137380191693291 0.6537142857142857 0.7876264128494943\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1681 626 0.3723973825104105\n",
      "sensitive attribute  7\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9028985507246376 0.6838638858397366 0.793002915451895\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1715 690 0.40233236151603496\n",
      "sensitive attribute  8\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7541666666666667 0.6198630136986302 0.7518248175182481\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "685 240 0.35036496350364965\n",
      "sensitive attribute  9\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7091346153846154 0.8217270194986073 0.8442760942760943\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1188 416 0.3501683501683502\n",
      "sensitive attribute  10\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9902723735408561 0.6030805687203792 0.7194719471947195\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1212 514 0.4240924092409241\n",
      "sensitive attribute  11\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9282868525896414 0.8262411347517731 0.8868243243243243\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "592 251 0.4239864864864865\n",
      "sensitive attribute  12\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8409425625920471 0.6199782844733985 0.7466814159292036\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1808 679 0.37555309734513276\n",
      "sensitive attribute  13\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9399293286219081 0.7450980392156863 0.8383233532934131\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "668 283 0.4236526946107784\n",
      "sensitive attribute  14\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8315301391035549 0.6359338061465721 0.7592378752886836\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1732 647 0.37355658198614317\n",
      "sensitive attribute  15\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7086419753086419 0.9140127388535032 0.8744588744588745\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1155 405 0.35064935064935066\n",
      "sensitive attribute  16\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9847619047619047 0.5815523059617548 0.6947791164658634\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1245 525 0.42168674698795183\n",
      "data acceptance rates\n",
      "[0.42406876790830944, 0.372502937720329, 0.4247714048212801, 0.35004177109440265, 0.4228094575799722, 0.3723973825104105, 0.40233236151603496, 0.35036496350364965, 0.3501683501683502, 0.4240924092409241, 0.4239864864864865, 0.37555309734513276, 0.4236526946107784, 0.37355658198614317, 0.35064935064935066, 0.42168674698795183]\n",
      "data DP\n",
      "0.07472963372687746\n",
      "total accepted \n",
      "finalprecision 0.864516129032258\n",
      "finalrecall 0.6683291770573566\n",
      "finalaccuracy 0.78125\n",
      "dimension of data\n",
      "16 2400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal\n",
      "discripency is:\n",
      "0.65833333\n",
      "gamma-epsilon-delta 0.35 0.2 1\n",
      "################################\n",
      "sensitive attribute  1\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8535031847133758 0.7342465753424657 0.7951289398280802\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "698 314 0.4498567335243553\n",
      "sensitive attribute  2\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8808049535603715 0.6789976133651552 0.7967097532314924\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1702 646 0.3795534665099882\n",
      "sensitive attribute  3\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9963031423290203 0.6174112256586484 0.7206982543640897\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1203 541 0.44970906068162925\n",
      "sensitive attribute  4\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.711217183770883 0.9030303030303031 0.8721804511278195\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1197 419 0.35004177109440265\n",
      "sensitive attribute  5\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.84472049689441 0.8292682926829268 0.8525730180806675\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "719 322 0.4478442280945758\n",
      "sensitive attribute  6\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8855799373040752 0.6457142857142857 0.7721594289113622\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1681 638 0.37953599048185604\n",
      "sensitive attribute  7\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9187116564417178 0.6575192096597146 0.7871720116618076\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1715 652 0.3801749271137026\n",
      "sensitive attribute  8\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7727272727272727 0.815068493150685 0.8189781021897811\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "685 308 0.44963503649635034\n",
      "sensitive attribute  9\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7283653846153846 0.8440111420612814 0.8577441077441077\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1188 416 0.3501683501683502\n",
      "sensitive attribute  10\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9816176470588235 0.6327014218009479 0.735973597359736\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1212 544 0.44884488448844884\n",
      "sensitive attribute  11\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8558951965065502 0.6950354609929078 0.7989864864864865\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "592 229 0.38682432432432434\n",
      "sensitive attribute  12\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8768809849521204 0.6959826275787188 0.7953539823008849\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1808 731 0.4043141592920354\n",
      "sensitive attribute  13\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9333333333333333 0.7843137254901961 0.8547904191616766\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "668 300 0.4491017964071856\n",
      "sensitive attribute  14\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.843939393939394 0.6583924349881797 0.7736720554272517\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1732 660 0.3810623556581986\n",
      "sensitive attribute  15\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7135802469135802 0.9203821656050956 0.8779220779220779\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1155 405 0.35064935064935066\n",
      "sensitive attribute  16\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9873873873873874 0.6164229471316085 0.7204819277108434\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1245 555 0.4457831325301205\n",
      "data acceptance rates\n",
      "[0.4498567335243553, 0.3795534665099882, 0.44970906068162925, 0.35004177109440265, 0.4478442280945758, 0.37953599048185604, 0.3801749271137026, 0.44963503649635034, 0.3501683501683502, 0.44884488448844884, 0.38682432432432434, 0.4043141592920354, 0.4491017964071856, 0.3810623556581986, 0.35064935064935066, 0.4457831325301205]\n",
      "data DP\n",
      "0.09981496242995264\n",
      "total accepted \n",
      "finalprecision 0.871875\n",
      "finalrecall 0.6957605985037406\n",
      "finalaccuracy 0.79625\n",
      "dimension of data\n",
      "16 2400\n",
      "Optimal\n",
      "discripency is:\n",
      "0.70833333\n",
      "gamma-epsilon-delta 0.35 0.3 1\n",
      "################################\n",
      "sensitive attribute  1\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9455587392550143 0.9041095890410958 0.9226361031518625\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "698 349 0.5\n",
      "sensitive attribute  2\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8435171385991058 0.6754176610978521 0.7784958871915394\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1702 671 0.39424206815511165\n",
      "sensitive attribute  3\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9950083194675541 0.6849942726231386 0.7689110556940981\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1203 601 0.4995843724023275\n",
      "sensitive attribute  4\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.711217183770883 0.9030303030303031 0.8721804511278195\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1197 419 0.35004177109440265\n",
      "sensitive attribute  5\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7267267267267268 0.7378048780487805 0.7538247566063978\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "719 333 0.4631432545201669\n",
      "sensitive attribute  6\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9519650655021834 0.7474285714285714 0.8488994646044021\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1681 687 0.4086853063652588\n",
      "sensitive attribute  7\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8992907801418439 0.6959385290889133 0.7970845481049562\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1715 705 0.4110787172011662\n",
      "sensitive attribute  8\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8317460317460318 0.8972602739726028 0.8788321167883212\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "685 315 0.45985401459854014\n",
      "sensitive attribute  9\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.75 0.8690807799442897 0.872895622895623\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1188 416 0.3501683501683502\n",
      "sensitive attribute  10\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9668874172185431 0.6919431279620853 0.768976897689769\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1212 604 0.49834983498349833\n",
      "sensitive attribute  11\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.90625 0.925531914893617 0.918918918918919\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "592 288 0.4864864864864865\n",
      "sensitive attribute  12\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8674863387978142 0.6894679695982627 0.7881637168141593\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1808 732 0.40486725663716816\n",
      "sensitive attribute  13\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8952095808383234 0.8375350140056023 0.8607784431137725\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "668 334 0.5\n",
      "sensitive attribute  14\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8702623906705539 0.7056737588652482 0.8048498845265589\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1732 686 0.3960739030023095\n",
      "sensitive attribute  15\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.725925925925926 0.9363057324840764 0.8865800865800866\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1155 405 0.35064935064935066\n",
      "sensitive attribute  16\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9788617886178862 0.6771653543307087 0.7590361445783133\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1245 615 0.4939759036144578\n",
      "data acceptance rates\n",
      "[0.5, 0.39424206815511165, 0.4995843724023275, 0.35004177109440265, 0.4631432545201669, 0.4086853063652588, 0.4110787172011662, 0.45985401459854014, 0.3501683501683502, 0.49834983498349833, 0.4864864864864865, 0.40486725663716816, 0.5, 0.3960739030023095, 0.35064935064935066, 0.4939759036144578]\n",
      "data DP\n",
      "0.14995822890559735\n",
      "total accepted \n",
      "finalprecision 0.8784313725490196\n",
      "finalrecall 0.744804655029094\n",
      "finalaccuracy 0.8204166666666667\n",
      "dimension of data\n",
      "16 2400\n",
      "Optimal\n",
      "discripency is:\n",
      "0.75833333\n",
      "gamma-epsilon-delta 0.35 0.4 1\n",
      "################################\n",
      "sensitive attribute  1\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8488063660477454 0.8767123287671232 0.8538681948424068\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "698 377 0.5401146131805158\n",
      "sensitive attribute  2\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9089615931721194 0.7625298329355609 0.845475910693302\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1702 703 0.41304347826086957\n",
      "sensitive attribute  3\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9954614220877458 0.7537227949599083 0.8187863674147964\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1203 661 0.5494596841230258\n",
      "sensitive attribute  4\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7183770883054893 0.9121212121212121 0.8771929824561403\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1197 419 0.35004177109440265\n",
      "sensitive attribute  5\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7785016286644951 0.7286585365853658 0.7816411682892906\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "719 307 0.42698191933240615\n",
      "sensitive attribute  6\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9314359637774903 0.8228571428571428 0.8762641284949435\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1681 773 0.4598453301606187\n",
      "sensitive attribute  7\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8969849246231156 0.7837541163556532 0.8373177842565598\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1715 796 0.4641399416909621\n",
      "sensitive attribute  8\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8626760563380281 0.839041095890411 0.8744525547445255\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "685 284 0.4145985401459854\n",
      "sensitive attribute  9\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7740384615384616 0.8969359331476323 0.8897306397306397\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1188 416 0.3501683501683502\n",
      "sensitive attribute  10\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9593373493975904 0.754739336492891 0.806930693069307\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1212 664 0.5478547854785478\n",
      "sensitive attribute  11\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8426229508196721 0.9113475177304965 0.8766891891891891\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "592 305 0.5152027027027027\n",
      "sensitive attribute  12\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9058064516129032 0.762214983713355 0.838495575221239\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1808 775 0.4286504424778761\n",
      "sensitive attribute  13\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8419618528610354 0.865546218487395 0.8413173652694611\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "668 367 0.5494011976047904\n",
      "sensitive attribute  14\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9116409537166901 0.7683215130023641 0.8504618937644342\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1732 713 0.41166281755196305\n",
      "sensitive attribute  15\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7283950617283951 0.9394904458598726 0.8883116883116883\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1155 405 0.35064935064935066\n",
      "sensitive attribute  16\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9837037037037037 0.7469066366704162 0.8104417670682731\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1245 675 0.5421686746987951\n",
      "data acceptance rates\n",
      "[0.5401146131805158, 0.41304347826086957, 0.5494596841230258, 0.35004177109440265, 0.42698191933240615, 0.4598453301606187, 0.4641399416909621, 0.4145985401459854, 0.3501683501683502, 0.5478547854785478, 0.5152027027027027, 0.4286504424778761, 0.5494011976047904, 0.41166281755196305, 0.35064935064935066, 0.5421686746987951]\n",
      "data DP\n",
      "0.19941791302862316\n",
      "total accepted \n",
      "finalprecision 0.887962962962963\n",
      "finalrecall 0.7971737323358271\n",
      "finalaccuracy 0.8479166666666667\n",
      "dimension of data\n",
      "16 2400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal\n",
      "discripency is:\n",
      "0.80833333\n",
      "gamma-epsilon-delta 0.35 0.5 1\n",
      "################################\n",
      "sensitive attribute  1\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9052631578947369 0.9424657534246575 0.9183381088825215\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "698 380 0.5444126074498568\n",
      "sensitive attribute  2\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8855263157894737 0.8031026252983293 0.8519388954171563\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1702 760 0.44653349001175086\n",
      "sensitive attribute  3\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9958391123439667 0.8224513172966781 0.8686616791354946\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1203 721 0.5993349958437241\n",
      "sensitive attribute  4\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7136038186157518 0.906060606060606 0.8738512949039264\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1197 419 0.35004177109440265\n",
      "sensitive attribute  5\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7654320987654321 0.9451219512195121 0.8428372739916551\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "719 405 0.5632823365785814\n",
      "sensitive attribute  6\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9619047619047619 0.808 0.883402736466389\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1681 735 0.43723973825104107\n",
      "sensitive attribute  7\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9032258064516129 0.8298572996706916 0.8623906705539358\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1715 837 0.4880466472303207\n",
      "sensitive attribute  8\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8613861386138614 0.8938356164383562 0.8934306569343066\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "685 303 0.44233576642335765\n",
      "sensitive attribute  9\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7644230769230769 0.8857938718662952 0.882996632996633\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1188 416 0.3501683501683502\n",
      "sensitive attribute  10\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9654696132596685 0.8281990521327014 0.8597359735973598\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1212 724 0.5973597359735974\n",
      "sensitive attribute  11\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8446601941747572 0.925531914893617 0.8834459459459459\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "592 309 0.5219594594594594\n",
      "sensitive attribute  12\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9097472924187726 0.8208469055374593 0.8672566371681416\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1808 831 0.4596238938053097\n",
      "sensitive attribute  13\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8709677419354839 0.907563025210084 0.8787425149700598\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "668 372 0.5568862275449101\n",
      "sensitive attribute  14\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.90234375 0.8191489361702128 0.8683602771362586\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1732 768 0.44341801385681295\n",
      "sensitive attribute  15\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7241379310344828 0.9363057324840764 0.8857142857142857\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1155 406 0.3515151515151515\n",
      "sensitive attribute  16\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9850136239782016 0.813273340832396 0.8578313253012049\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1245 734 0.5895582329317269\n",
      "data acceptance rates\n",
      "[0.5444126074498568, 0.44653349001175086, 0.5993349958437241, 0.35004177109440265, 0.5632823365785814, 0.43723973825104107, 0.4880466472303207, 0.44233576642335765, 0.3501683501683502, 0.5973597359735974, 0.5219594594594594, 0.4596238938053097, 0.5568862275449101, 0.44341801385681295, 0.3515151515151515, 0.5895582329317269]\n",
      "data DP\n",
      "0.24929322474932142\n",
      "total accepted \n",
      "finalprecision 0.8921052631578947\n",
      "finalrecall 0.8453865336658354\n",
      "finalaccuracy 0.87125\n",
      "dimension of data\n",
      "16 2400\n",
      "Optimal\n",
      "discripency is:\n",
      "0.85833333\n",
      "gamma-epsilon-delta 0.35 0.6 1\n",
      "################################\n",
      "sensitive attribute  1\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8763440860215054 0.8931506849315068 0.8782234957020058\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "698 372 0.5329512893982808\n",
      "sensitive attribute  2\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9082125603864735 0.8973747016706444 0.9048178613396005\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1702 828 0.4864864864864865\n",
      "sensitive attribute  3\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9961587708066582 0.8911798396334479 0.9185369908561929\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1203 781 0.6492103075644223\n",
      "sensitive attribute  4\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7159904534606205 0.9090909090909091 0.8755221386800334\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1197 419 0.35004177109440265\n",
      "sensitive attribute  5\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7722772277227723 0.9512195121951219 0.8497913769123783\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "719 404 0.5618915159944368\n",
      "sensitive attribute  6\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9623115577889447 0.8754285714285714 0.9173111243307555\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1681 796 0.47352766210588937\n",
      "sensitive attribute  7\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9209932279909706 0.8957189901207464 0.9037900874635568\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1715 886 0.5166180758017492\n",
      "sensitive attribute  8\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8343949044585988 0.8972602739726028 0.8802919708029197\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "685 314 0.4583941605839416\n",
      "sensitive attribute  9\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7692307692307693 0.8913649025069638 0.8863636363636364\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1188 416 0.3501683501683502\n",
      "sensitive attribute  10\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9668367346938775 0.8981042654028436 0.9075907590759076\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1212 784 0.6468646864686468\n",
      "sensitive attribute  11\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8877551020408163 0.925531914893617 0.9087837837837838\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "592 294 0.4966216216216216\n",
      "sensitive attribute  12\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9017660044150111 0.8870792616720955 0.8932522123893806\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1808 906 0.5011061946902655\n",
      "sensitive attribute  13\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8571428571428571 0.907563025210084 0.8697604790419161\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "668 378 0.5658682634730539\n",
      "sensitive attribute  14\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9172749391727494 0.8912529550827423 0.9076212471131639\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1732 822 0.4745958429561201\n",
      "sensitive attribute  15\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7308641975308642 0.9426751592356688 0.8900432900432901\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1155 405 0.35064935064935066\n",
      "sensitive attribute  16\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9836477987421384 0.8796400449943758 0.9036144578313253\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1245 795 0.6385542168674698\n",
      "data acceptance rates\n",
      "[0.5329512893982808, 0.4864864864864865, 0.6492103075644223, 0.35004177109440265, 0.5618915159944368, 0.47352766210588937, 0.5166180758017492, 0.4583941605839416, 0.3501683501683502, 0.6468646864686468, 0.4966216216216216, 0.5011061946902655, 0.5658682634730539, 0.4745958429561201, 0.35064935064935066, 0.6385542168674698]\n",
      "data DP\n",
      "0.2991685364700197\n",
      "total accepted \n",
      "finalprecision 0.8983333333333333\n",
      "finalrecall 0.8960931005818786\n",
      "finalaccuracy 0.8970833333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/subham/anaconda3/envs/tensorflow/lib/python3.6/site-packages/pandas/core/indexing.py:671: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "/home/subham/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/subham/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0     1     2     3     4     5     6     7     8     9     ...  \\\n",
      "x1_0.0      0     0     0     1     1     0     0     0     0     0  ...   \n",
      "x1_1.0      1     1     1     0     0     1     1     1     1     1  ...   \n",
      "x2_0.0      0     1     1     0     0     1     1     1     0     1  ...   \n",
      "x2_1.0      1     0     0     1     1     0     0     0     1     0  ...   \n",
      "x3_0.0      0     1     0     0     0     0     0     1     0     0  ...   \n",
      "x3_1.0      1     0     1     1     1     1     1     0     1     1  ...   \n",
      "x4_0.0      1     1     0     1     1     1     0     1     1     1  ...   \n",
      "x4_1.0      0     0     1     0     0     0     1     0     0     0  ...   \n",
      "x5_0.0      1     0     1     1     1     1     1     0     1     0  ...   \n",
      "x5_1.0      0     1     0     0     0     0     0     1     0     1  ...   \n",
      "x6_0.0      0     1     0     0     1     0     0     1     0     0  ...   \n",
      "x6_1.0      1     0     1     1     0     1     1     0     1     1  ...   \n",
      "x7_0.0      0     0     0     0     0     0     1     0     0     0  ...   \n",
      "x7_1.0      1     1     1     1     1     1     0     1     1     1  ...   \n",
      "x8_0.0      0     1     0     0     0     1     0     1     0     0  ...   \n",
      "x8_1.0      1     0     1     1     1     0     1     0     1     1  ...   \n",
      "x9_0.0      0     1     1     1     0     0     1     1     0     0  ...   \n",
      "x9_1.0      1     0     0     0     1     1     0     0     1     1  ...   \n",
      "x10_0.0     1     0     0     0     0     0     1     0     0     0  ...   \n",
      "x10_1.0     0     1     1     1     1     1     0     1     1     1  ...   \n",
      "\n",
      "         2390  2391  2392  2393  2394  2395  2396  2397  2398  2399  \n",
      "x1_0.0      0     0     0     1     0     0     0     0     0     0  \n",
      "x1_1.0      1     1     1     0     1     1     1     1     1     1  \n",
      "x2_0.0      1     1     1     1     1     0     0     1     0     0  \n",
      "x2_1.0      0     0     0     0     0     1     1     0     1     1  \n",
      "x3_0.0      1     0     0     1     0     0     0     0     0     0  \n",
      "x3_1.0      0     1     1     0     1     1     1     1     1     1  \n",
      "x4_0.0      0     1     0     1     1     1     1     1     1     1  \n",
      "x4_1.0      1     0     1     0     0     0     0     0     0     0  \n",
      "x5_0.0      0     1     0     0     0     1     0     1     1     1  \n",
      "x5_1.0      1     0     1     1     1     0     1     0     0     0  \n",
      "x6_0.0      0     0     0     0     0     0     1     0     1     0  \n",
      "x6_1.0      1     1     1     1     1     1     0     1     0     1  \n",
      "x7_0.0      0     0     0     0     0     0     1     0     1     0  \n",
      "x7_1.0      1     1     1     1     1     1     0     1     0     1  \n",
      "x8_0.0      0     1     0     1     0     1     1     0     0     0  \n",
      "x8_1.0      1     0     1     0     1     0     0     1     1     1  \n",
      "x9_0.0      0     0     0     0     0     1     0     1     0     1  \n",
      "x9_1.0      1     1     1     1     1     0     1     0     1     0  \n",
      "x10_0.0     0     0     1     0     1     0     0     0     0     0  \n",
      "x10_1.0     1     1     0     1     0     1     1     1     1     1  \n",
      "\n",
      "[20 rows x 2400 columns]\n",
      "sensitive attribute  1\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "698\n",
      "365\n",
      "0.5229226361031518\n",
      "sensitive attribute  2\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1702\n",
      "838\n",
      "0.49236192714453586\n",
      "sensitive attribute  3\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1203\n",
      "873\n",
      "0.7256857855361596\n",
      "sensitive attribute  4\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1197\n",
      "330\n",
      "0.2756892230576441\n",
      "sensitive attribute  5\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "719\n",
      "328\n",
      "0.4561891515994437\n",
      "sensitive attribute  6\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1681\n",
      "875\n",
      "0.520523497917906\n",
      "sensitive attribute  7\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1715\n",
      "911\n",
      "0.5311953352769679\n",
      "sensitive attribute  8\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "685\n",
      "292\n",
      "0.42627737226277373\n",
      "sensitive attribute  9\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1188\n",
      "359\n",
      "0.3021885521885522\n",
      "sensitive attribute  10\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1212\n",
      "844\n",
      "0.6963696369636964\n",
      "sensitive attribute  11\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "592\n",
      "282\n",
      "0.47635135135135137\n",
      "sensitive attribute  12\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1808\n",
      "921\n",
      "0.5094026548672567\n",
      "sensitive attribute  13\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "668\n",
      "357\n",
      "0.5344311377245509\n",
      "sensitive attribute  14\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1732\n",
      "846\n",
      "0.4884526558891455\n",
      "sensitive attribute  15\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1155\n",
      "314\n",
      "0.27186147186147186\n",
      "sensitive attribute  16\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1245\n",
      "889\n",
      "0.7140562248995984\n",
      "sensitive attribute  17\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1141\n",
      "325\n",
      "0.2848378615249781\n",
      "sensitive attribute  18\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1259\n",
      "878\n",
      "0.6973788721207307\n",
      "sensitive attribute  19\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "667\n",
      "387\n",
      "0.5802098950524738\n",
      "sensitive attribute  20\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1733\n",
      "816\n",
      "0.4708597807270629\n",
      "data acceptance rates\n",
      "[0.5229226361031518, 0.49236192714453586, 0.7256857855361596, 0.2756892230576441, 0.4561891515994437, 0.520523497917906, 0.5311953352769679, 0.42627737226277373, 0.3021885521885522, 0.6963696369636964, 0.47635135135135137, 0.5094026548672567, 0.5344311377245509, 0.4884526558891455, 0.27186147186147186, 0.7140562248995984, 0.2848378615249781, 0.6973788721207307, 0.5802098950524738, 0.4708597807270629]\n",
      "data DP\n",
      "0.45382431367468773\n",
      "sensitive attribute  1\n",
      "prec reca accuracy for each sens\n",
      "0.9971014492753624 0.9424657534246575 0.9684813753581661\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "698\n",
      "345\n",
      "0.49426934097421205\n",
      "sensitive attribute  2\n",
      "prec reca accuracy for each sens\n",
      "0.9886506935687264 0.9355608591885441 0.9629847238542891\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1702\n",
      "793\n",
      "0.46592244418331374\n",
      "sensitive attribute  3\n",
      "prec reca accuracy for each sens\n",
      "0.9964071856287425 0.9530355097365406 0.9634247714048213\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1203\n",
      "835\n",
      "0.6940980881130507\n",
      "sensitive attribute  4\n",
      "prec reca accuracy for each sens\n",
      "0.976897689768977 0.896969696969697 0.9657477025898078\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1197\n",
      "303\n",
      "0.2531328320802005\n",
      "sensitive attribute  5\n",
      "prec reca accuracy for each sens\n",
      "0.9967948717948718 0.948170731707317 0.9749652294853964\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "719\n",
      "312\n",
      "0.4339360222531293\n",
      "sensitive attribute  6\n",
      "prec reca accuracy for each sens\n",
      "0.9891041162227603 0.9337142857142857 0.9601427721594289\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1681\n",
      "826\n",
      "0.4913741820345033\n",
      "sensitive attribute  7\n",
      "prec reca accuracy for each sens\n",
      "0.9908675799086758 0.9527991218441273 0.970262390670554\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1715\n",
      "876\n",
      "0.5107871720116618\n",
      "sensitive attribute  8\n",
      "prec reca accuracy for each sens\n",
      "0.9923664122137404 0.8904109589041096 0.9503649635036496\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "685\n",
      "262\n",
      "0.38248175182481753\n",
      "sensitive attribute  9\n",
      "prec reca accuracy for each sens\n",
      "0.9783950617283951 0.883008356545961 0.9587542087542088\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1188\n",
      "324\n",
      "0.2727272727272727\n",
      "sensitive attribute  10\n",
      "prec reca accuracy for each sens\n",
      "0.9963144963144963 0.9609004739336493 0.9702970297029703\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1212\n",
      "814\n",
      "0.6716171617161716\n",
      "sensitive attribute  11\n",
      "prec reca accuracy for each sens\n",
      "0.9961685823754789 0.9219858156028369 0.9611486486486487\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "592\n",
      "261\n",
      "0.4408783783783784\n",
      "sensitive attribute  12\n",
      "prec reca accuracy for each sens\n",
      "0.9897377423033067 0.9424538545059717 0.9657079646017699\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1808\n",
      "877\n",
      "0.4850663716814159\n",
      "sensitive attribute  13\n",
      "prec reca accuracy for each sens\n",
      "0.981651376146789 0.8991596638655462 0.937125748502994\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "668\n",
      "327\n",
      "0.48952095808383234\n",
      "sensitive attribute  14\n",
      "prec reca accuracy for each sens\n",
      "0.9950678175092479 0.9539007092198581 0.9751732101616628\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1732\n",
      "811\n",
      "0.4682448036951501\n",
      "sensitive attribute  15\n",
      "prec reca accuracy for each sens\n",
      "0.9931972789115646 0.9299363057324841 0.9792207792207792\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1155\n",
      "294\n",
      "0.2545454545454545\n",
      "sensitive attribute  16\n",
      "prec reca accuracy for each sens\n",
      "0.990521327014218 0.9403824521934758 0.951004016064257\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1245\n",
      "844\n",
      "0.6779116465863454\n",
      "sensitive attribute  17\n",
      "prec reca accuracy for each sens\n",
      "0.9865771812080537 0.9046153846153846 0.9693251533742331\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1141\n",
      "298\n",
      "0.26117440841367223\n",
      "sensitive attribute  18\n",
      "prec reca accuracy for each sens\n",
      "0.9928571428571429 0.9498861047835991 0.960285941223193\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1259\n",
      "840\n",
      "0.6671961874503575\n",
      "sensitive attribute  19\n",
      "prec reca accuracy for each sens\n",
      "0.9972144846796658 0.9250645994832042 0.9550224887556222\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "667\n",
      "359\n",
      "0.5382308845577212\n",
      "sensitive attribute  20\n",
      "prec reca accuracy for each sens\n",
      "0.9884467265725289 0.9436274509803921 0.9682631275245239\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1733\n",
      "779\n",
      "0.44950952106174263\n",
      "data acceptance rates\n",
      "[0.49426934097421205, 0.46592244418331374, 0.6940980881130507, 0.2531328320802005, 0.4339360222531293, 0.4913741820345033, 0.5107871720116618, 0.38248175182481753, 0.2727272727272727, 0.6716171617161716, 0.4408783783783784, 0.4850663716814159, 0.48952095808383234, 0.4682448036951501, 0.2545454545454545, 0.6779116465863454, 0.26117440841367223, 0.6671961874503575, 0.5382308845577212, 0.44950952106174263]\n",
      "data DP\n",
      "0.44096525603285025\n",
      "SVM accuracy--------------------------\n",
      "0.9912126537785588 0.9376558603491272 0.9645833333333333\n",
      "precision recall accuracy\n",
      "dimension of data\n",
      "20 2400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal\n",
      "discripency is:\n",
      "0.54166667\n",
      "gamma-epsilon-delta 0.35 0.02 1\n",
      "################################\n",
      "sensitive attribute  1\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.896414342629482 0.6164383561643836 0.7621776504297995\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "698 251 0.35959885386819485\n",
      "sensitive attribute  2\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8086522462562395 0.5799522673031027 0.7256169212690952\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1702 601 0.35311398354876616\n",
      "sensitive attribute  3\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9953810623556582 0.4936998854524628 0.6309226932668329\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1203 433 0.35993349958437243\n",
      "sensitive attribute  4\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.6682577565632458 0.8484848484848485 0.8421052631578947\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1197 419 0.35004177109440265\n",
      "sensitive attribute  5\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8379446640316206 0.6463414634146342 0.7816411682892906\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "719 253 0.3518776077885953\n",
      "sensitive attribute  6\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8330550918196995 0.5702857142857143 0.7168352171326592\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1681 599 0.35633551457465795\n",
      "sensitive attribute  7\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8513071895424836 0.5718990120746432 0.719533527696793\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1715 612 0.3568513119533528\n",
      "sensitive attribute  8\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7916666666666666 0.6506849315068494 0.7781021897810219\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "685 240 0.35036496350364965\n",
      "sensitive attribute  9\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.6658653846153846 0.7715877437325905 0.813973063973064\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1188 416 0.3501683501683502\n",
      "sensitive attribute  10\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9954128440366973 0.514218009478673 0.6600660066006601\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1212 436 0.35973597359735976\n",
      "sensitive attribute  11\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8701923076923077 0.6418439716312057 0.7837837837837838\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "592 208 0.35135135135135137\n",
      "sensitive attribute  12\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8229813664596274 0.5754614549402823 0.7206858407079646\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1808 644 0.3561946902654867\n",
      "sensitive attribute  13\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9166666666666666 0.6162464985994398 0.7649700598802395\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "668 240 0.3592814371257485\n",
      "sensitive attribute  14\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8022875816993464 0.5803782505910166 0.7251732101616628\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1732 612 0.3533487297921478\n",
      "sensitive attribute  15\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.6691358024691358 0.8630573248407644 0.8467532467532467\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1155 405 0.35064935064935066\n",
      "sensitive attribute  16\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9843400447427293 0.4949381327334083 0.6337349397590362\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1245 447 0.35903614457831323\n",
      "sensitive attribute  17\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.66 0.8123076923076923 0.8273444347063978\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1141 400 0.35056967572305\n",
      "sensitive attribute  18\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.9889380530973452 0.5091116173120729 0.653693407466243\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1259 452 0.35901509134233517\n",
      "sensitive attribute  19\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.7957446808510639 0.48320413436692505 0.6281859070464768\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "667 235 0.3523238380809595\n",
      "sensitive attribute  20\n",
      "lp ############### prec reca accuracy for each sens\n",
      "0.8492706645056726 0.6421568627450981 0.7778418926716676\n",
      "By lp---------total , accepted, aceeptance rate:\n",
      "1733 617 0.35603000577034044\n",
      "data acceptance rates\n",
      "[0.35959885386819485, 0.35311398354876616, 0.35993349958437243, 0.35004177109440265, 0.3518776077885953, 0.35633551457465795, 0.3568513119533528, 0.35036496350364965, 0.3501683501683502, 0.35973597359735976, 0.35135135135135137, 0.3561946902654867, 0.3592814371257485, 0.3533487297921478, 0.35064935064935066, 0.35903614457831323, 0.35056967572305, 0.35901509134233517, 0.3523238380809595, 0.35603000577034044]\n",
      "data DP\n",
      "0.009891728489969775\n",
      "total accepted \n",
      "finalprecision 0.8345070422535211\n",
      "finalrecall 0.5910224438902744\n",
      "finalaccuracy 0.73625\n",
      "dimension of data\n",
      "20 2400\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-fcb23ec6b834>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m# print(r.value_counts())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0maccu_all\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mDP_all\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macceptance_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malpha_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msensitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-401b1f832e1b>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(datax, y_test, y_test_pred)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgama\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0meps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m             \u001b[0mu1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mu2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_max_lp_all_ng2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m             \u001b[0;31m#######################Disp_impact#######################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gamma-epsilon-delta\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-05ca00daab3b>\u001b[0m in \u001b[0;36mmin_max_lp_all_ng2\u001b[0;34m(data1, gamma, eps, r, delta)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m#####################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLp_prob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# Solver\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLpStatus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"discripency is:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/pulp/pulp.py\u001b[0m in \u001b[0;36msolve\u001b[0;34m(self, solver, **kwargs)\u001b[0m\n\u001b[1;32m   1662\u001b[0m         \u001b[0;31m#time it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolutionTime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mclock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1664\u001b[0;31m         \u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactualSolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1665\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolutionTime\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mclock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1666\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestoreObjective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwasNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummyVar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/pulp/solvers.py\u001b[0m in \u001b[0;36mactualSolve\u001b[0;34m(self, lp, **kwargs)\u001b[0m\n\u001b[1;32m   1360\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mactualSolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \u001b[0;34m\"\"\"Solve a well formulated lp problem\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1362\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve_CBC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mavailable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/pulp/solvers.py\u001b[0m in \u001b[0;36msolve_CBC\u001b[0;34m(self, lp, use_mps)\u001b[0m\n\u001b[1;32m   1419\u001b[0m         cbc = subprocess.Popen((self.path + cmds).split(), stdout = pipe,\n\u001b[1;32m   1420\u001b[0m                              stderr = pipe)\n\u001b[0;32m-> 1421\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mcbc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1422\u001b[0m             raise PulpSolverError(\"Pulp: Error while trying to execute \" +  \\\n\u001b[1;32m   1423\u001b[0m                                     self.path)\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout, endtime)\u001b[0m\n\u001b[1;32m   1475\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1476\u001b[0m                             \u001b[0;32mbreak\u001b[0m  \u001b[0;31m# Another thread waited.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1477\u001b[0;31m                         \u001b[0;34m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1478\u001b[0m                         \u001b[0;31m# Check the pid and loop as waitpid has been known to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1479\u001b[0m                         \u001b[0;31m# return 0 even without WNOHANG in odd situations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36m_try_wait\u001b[0;34m(self, wait_flags)\u001b[0m\n\u001b[1;32m   1422\u001b[0m             \u001b[0;34m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1423\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1424\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait_flags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1425\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mChildProcessError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m                 \u001b[0;31m# This happens if SIGCLD is set to be ignored or waiting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from random import seed, shuffle\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.datasets import make_classification\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "data3 = make_classification(n_samples=8000, n_features=20, n_informative=10, n_redundant=10, n_repeated=0, \n",
    "                            n_classes=2, n_clusters_per_class=1, weights=None, flip_y=0.00, class_sep=1.0, \n",
    "                            hypercube=True, shift=0.0, scale=1.0, shuffle=False, random_state=11)\n",
    "df3 = pd.DataFrame(data3[0],columns=['x'+str(i) for i in range(1,21)])\n",
    "df3['y'] = data3[1]\n",
    "# print(df3.head())\n",
    "\n",
    "data=df3.drop(columns=['y'])\n",
    "print(data)\n",
    "r=df3[['y']]\n",
    "print(r)\n",
    "\n",
    "\n",
    "\n",
    "X_test,Y_test,Y_test_pred,e = synthetic_svm(data , r)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "# Y_test_pred.reset_index()\n",
    "Y_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(X_test)\n",
    "print(Y_test_pred)\n",
    "print(Y_test)\n",
    "# set_k=2\n",
    "p=2\n",
    "for set_k in np.arange(p,11,2):\n",
    "#for set_k in np.arange(2,11,2): \n",
    "    \n",
    "    sens=X_test[['x'+str(k) for k in range(1,set_k+1)]]\n",
    "#     print(set_k)\n",
    "#     print(sens)\n",
    "\n",
    "\n",
    "    p=sens.shape[0]\n",
    "    q=set_k\n",
    "    for i in range(0,p): \n",
    "        for j in range(0,set_k):  \n",
    "            if sens.iloc[i,j] > 0 :\n",
    "                       sens.iloc[i,j] = 1 \n",
    "            else: \n",
    "                       sens.iloc[i,j] = 0 \n",
    "#     print(sens)\n",
    "\n",
    "    sens1 = pd.get_dummies(sens, columns=['x'+str(k) for k in range(1,set_k+1)])\n",
    "    sensitive=sens1.T\n",
    "\n",
    "    print(sensitive)\n",
    "\n",
    "    # print(r.value_counts())\n",
    "    accu_all,DP_all,acceptance_rate,alpha_weight=main(sensitive, Y_test, Y_test_pred,e)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# import pulp as p \n",
    "# data= pd.read_csv('data/bank_train.csv',  skipinitialspace=True)\n",
    "# # print(data.head())\n",
    "# # print(data.shape[0],data.shape[1])\n",
    "# #sensitive columns name 0='age',2='marital'\n",
    "# sens=data[['age','marital']]\n",
    "# # print(sens)\n",
    "# r=data[['y']]\n",
    "# # print(r)\n",
    "# p=sens.shape[0]\n",
    "# for i in range(0,p):\n",
    "#     if sens.loc[i,\"age\"] > 60 or sens.loc[i,'age'] < 25 :\n",
    "#                sens.loc[i,\"age\"] = 1 \n",
    "#     else :\n",
    "#                sens.loc[i,\"age\"] = 2        \n",
    "#     if r.loc[i,'y'] == 1 :\n",
    "#                r.loc[i,\"y\"] = 1 \n",
    "#     else: \n",
    "#                r.loc[i,\"y\"] = 0  \n",
    "            \n",
    "            \n",
    "# sens1=pd.get_dummies(sens, columns=['age','marital'], prefix =['a','m'])\n",
    "# sensitive=sens1.T\n",
    "# print(sensitive)\n",
    "\n",
    "\n",
    "# print(r['y'].value_counts())\n",
    "# x=main(sensitive, r)\n",
    "\n",
    "# # r=main(sens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data= pd.read_csv('data/compass.csv', skipinitialspace=True)\n",
    "# print(data.head())\n",
    "# print(data.shape[0],data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data= pd.read_csv('data/crimes.csv',skipinitialspace=True)\n",
    "# print(data.head())\n",
    "# print(data.shape[0],data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data= pd.read_csv('data/german.csv' , skipinitialspace=True)\n",
    "# print(data.head())\n",
    "# print(data.shape[0],data.shape[1])\n",
    "\n",
    "# #sensitive columns name '12'='age','8'='gender/personal_status'\n",
    "# sens=data[['8','12']]\n",
    "# print(sens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column names to data set\n",
    "columns = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', \n",
    "           'relationship', 'race','sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income']\n",
    "\n",
    "# Read in train data\n",
    "adult_train = pd.read_csv('../input/adult/adult_train_data.csv', header=None, names=columns, skipinitialspace=True)\n",
    "\n",
    "# Drop the fnlwgt column which is useless for later analysis\n",
    "adult_train = adult_train.drop('fnlwgt', axis=1)\n",
    "\n",
    "# Read in test data\n",
    "adult_test = pd.read_csv('../input/adult/adult_test_data.csv', header=None, skiprows=1, names=columns, skipinitialspace=True)\n",
    "\n",
    "# Drop the fnlwgt column which is useless for later analysis\n",
    "adult_test = adult_test.drop('fnlwgt', axis=1)\n",
    "\n",
    "# Remove '.' in income column\n",
    "adult_test['income'] = adult_test['income'].apply(lambda x: '>50k' if x=='>50k.'  else '<=50k')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert '?' to NaNs and remove the entries with NaN value\n",
    "# Check missing value code and convert to NaNs\n",
    "object_col = adult_train.select_dtypes(include=object).columns.tolist()\n",
    "for col in object_col:\n",
    "    adult_train.loc[adult_train[col]=='?', col] = np.nan\n",
    "    adult_test.loc[adult_test[col]=='?', col] = np.nan\n",
    "\n",
    "# Perform an mssing assessment in each column of the dataset.\n",
    "col_missing_pct = adult_train.isna().sum()/adult_train.shape[0]\n",
    "col_missing_pct.sort_values(ascending=False)\n",
    "\n",
    "# Remove data entries with missing value\n",
    "adult_train = adult_train.dropna(axis=0, how='any')\n",
    "adult_test = adult_test.dropna(axis=0, how='any')\n",
    "\n",
    "# Show the results of the split\n",
    "print(\"After removing the missing value:\")\n",
    "print(\"Training set has {} samples.\".format(adult_train.shape[0]))\n",
    "print(\"Testing set has {} samples.\".format(adult_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " for col in object_col:\n",
    "    print(adult_train[col].value_counts(dropna=False)/adult_train.shape[0],'\\n')\n",
    "print(adult_train.head())\n",
    "print(adult_test.head())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_train.reset_index(drop=True, inplace=True)\n",
    "adult_test.reset_index(drop=True, inplace=True)\n",
    "p=adult_train.shape[0]\n",
    "q =adult_test.shape[0]\n",
    "# reducing dimensionality of some very sparse features\n",
    "for i in range(0,p):\n",
    "    if adult_train.loc[i,'native-country'] not in [\"united-states\"] :\n",
    "               adult_train.loc[i,\"native-country\"] = \"non-united-stated\"        \n",
    "    if adult_train.loc[i,\"education\"] in [\"Preschool\", \"1st-4th\", \"5th-6th\", \"7th-8th\"]:\n",
    "               adult_train.loc[i,\"education\"] = \"prim-middle-school\"\n",
    "    elif adult_train.loc[i,\"education\"] in [\"9th\", \"10th\", \"11th\", \"12th\"]:\n",
    "               adult_train.loc[i,\"education\"] = \"high-school\"   \n",
    "    if adult_train.loc[i,'income'] in [\">50k\"] :\n",
    "               adult_train.loc[i,\"income\"] = 1 \n",
    "    else: \n",
    "               adult_train.loc[i,\"income\"] = 0         \n",
    "#reducing dimensionality of some very sparse features\n",
    "for i in range(0,q):                \n",
    "    if adult_test.loc[i,'native-country'] not in [\"united-states\"]:\n",
    "               adult_test.loc[i,'native-country'] = \"Non-United-Stated\"\n",
    "    if adult_test.loc[i,'education'] in [\"Preschool\", \"1st-4th\", \"5th-6th\", \"7th-8th\"]:\n",
    "               adult_test.loc[i,'education'] = \"prim-middle-school\"\n",
    "    elif adult_test.loc[i,'education'] in [\"9th\", \"10th\", \"11th\", \"12th\"]:\n",
    "               adult_test.loc[i,'education'] = \"high-school\"   \n",
    "    if adult_test.loc[i,'native-country'] not in [\"united-states\"] :\n",
    "               adult_train.loc[i,\"native-country\"] = \"non-united-stated\"\n",
    "    if adult_test.loc[i,'income'] in [\">50k\"] :\n",
    "               adult_test.loc[i,\"income\"] = 1 \n",
    "    else: \n",
    "               adult_test.loc[i,\"income\"] = 0            \n",
    "print(adult_train.head())\n",
    "print(adult_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a scaler, then apply it to the features\n",
    "scaler = MinMaxScaler() # default=(0, 1)\n",
    "num_col = adult_train.dtypes[adult_train.dtypes != 'object'].index\n",
    "features_log_minmax_transform = pd.DataFrame(data = adult_train)\n",
    "features_log_minmax_transform[num_col] = scaler.fit_transform(features_log_minmax_transform[num_col])\n",
    "\n",
    "# Transform the test data set\n",
    "features_log_minmax_transform_test = pd.DataFrame(data = adult_test)\n",
    "features_log_minmax_transform_test[num_col] = scaler.transform(features_log_minmax_transform_test[num_col])\n",
    "\n",
    "# Show an example of a record with scaling applied\n",
    "display(features_log_minmax_transform.head())\n",
    "display(features_log_minmax_transform_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Data_train = pd.get_dummies(features_log_minmax_transform, columns=['workclass','education','marital-status','occupation','relationship','native-country'], prefix =['work','edu','ms','occ','rls','nc'])\n",
    "Data_train['INC'] = Data_train.loc[:,'income']\n",
    "Data_train=Data_train.drop(columns=['income','race','sex'])\n",
    "\n",
    "Data_test = pd.get_dummies(features_log_minmax_transform_test, columns=['workclass','education','marital-status','occupation','relationship','native-country'], prefix =['work','edu','ms','occ','rls','nc'])\n",
    "Data_test['INC'] = Data_test.loc[:,'income']\n",
    "Data_test=Data_test.drop(columns=['income','race','sex'])\n",
    "\n",
    "m=Data_train.shape[1]\n",
    "\n",
    "X_train=Data_train.iloc[:,0:m-1]\n",
    "Y_train=Data_train.iloc[:,m-1]\n",
    "print(m)\n",
    "\n",
    "m=Data_test.shape[1]\n",
    "X_test=Data_test.iloc[:,0:m-1]\n",
    "Y_test=Data_test.iloc[:,m-1]\n",
    "\n",
    "\n",
    "print(m)\n",
    "sensitive_attr_train=adult_train.drop(columns=['age','workclass','education','education-num','marital-status','occupation','relationship','capital-gain','capital-loss','hours-per-week','native-country','income'])\n",
    "sensitive_attr_train = pd.get_dummies(sensitive_attr_train, columns=['race','sex'], prefix =['r','s'])\n",
    "#print(sensitive_attr_train.head())\n",
    "sensitive_attr_test=adult_test.drop(columns=['age','workclass','education','education-num','marital-status','occupation','relationship','capital-gain','capital-loss','hours-per-week','native-country','income'])\n",
    "sensitive_attr_test = pd.get_dummies(sensitive_attr_test, columns=['race','sex'], prefix =['r','s'])\n",
    "#print(sensitive_attr_train.head())\n",
    "\n",
    "\n",
    "# display(X_train.head())\n",
    "# display(X_test.head())\n",
    "# display(Y_train.head())\n",
    "# display(Y_test.head())\n",
    "\n",
    "\n",
    "print(Y_test.isin(['>50k'])) \n",
    "\n",
    "\n",
    "\n",
    "print(Y_test.dtype)\n",
    "Y_test=Y_test.astype('int')\n",
    "Y_train=Y_train.astype('int')\n",
    "\n",
    "print(Y_test.dtype)\n",
    "# print(X_train.shape[0])\n",
    "# print(X_test.shape[0])\n",
    "# print(Y_train.shape[0])\n",
    "# print(Y_test.shape[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM \n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline\n",
    "from random import *\n",
    "from subprocess import check_output\n",
    "def Adult_svm(X_train,X_test,Y_train,Y_test):\n",
    "    #Split data into training and test datasets (training will be based on 70% of data)\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0,shuffle=False) \n",
    "    #test_size: if integer, number of examples into test dataset; if between 0.0 and 1.0, means proportion\n",
    "    print('There are {} samples in the training set and {} samples in the test set'.format(X_train.shape[0], X_test.shape[0]))\n",
    "\n",
    "\n",
    "    #Scaling data\n",
    "    #from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    from sklearn.model_selection import cross_val_score\n",
    "\n",
    "    #sc = StandardScaler(with_mean=False)\n",
    "    \n",
    "    \n",
    "    #sc.fit(X_train)\n",
    "    #X_train_std = sc.transform(X_train)\n",
    "    #X_test_std = sc.transform(X_test)\n",
    "\n",
    "    #X_train_std and X_test_std are the scaled datasets to be used in algorithms\n",
    "\n",
    "    #Applying SVC (Support Vector Classification)\n",
    "    from sklearn.svm import SVC\n",
    "    svm = SVC(kernel='rbf', random_state=0, gamma=.1, C=10.0)\n",
    "    svm.fit(X_train, Y_train)\n",
    "    print('The accuracy of the SVM classifier on training data is {:.2f}'.format(svm.score(X_train, Y_train)))\n",
    "    print('The accuracy of the SVM classifier on test data is {:.2f}'.format(svm.score(X_test, Y_test)))\n",
    "    print('####Train prediction Label###############################################')\n",
    "    Y_train_pred=svm.predict(X_train)\n",
    "    #print(y_1)\n",
    "    Y_test_pred=svm.predict(X_test)\n",
    "\n",
    "    print('####Actual Train Label###############################################')\n",
    "\n",
    "\n",
    "    print('####Change to colors###############################################')\n",
    "        \n",
    "    \n",
    "    return Y_train_pred,Y_train,Y_test_pred,Y_test\n",
    "     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Adult_rf(X_train,X_test,Y_train,Y_test):   \n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    rf = RandomForestClassifier(n_estimators= 100, random_state=42)\n",
    "    # Extract the two most important features\n",
    "    \n",
    "    \n",
    "    # Train the random forest\n",
    "    rf.fit(X_train, Y_train)\n",
    "    # Make predictions and determine the error\n",
    "    Y_train_pred = rf.predict(X_train)\n",
    "    Y_test_pred = rf.predict(X_test)\n",
    "    print('The accuracy of the SVM classifier on training data is {:.2f}'.format(rf.score(X_train, Y_train)))\n",
    "    print('The accuracy of the SVM classifier on test data is {:.2f}'.format(rf.score(X_test, Y_test)))\n",
    "    \n",
    "    return Y_train_pred,Y_train,Y_test_pred,Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Adult_mlp(X_train,X_test,Y_train,Y_test):   \n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    mlp = MLPClassifier(solver='adam', hidden_layer_sizes=(53, 2),alpha=1e-5, random_state=42)\n",
    "    # Extract the two most important features\n",
    "    \n",
    "    \n",
    "    # Train the random forest\n",
    "    mlp.fit(X_train, Y_train)\n",
    "    # Make predictions and determine the error\n",
    "    Y_train_pred = mlp.predict(X_train)\n",
    "    Y_test_pred = mlp.predict(X_test)\n",
    "    print('The accuracy of the SVM classifier on training data is {:.2f}'.format(mlp.score(X_train, Y_train)))\n",
    "    print('The accuracy of the SVM classifier on test data is {:.2f}'.format(mlp.score(X_test, Y_test)))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return Y_train_pred,Y_train,Y_test_pred,Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#table1 for Adult\n",
    "\n",
    "import time\n",
    "import pulp as p \n",
    "def min_max_lp_all2(data1,gamma,eps,r,delta):\n",
    "    \n",
    "    m=data1.shape[0]\n",
    "    n=data1.shape[1]\n",
    "    print('dimension of data')\n",
    "    print(m,n)\n",
    "    Lp_prob = p.LpProblem('Problem', p.LpMinimize)  \n",
    "   \n",
    "    \n",
    "    X=np.zeros(n+1,dtype=p.LpVariable)\n",
    "    sizes=np.zeros(m,dtype=int)\n",
    "    for i in range(m):\n",
    "        count=0\n",
    "        for j in range(n):\n",
    "            if data1[i][j]==1:\n",
    "                count=count+1\n",
    "                \n",
    "        sizes[i]=count\n",
    "  \n",
    "\n",
    "    for i in range(n):\n",
    "        var1=str(i)\n",
    "        \n",
    "        X[i]=p.LpVariable(var1,lowBound=0,upBound=1,cat='Integer')\n",
    "       \n",
    "        \n",
    "    X[n]=  p.LpVariable(\"z1\",lowBound=0)\n",
    "    #X[n+1]=  p.LpVariable(\"z2\",lowBound=0)\n",
    "\n",
    "\n",
    "    #########objective function#####################\n",
    "    Lp_prob += X[n] \n",
    "    #Lp_prob += 1 \n",
    "\n",
    "\n",
    "    ##############constraint#################\n",
    "    for i in range(2*m):\n",
    "        if i<m:\n",
    "            Lp_prob += X[n] >= p.lpSum([2*(X[j]-0.5)*data1[i][j] for j in range(n)])\n",
    "            Lp_prob += p.lpSum([2*(X[j]-0.5)*data1[i][j] for j in range(n)]) >= (2*gamma-1)*sizes[i]\n",
    "            Lp_prob += p.lpSum([2*(X[j]-0.5)*data1[i][j] for j in range(n)]) <= ((2*gamma-1)+eps)*sizes[i]\n",
    "            \n",
    "        else:        \n",
    "            Lp_prob += X[n] >= p.lpSum([-1*2*(X[j]-0.5)*data1[i-m][j] for j in range(n)])\n",
    "            #Lp_prob += X[n+1] >= p.lpSum([-1*2*(X[j]-0.5)+r[j] for j in range(n)]) \n",
    "#     Lp_prob += X[n+1] >= p.lpSum([2*(X[j]-0.5)-r[j] for j in range(n)])\n",
    "#     Lp_prob += X[n+1] >= p.lpSum([-1*2*(X[j]-0.5)+r[j] for j in range(n)])       \n",
    "         \n",
    "    Lp_prob += p.lpSum([2*(X[i]-0.5)*r[i] for i in range(n)])>=delta*n\n",
    "    #n is the number of elements in sensitive attribute \n",
    "           \n",
    "       \n",
    "    Lp_prob += X[n] <= 42000\n",
    "    \n",
    "    #####################################\n",
    "    status = Lp_prob.solve()   # Solver \n",
    "    print(p.LpStatus[status]) \n",
    "    print(\"discripency is:\")        \n",
    "    print(p.value(Lp_prob.objective))\n",
    "    x=np.zeros(n,dtype=float)\n",
    "\n",
    "   # The solution status \n",
    "    Synth1={}\n",
    "    Synth2={}\n",
    "    # # Printing the final solution \n",
    "    for i in range(n):\n",
    "        if(p.value(X[i])==1):\n",
    "            Synth1[i]=1 \n",
    "            Synth2[i]=-1\n",
    "        else:\n",
    "            Synth1[i]=-1\n",
    "            Synth2[i]=1\n",
    "    Synthu1=Synth1  \n",
    "    Synthu2=Synth2  \n",
    "    \n",
    "              \n",
    "    return Synthu1,Synthu2   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(sensitive_attr_test)\n",
    "sens_train=sensitive_attr_train.transpose()\n",
    "sens_test=sensitive_attr_test.transpose()\n",
    "#gamma=0.1\n",
    "#eps=0.1\n",
    "#min_max_lp_all(sens_test,gamma,eps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_pred,Y_train,Y_test_pred,Y_test=Adult_svm(X_train,X_test,Y_train,Y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_pred,Y_train,Y_test_pred,Y_test=Adult_rf(X_train,X_test,Y_train,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_pred,Y_train,Y_test_pred,Y_test=Adult_mlp(X_train,X_test,Y_train,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########Feldman----nonwhite white female male \n",
    "\n",
    "#data1=sens_test\n",
    "\n",
    "# min_max_lp_all(data1,gamma,eps,r):\n",
    "\n",
    "rows=Y_test.shape[0]   \n",
    "\n",
    "###############################################33       \n",
    "r = np.zeros(rows, dtype = int)\n",
    "for i in range(rows):\n",
    "    if Y_test.iloc[i]==0 :\n",
    "        r[i]=-1\n",
    "    else :\n",
    "        r[i]= 1  \n",
    "r2 = np.zeros(rows, dtype = int)        \n",
    "        \n",
    "for i in range(rows):\n",
    "    if Y_test_pred[i]==0 :\n",
    "        r2[i]=-1\n",
    "    else :\n",
    "        r2[i]= 1        \n",
    "        \n",
    "data = np.zeros((4, rows), dtype = float)\n",
    "\n",
    "a=0\n",
    "b=0\n",
    "c=0\n",
    "d=0\n",
    "\n",
    "acc1=0\n",
    "acc2=0\n",
    "acc3=0\n",
    "acc4=0\n",
    "\n",
    "\n",
    "for i in range(rows):\n",
    "    if (sensitive_attr_test.iloc[i,0]==1 or sensitive_attr_test.iloc[i,1]==1 or sensitive_attr_test.iloc[i,2]==1 or sensitive_attr_test.iloc[i,3]==1):\n",
    "            if(sensitive_attr_test.iloc[i,5]==1):\n",
    "                data[0][i]= 1\n",
    "                a=a+1\n",
    "                if r[i]==1:\n",
    "                    acc1=acc1+1\n",
    "    elif sensitive_attr_test.iloc[i,4]==1:\n",
    "            if(sensitive_attr_test.iloc[i,5]==1):\n",
    "                data[1][i]= 1\n",
    "                b=b+1\n",
    "                if r[i]==1:\n",
    "                    acc2=acc2+1\n",
    "    if (sensitive_attr_test.iloc[i,0]==1 or sensitive_attr_test.iloc[i,1]==1 or sensitive_attr_test.iloc[i,2]==1 or sensitive_attr_test.iloc[i,3]==1):\n",
    "           if(sensitive_attr_test.iloc[i,6]==1):\n",
    "                data[2][i]= 1\n",
    "                c=c+1\n",
    "                if r[i]==1:\n",
    "                    acc3=acc3+1\n",
    "    elif sensitive_attr_test.iloc[i,4]==1:\n",
    "           if(sensitive_attr_test.iloc[i,6]==1):\n",
    "                data[3][i]= 1\n",
    "                d=d+1\n",
    "                if r[i]==1:\n",
    "                    acc4=acc4+1   \n",
    "print(a,b,c,d)              \n",
    "print(acc1,acc2,acc3,acc4)\n",
    "a1=float(acc1/a)\n",
    "b1=float(acc2/b)\n",
    "c1=float(acc3/c)\n",
    "d1=float(acc4/d)\n",
    "\n",
    "\n",
    "print(a1,b1,c1,d1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###########################################\n",
    "\n",
    "\n",
    "\n",
    "# for gamma in np.arange(0.15,0.5,.05):\n",
    "#     for eps in np.arange(0.05,0.02,-.01): \n",
    "gamma=0.16\n",
    "eps=0.06\n",
    "for gamma in np.arange(.05,.28,0.05):\n",
    "    acc1=0\n",
    "    acc2=0\n",
    "    acc3=0\n",
    "    acc4=0\n",
    "   \n",
    "\n",
    "\n",
    "    u1,u2=min_max_lp_all(data,gamma,eps,r)\n",
    "\n",
    "    #######################Disp_impact#######################  \n",
    "\n",
    "    for i in range(rows):\n",
    "            if data[0][i]== 1 and u1[i]==1:\n",
    "                    acc1=acc1+1\n",
    "            elif data[1][i]== 1 and u1[i]==1:\n",
    "                    acc2=acc2+1  \n",
    "            elif data[2][i]== 1 and u1[i]==1:\n",
    "                    acc3=acc3+1 \n",
    "            elif data[3][i]== 1 and u1[i]==1:\n",
    "                    acc4=acc4+1                \n",
    "\n",
    "    a1=float(acc1/a)\n",
    "    b1=float(acc2/b)\n",
    "    c1=float(acc3/c)\n",
    "    d1=float(acc4/d)\n",
    "#    print(acc1,acc2,acc3,acc4)\n",
    "#    print(a1,b1,c1,d1)\n",
    "\n",
    "#     count1=0\n",
    "#     count2=0\n",
    "#     for j in range(r.shape[0]):\n",
    "#             if(r[j]==u1[j]):\n",
    "#                 count1+=1\n",
    "#     acc1=float(count1/r.shape[0])        \n",
    "\n",
    "#     for j in range(r.shape[0]):\n",
    "#             if(r[j]==u2[j]):\n",
    "#                 count2+=1\n",
    "#     acc2=float(count2/r.shape[0]) \n",
    "#     print(acc1)\n",
    "#     print(acc2)\n",
    "#     print(gamma)\n",
    "#     print(eps)\n",
    "    \n",
    "#     acc2_l.append(acc2)\n",
    "#     acc1_l.append(acc1)\n",
    "\n",
    "\n",
    "#     print(\"Accuracy::\")    \n",
    "#     print(acc1_l)     \n",
    "#     print(acc2_l)\n",
    "\n",
    "#     ###########################\n",
    "#     a_acc=0\n",
    "#     b_acc=0\n",
    "\n",
    "    fi= np.zeros(rows,dtype=int) \n",
    "    count=0\n",
    "    acc=0\n",
    "    ci=[]\n",
    "    for alpha in np.arange(0,1.05,0.05):\n",
    "        f_acc=0\n",
    "        acc1=0\n",
    "        acc2=0\n",
    "        acc3=0\n",
    "        acc4=0\n",
    "       \n",
    "        for i in range(rows):\n",
    "\n",
    "            z=random()\n",
    "            if z < alpha:\n",
    "                    fi[i]= u1[i] \n",
    "                    count=count+1\n",
    "            else:\n",
    "                   fi[i]= r2[i]\n",
    "\n",
    "        for i in range(rows):\n",
    "             if fi[i] == r[i]:\n",
    "                    f_acc=f_acc+1\n",
    "\n",
    "\n",
    "        f_acc_percent=f_acc/rows\n",
    "        ci.append(f_acc_percent)\n",
    "\n",
    "        for i in range(rows):\n",
    "            if data[0][i]== 1 and fi[i]==1:\n",
    "                    acc1=acc1+1\n",
    "            elif data[1][i]== 1 and fi[i]==1:\n",
    "                    acc2=acc2+1  \n",
    "            elif data[2][i]== 1 and fi[i]==1:\n",
    "                    acc3=acc3+1 \n",
    "            elif data[3][i]== 1 and fi[i]==1:\n",
    "                    acc4=acc4+1         \n",
    "            \n",
    "\n",
    "\n",
    "        a1=float(acc1/a)\n",
    "        b1=float(acc2/b)\n",
    "        c1=float(acc3/c)\n",
    "        d1=float(acc4/d)\n",
    "       \n",
    "        print(acc1,acc2,acc3,acc4)\n",
    "        print(a1,b1,c1,d1)\n",
    "    print(ci)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "########Bilalzafar  (race-5) + (gender-2)\n",
    "\n",
    "#data1=sens_test\n",
    "\n",
    "# min_max_lp_all(data1,gamma,eps,r):\n",
    "\n",
    "rows=Y_test.shape[0]   \n",
    "\n",
    "###############################################33       \n",
    "r = np.zeros(rows, dtype = int)\n",
    "for i in range(rows):\n",
    "    if Y_test.iloc[i]==0 :\n",
    "        r[i]=-1\n",
    "    else :\n",
    "        r[i]= 1  \n",
    "r2 = np.zeros(rows, dtype = int)        \n",
    "        \n",
    "for i in range(rows):\n",
    "    if Y_test_pred[i]==0 :\n",
    "        r2[i]=-1\n",
    "    else :\n",
    "        r2[i]= 1        \n",
    "        \n",
    "data = np.zeros((7, rows), dtype = float)\n",
    "\n",
    "a=0\n",
    "b=0\n",
    "c=0\n",
    "d=0\n",
    "e=0\n",
    "f=0\n",
    "g=0\n",
    "\n",
    "acc1=0\n",
    "acc2=0\n",
    "acc3=0\n",
    "acc4=0\n",
    "acc5=0\n",
    "acc6=0\n",
    "acc7=0\n",
    "\n",
    "\n",
    "for i in range(rows):\n",
    "    if (sensitive_attr_test.iloc[i,0]==1):\n",
    "                data[0][i]= 1\n",
    "                a=a+1\n",
    "                if r[i]==1:\n",
    "                    acc1=acc1+1\n",
    "    elif (sensitive_attr_test.iloc[i,1]==1):\n",
    "                data[1][i]= 1\n",
    "                b=b+1\n",
    "                if r[i]==1:\n",
    "                    acc2=acc2+1\n",
    "    elif (sensitive_attr_test.iloc[i,2]==1):\n",
    "                data[2][i]= 1\n",
    "                c=c+1\n",
    "                if r[i]==1:\n",
    "                    acc3=acc3+1  \n",
    "    elif (sensitive_attr_test.iloc[i,3]==1):\n",
    "                data[3][i]= 1\n",
    "                d=d+1\n",
    "                if r[i]==1:\n",
    "                    acc4=acc4+1\n",
    "    elif (sensitive_attr_test.iloc[i,4]==1):\n",
    "                data[4][i]= 1\n",
    "                e=e+1\n",
    "                if r[i]==1:\n",
    "                    acc5=acc5+1\n",
    "    if (sensitive_attr_test.iloc[i,5]==1):\n",
    "                data[5][i]= 1\n",
    "                f=f+1\n",
    "                if r[i]==1:\n",
    "                    acc6=acc6+1\n",
    "    elif (sensitive_attr_test.iloc[i,6]==1):\n",
    "                data[6][i]= 1\n",
    "                g=g+1\n",
    "                if r[i]==1:\n",
    "                    acc7=acc7+1                \n",
    "print(a,b,c,d,e,f,g)              \n",
    "print(acc1,acc2,acc3,acc4,acc5,acc6,acc7)\n",
    "a1=float(acc1/a)\n",
    "b1=float(acc2/b)\n",
    "c1=float(acc3/c)\n",
    "d1=float(acc4/d)\n",
    "e1=float(acc5/e)\n",
    "f1=float(acc6/f)\n",
    "g1=float(acc7/g)\n",
    "\n",
    "\n",
    "\n",
    "print(a1,b1,c1,d1,e1,f1,g1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###########################################\n",
    "\n",
    "\n",
    "\n",
    "# for gamma in np.arange(0.15,0.5,.05):\n",
    "#     for eps in np.arange(0.05,0.02,-.01): \n",
    "#gamma=0.16\n",
    "eps=0.03\n",
    "delta=.80\n",
    "for gamma in np.arange(.10,.15,0.05):\n",
    "    a1=0\n",
    "    b1=0\n",
    "    c1=0\n",
    "    d1=0\n",
    "    e1=0\n",
    "    f1=0\n",
    "    g1=0\n",
    "   \n",
    "\n",
    "\n",
    "    u1,u2=min_max_lp_all2(data,gamma,eps,r2,delta)\n",
    "\n",
    "    #######################Disp_impact#######################  \n",
    "\n",
    "    for i in range(rows):\n",
    "            if data[0][i]== 1 and u1[i]==1:\n",
    "                    acc1=acc1+1\n",
    "            elif data[1][i]== 1 and u1[i]==1:\n",
    "                    acc2=acc2+1  \n",
    "            elif data[2][i]== 1 and u1[i]==1:\n",
    "                    acc3=acc3+1 \n",
    "            elif data[3][i]== 1 and u1[i]==1:\n",
    "                    acc4=acc4+1 \n",
    "            elif data[4][i]== 1 and u1[i]==1:\n",
    "                    acc5=acc5+1  \n",
    "            if data[5][i]== 1 and u1[i]==1:\n",
    "                    acc6=acc6+1 \n",
    "            elif data[6][i]== 1 and u1[i]==1:\n",
    "                    acc7=acc7+1        \n",
    "\n",
    "    a1=float(acc1/a)\n",
    "    b1=float(acc2/b)\n",
    "    c1=float(acc3/c)\n",
    "    d1=float(acc4/d)\n",
    "    e1=float(acc5/e)\n",
    "    f1=float(acc6/f)\n",
    "    g1=float(acc7/g)\n",
    "#    print(acc1,acc2,acc3,acc4)\n",
    "#    print(a1,b1,c1,d1)\n",
    "\n",
    "#     count1=0\n",
    "#     count2=0\n",
    "#     for j in range(r.shape[0]):\n",
    "#             if(r[j]==u1[j]):\n",
    "#                 count1+=1\n",
    "#     acc1=float(count1/r.shape[0])        \n",
    "\n",
    "#     for j in range(r.shape[0]):\n",
    "#             if(r[j]==u2[j]):\n",
    "#                 count2+=1\n",
    "#     acc2=float(count2/r.shape[0]) \n",
    "#     print(acc1)\n",
    "#     print(acc2)\n",
    "#     print(gamma)\n",
    "#     print(eps)\n",
    "    \n",
    "#     acc2_l.append(acc2)\n",
    "#     acc1_l.append(acc1)\n",
    "\n",
    "\n",
    "#     print(\"Accuracy::\")    \n",
    "#     print(acc1_l)     \n",
    "#     print(acc2_l)\n",
    "\n",
    "#     ###########################\n",
    "#     a_acc=0\n",
    "#     b_acc=0\n",
    "\n",
    "    fi= np.zeros(rows,dtype=int) \n",
    "    count=0\n",
    "    acc=0\n",
    "    ci=[]\n",
    "    k=0\n",
    "    di= [] \n",
    "    for alpha in np.arange(0,1.05,0.05):\n",
    "        f_acc=0\n",
    "        acc1=0\n",
    "        acc2=0\n",
    "        acc3=0\n",
    "        acc4=0\n",
    "        acc5=0\n",
    "        acc6=0\n",
    "        acc7=0\n",
    "       \n",
    "        for i in range(rows):\n",
    "\n",
    "            z=random()\n",
    "            if z < alpha:\n",
    "                    fi[i]= u1[i] \n",
    "                    count=count+1\n",
    "            else:\n",
    "                   fi[i]= r2[i]\n",
    "\n",
    "        for i in range(rows):\n",
    "             if fi[i] == r[i]:\n",
    "                    f_acc=f_acc+1\n",
    "\n",
    "\n",
    "        f_acc_percent=f_acc/rows\n",
    "        ci.append(f_acc_percent)\n",
    "\n",
    "        for i in range(rows):\n",
    "            if data[0][i]== 1 and fi[i]==1:\n",
    "                    acc1=acc1+1\n",
    "            elif data[1][i]== 1 and fi[i]==1:\n",
    "                    acc2=acc2+1  \n",
    "            elif data[2][i]== 1 and fi[i]==1:\n",
    "                    acc3=acc3+1 \n",
    "            elif data[3][i]== 1 and fi[i]==1:\n",
    "                    acc4=acc4+1         \n",
    "            elif data[4][i]== 1 and fi[i]==1:\n",
    "                    acc5=acc5+1  \n",
    "            if data[5][i]== 1 and fi[i]==1:\n",
    "                    acc6=acc6+1 \n",
    "            elif data[6][i]== 1 and fi[i]==1:\n",
    "                    acc7=acc7+1   \n",
    "\n",
    "\n",
    "        a1=float(acc1/a)\n",
    "        b1=float(acc2/b)\n",
    "        c1=float(acc3/c)\n",
    "        d1=float(acc4/d)\n",
    "        e1=float(acc5/e)\n",
    "        f1=float(acc6/f)\n",
    "        g1=float(acc7/g)\n",
    "        #print(acc1,acc2,acc3,acc4,acc4,acc6,acc7)\n",
    "        #print(a1,b1,c1,d1,e1,f1,g1)\n",
    "        k=max(a1,b1,c1,d1,e1,f1,g1)-min(a1,b1,c1,d1,e1,f1,g1)\n",
    "        di.append(k)\n",
    "    print(ci)\n",
    "    print(di)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=2\n",
    "b=3\n",
    "print(\"gamma\",a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accp=[0.5229226361031518, 0.49236192714453586, 0.7256857855361596, 0.2756892230576441, 0.4561891515994437, 0.520523497917906, 0.5311953352769679, 0.42627737226277373, 0.3021885521885522, 0.6963696369636964, 0.47635135135135137, 0.5094026548672567, 0.5344311377245509, 0.4884526558891455, 0.27186147186147186, 0.7140562248995984, 0.2848378615249781, 0.6973788721207307, 0.5802098950524738, 0.4708597807270629]\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import savgol_filter\n",
    "np.set_printoptions(precision=4)  # For compact display.\n",
    "plt.style.use('seaborn-white')\n",
    "\n",
    "\n",
    "'''\n",
    "y1=savgol_filter(a, 6, 2)\n",
    "y2=savgol_filter(b, 6, 2)\n",
    "y3=savgol_filter(c, 6, 2)\n",
    "y4=savgol_filter(d, 6, 2)\n",
    "y5=savgol_filter(e, 6, 2)\n",
    "y6=savgol_filter(f, 6, 2)\n",
    "y7=savgol_filter(g, 6, 2)\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "num=[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n",
    "\n",
    "ax.plot(num,accp,label=' Acceptance Rate(for each group)',color='blue',marker='^',linestyle='--')\n",
    "#ax.vlines(y=[.1992], ymin=[0], ymax=[1], colors='purple', linestyles='--', lw=2, label='PRedict avg. acc.')\n",
    "#plt.axvline(.1992, color='green', linestyle='--')\n",
    "plt.axhline(0.72568, color='orange', label='Max Acc. Rate',linestyle='--')\n",
    "plt.axhline(0.2718, color='orange', label='Min Acc. Rate',linestyle='--')\n",
    "\n",
    "   \n",
    "plt.title('Synthetic Dataset')\n",
    "ax.set_xlabel('Sensitive Groups')\n",
    "ax.set_ylabel('Acceptance Rate') \n",
    "# ax.set_ylabel('% in +ve class (Acceptance Rate)') \n",
    "\n",
    "ax.legend(loc='lower center', bbox_to_anchor=(0.5, -0.30), shadow=True, ncol=10)\n",
    "plt.show() \n",
    "fig.savefig('a2.png') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
