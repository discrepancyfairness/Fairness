{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries necessary for this project\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "from time import time\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Import 'GridSearchCV', 'make_scorer', and any other necessary libraries\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Import the three supervised learning models from sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA    \n",
    "\n",
    "# Pretty display for notebooks\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/subham/anaconda3/envs/tensorflow/lib/python3.6/site-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['seed', 'shuffle', 'time', 'uniform', 'random', 'choice', 'randint', 'e', 'triangular', 'sample']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "#without accuracy ---> 2\n",
    "#SVM \n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline\n",
    "from random import *\n",
    "from subprocess import check_output\n",
    "def Propublica_svm(X,Y):\n",
    "    #Split data into training and test datasets (training will be based on 70% of data)\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0,shuffle=True) \n",
    "    #test_size: if integer, number of examples into test dataset; if between 0.0 and 1.0, means proportion\n",
    "    print('There are {} samples in the training set and {} samples in the test set'.format(X_train.shape[0], X_test.shape[0]))\n",
    "\n",
    "    \n",
    "    #Scaling data\n",
    "    #from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    from sklearn.model_selection import cross_val_score\n",
    "\n",
    "    #sc = StandardScaler(with_mean=False)\n",
    "    \n",
    "    \n",
    "    #sc.fit(X_train)\n",
    "    #X_train_std = sc.transform(X_train)\n",
    "    #X_test_std = sc.transform(X_test)\n",
    "\n",
    "    #X_train_std and X_test_std are the scaled datasets to be used in algorithms\n",
    "\n",
    "    #Applying SVC (Support Vector Classification)\n",
    "    from sklearn.svm import SVC\n",
    "    svm = SVC(kernel='rbf', random_state=1, gamma=.1, C=10.0,probability=True)\n",
    "    svm.fit(X_train, Y_train)\n",
    "    print('The accuracy of the SVM classifier on training data is {:.2f}'.format(svm.score(X_train, Y_train)))\n",
    "    print('The accuracy of the SVM classifier on test data is {:.2f}'.format(svm.score(X_test, Y_test)))\n",
    "    print('####Train prediction Label###############################################')\n",
    "    Y_train_pred=svm.predict(X_train)\n",
    "    #print(y_1)\n",
    "    Y_test_pred=svm.predict(X_test)\n",
    "\n",
    "    print('####Actual Train Label###############################################')\n",
    "\n",
    "\n",
    "    print('####Change to colors###############################################')\n",
    "    e=svm.predict_proba(X_test)\n",
    "    print(e)\n",
    "    print(Y_test_pred)\n",
    "    \n",
    "    return X_test,Y_test_pred,Y_test,e\n",
    "     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#without accuracy ---> 2\n",
    "def main(datax, y_test, y_test_pred,e): \n",
    "        \n",
    "    n=datax.shape[1]\n",
    "    s=datax.shape[0]    \n",
    "    data = datax\n",
    "    \n",
    "    r = np.zeros(n, dtype = int) \n",
    "    \n",
    "    for i in range(n):\n",
    "        if int(y_test.iloc[i])==1 :\n",
    "            r[i]=1\n",
    "        else :\n",
    "            r[i]= -1  \n",
    "    \n",
    "    r2 = np.zeros(n, dtype = int) \n",
    "    for i in range(n):\n",
    "        if int(y_test_pred[i])==1 :\n",
    "            r2[i]=1\n",
    "        else :\n",
    "            r2[i]= -1          \n",
    "    ar=[]\n",
    "    \n",
    "    for j in range(s):\n",
    "        print(\"sensitive attribute \",(j+1)) \n",
    "        a=0\n",
    "        b=0\n",
    "        acc1=0\n",
    "        acc2=0\n",
    "        for i in range(n):\n",
    "                if data[j][i]== 1 :\n",
    "                    a=a+1\n",
    "                    if r[i]==1:\n",
    "                         acc1=acc1+1 \n",
    "\n",
    "        print(\"ACTUAL----------total ,accepted, aceeptance rate:\")             \n",
    "        a1=float(acc1/a)\n",
    "        print(a)\n",
    "        \n",
    "        print(acc1)\n",
    "        print(a1)\n",
    "        ar.append(a1)\n",
    "        \n",
    "    maxi= max(ar)\n",
    "    mini= min(ar)\n",
    "    DP=float(maxi-mini)\n",
    "    print(\"data acceptance rates\")\n",
    "    print(ar)\n",
    "    print(\"data DP\")\n",
    "    print(DP)\n",
    "    \n",
    "    ar=[]\n",
    "    \n",
    "    for j in range(s):\n",
    "        print(\"sensitive attribute \",(j+1)) \n",
    "        a=0\n",
    "        b=0\n",
    "        acc1=0\n",
    "        acc2=0\n",
    "        prec=0\n",
    "        reca=0\n",
    "        accur=0\n",
    "        FP=0\n",
    "        FN=0\n",
    "        TP=0\n",
    "        TN=0\n",
    "        for i in range(n):\n",
    "             if data[j][i]== 1 :\n",
    "                    a=a+1\n",
    "                    if r2[i]==1:\n",
    "                        acc1=acc1+1 \n",
    "                        if r[i]==1:\n",
    "                            TP=TP+1\n",
    "                        else:\n",
    "                             FP=FP+1                \n",
    "                    else:\n",
    "                        if r[i]==1:\n",
    "                            FN=FN+1\n",
    "                        else:\n",
    "                            TN=TN+1    \n",
    "        \n",
    "        print(\"prec reca accuracy for each sens\") \n",
    "        prec= float(TP/(TP+FP))\n",
    "        reca= float(TP/(TP+FN))\n",
    "        accur= float((TP+TN)/a)\n",
    "        print(prec,reca,accur)\n",
    "        \n",
    "        print(\"SVM----------total , accepted, aceeptance rate:\")             \n",
    "        \n",
    "        a1=float(acc1/a)\n",
    "        print(a)\n",
    "        \n",
    "        print(acc1)\n",
    "        print(a1)\n",
    "        ar.append(a1)\n",
    "        \n",
    "    maxi= max(ar)\n",
    "    mini= min(ar)\n",
    "    DP=float(maxi-mini)\n",
    "    print(\"data acceptance rates\")\n",
    "    print(ar)\n",
    "    print(\"data DP\")\n",
    "    print(DP) \n",
    "    \n",
    "    print(\"SVM accuracy--------------------------\")\n",
    "    prec=0\n",
    "    reca=0\n",
    "    accur=0\n",
    "    FP=0\n",
    "    FN=0\n",
    "    TP=0\n",
    "    TN=0\n",
    "    for i in range(n):\n",
    "            if r2[i]==1:\n",
    "                acc1=acc1+1 \n",
    "                if r[i]==1:\n",
    "                    TP=TP+1\n",
    "                else:\n",
    "                     FP=FP+1                \n",
    "            else:\n",
    "                if r[i]==1:\n",
    "                     FN=FN+1\n",
    "                else:\n",
    "                     TN=TN+1    \n",
    "\n",
    "        \n",
    "    prec= float(TP/(TP+FP))\n",
    "    reca= float(TP/(TP+FN))\n",
    "    accur= float((TP+TN)/n)\n",
    "    print(prec,reca,accur)\n",
    "    \n",
    "    \n",
    "\n",
    "   \n",
    "   \n",
    "    gamma=[[0.4451345755693582, 0.41134751773049644, 0.462033462033462, 0.46994535519125685, 0.4375, 0.4635416666666667, 0.45555555555555555, 0.4444444444444444],\n",
    "           [0.4451345755693582, 0.41134751773049644, 0.47619047619047616, 0.46994535519125685, 0.4375, 0.475, 0.4642857142857143, 0.4444444444444444],\n",
    "           [0.4306418219461698, 0.41134751773049644, 0.4774774774774775, 0.46994535519125685, 0.42628205128205127, 0.47604166666666664, 0.4595238095238095, 0.4444444444444444],\n",
    "           [0.4451345755693582, 0.41134751773049644, 0.47619047619047616, 0.4918032786885246, 0.4375, 0.4791666666666667, 0.4642857142857143, 0.4567901234567901]]\n",
    "    alpha=[[1,1,1,1]]\n",
    "    #alpha=[[1,1,1,1],[1,1,2,1],[1,1,4,1],[1,1,6,1],[1,1,8,1],[1,1,10,1],[1,1,15,1],[1,1,.20,1],[1,1,25,1],[1,1,30,1],[1,1,.1,1]]\n",
    "    epsilon=[.01]\n",
    "    fi= np.zeros(n,dtype=int) \n",
    "    new=0\n",
    "    \n",
    "    \n",
    "    #for t in range(gamma.shape[0]):\n",
    "    for new in range(4):   \n",
    "        for t in range(1):\n",
    "            for eps in epsilon:\n",
    "                u1,u2=min_sum_lpca_g(data,gamma[new],eps,e,alpha[t])\n",
    "                #######################Disp_impact#######################  \n",
    "                print(\"gamma-epsilon-delta\",gamma[new],eps)\n",
    "                accu_all=[]\n",
    "                DP_all=[]\n",
    "                precision_all=[]\n",
    "                recall_all=[]\n",
    "                ar_all=[]\n",
    "                acceptance_rate = np.zeros( (7,28), dtype=float)\n",
    "                count=0\n",
    "                print(\"<--------------------------------------->\")\n",
    "                print(\"iteration t\",t)\n",
    "        #                 for alpha in np.arange(0,1.05,0.05):\n",
    "        #                     print(\"alpha: \",alpha)\n",
    "        #                     for i in range(n):\n",
    "\n",
    "        #                         z=random()\n",
    "        #                         if z < alpha:\n",
    "        #                                fi[i]= u1[i] \n",
    "\n",
    "        #                         else:\n",
    "        #                                fi[i]= r2[i]\n",
    "\n",
    "                for i in range(n):\n",
    "                     fi[i] = u1[i]\n",
    "\n",
    "\n",
    "                for j in range(s):\n",
    "                    print(\"sensitive attribute \",(j+1)) \n",
    "\n",
    "                    TP=0\n",
    "                    FP=0\n",
    "                    FN=0\n",
    "                    TN=0\n",
    "                    precision=0\n",
    "                    recall=0\n",
    "                    for i in range(n):\n",
    "                         if data[j][i]== 1 :                        \n",
    "                            if fi[i]==1 and r[i]==1:\n",
    "                                TP=TP+1\n",
    "                            if fi[i]==1 and r[i]==-1:\n",
    "                                FP=FP+1 \n",
    "                            if fi[i]==-1 and r[i]==1:\n",
    "                                FN=FN+1\n",
    "                            if fi[i]==-1 and r[i]==-1:\n",
    "                                TN=TN+1    \n",
    "                    if TP+FP !=0:\n",
    "                        precision=float(TP/(TP+FP))\n",
    "                    #print(\"precision\",precision)\n",
    "                    if TP+FN !=0:    \n",
    "                        recall=float(TP/(TP+FN))\n",
    "                    #print(\"recall\",recall)\n",
    "\n",
    "                    precision_all.append(precision)\n",
    "                    recall_all.append(recall)\n",
    "                    #print(\"TP,FP,TN,FN\")\n",
    "                    #print(TP,FP,TN,FN)\n",
    "\n",
    "                    a=0\n",
    "                    b=0\n",
    "                    acc1=0\n",
    "                    acc2=0\n",
    "                    for i in range(n):\n",
    "                            if data[j][i]== 1 :\n",
    "                                a=a+1\n",
    "                                if fi[i]==1:\n",
    "                                     acc1=acc1+1 \n",
    "\n",
    "        #                         print(\"total ,fair accepted, aceeptance rate:\")             \n",
    "                    a1=float(acc1/a)\n",
    "\n",
    "\n",
    "\n",
    "        #                         print(a)\n",
    "        #                         print(acc1)\n",
    "        #                         print(a1)\n",
    "                    ar_all.append(a1)\n",
    "\n",
    "                count = count+1\n",
    "                maxi=max(ar_all)\n",
    "                mini= min(ar_all)\n",
    "                DP=float(maxi-mini)\n",
    "                print(\"individual acceptance rates\")\n",
    "                print(ar_all)\n",
    "                print(\"individul precision\")\n",
    "                print(precision_all)\n",
    "                print(\"individual recall\")\n",
    "                print(recall_all)\n",
    "                print(\"DP all\")\n",
    "                print(DP)\n",
    "                f_acc=0\n",
    "                for i in range(n):\n",
    "                     if fi[i] == r[i]:\n",
    "                            f_acc=f_acc+1\n",
    "                f_acc_l=float((f_acc*100)/n) \n",
    "\n",
    "        #######################################################################33   \n",
    "\n",
    "        #                         print(\"sensitive attribute \",(j+1)) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                TP=0\n",
    "                FP=0\n",
    "                FN=0\n",
    "                TN=0\n",
    "                precision=0\n",
    "                recall=0\n",
    "                accu=0\n",
    "                for i in range(n):\n",
    "                        if fi[i]==1 and r[i]==1:\n",
    "                            TP=TP+1\n",
    "                        if fi[i]==1 and r[i]==-1:\n",
    "                            FP=FP+1 \n",
    "                        if fi[i]==-1 and r[i]==1:\n",
    "                            FN=FN+1\n",
    "                        if fi[i]==-1 and r[i]==-1:\n",
    "                            TN=TN+1    \n",
    "\n",
    "                if TP+FP!=0:\n",
    "                    precision=float(TP/(TP+FP))\n",
    "                print(\"precision all\",precision)\n",
    "                if TP+FN!=0:\n",
    "                    recall=float(TP/(TP+FN))\n",
    "\n",
    "\n",
    "                print(\"recall all\",recall)\n",
    "                accu=float((TP+TN)/(TP+FN+TN+FP))\n",
    "\n",
    "\n",
    "                print(\"accuracy all\",accu)\n",
    "\n",
    "\n",
    "\n",
    "                print(\"TP,FP,TN,FN\")\n",
    "                print(TP,FP,TN,FN)\n",
    "        #                         print(\"total ,fair accepted, aceeptance rate:\")             \n",
    "                a1=float(acc1/a)\n",
    "\n",
    "    alpha_weight=np.arange(0,1.05,.05)        \n",
    "    return accu_all,DP_all,acceptance_rate,alpha_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#NG\n",
    "import time\n",
    "import pulp as p \n",
    "def min_sum_lpca_g(data1,beta,eps,e,alpha):\n",
    "    import pulp as p \n",
    "    import math\n",
    "    m=data1.shape[0]\n",
    "    n=data1.shape[1]\n",
    "    print('dimension of data')\n",
    "    print(m,n)\n",
    "    \n",
    "    ################ sorted result\n",
    "       \n",
    "  \n",
    "    h1=[]\n",
    "    h2=[]\n",
    "    h3=[]\n",
    "    h4=[]\n",
    "    h5=[]\n",
    "    h6=[]\n",
    "    h7=[]\n",
    "    key1=[]\n",
    "    key2=[]\n",
    "    key3=[]\n",
    "    key4=[]\n",
    "    key5=[]\n",
    "    key6=[]\n",
    "    key7=[]\n",
    "    h8=[]\n",
    "    h9=[]\n",
    "    h10=[]\n",
    "    h11=[]\n",
    "    h12=[]\n",
    "    h13=[]\n",
    "    h14=[]\n",
    "    key8=[]\n",
    "    key9=[]\n",
    "    key10=[]\n",
    "    key11=[]\n",
    "    key12=[]\n",
    "    key13=[]\n",
    "    key14=[]\n",
    "    cost=np.zeros(n,dtype=int)\n",
    "    data2=np.zeros((m,n),dtype=int)\n",
    "    for i in range(n):\n",
    "        if data1[0][i]==1:            \n",
    "\n",
    "            h1.append(e[i][1])\n",
    "            key1.append(i)\n",
    "            \n",
    "\n",
    "        if data1[1][i]==1:\n",
    "            h2.append(e[i][1])\n",
    "            key2.append(i)\n",
    "            \n",
    "            \n",
    "        if data1[2][i]==1:\n",
    "            h3.append(e[i][1])\n",
    "            key3.append(i)\n",
    "            \n",
    "        if data1[3][i]==1:\n",
    "            h4.append(e[i][1])\n",
    "            key4.append(i)\n",
    "        if data1[4][i]==1:\n",
    "            h5.append(e[i][1])\n",
    "            key5.append(i)\n",
    "        if data1[5][i]==1:\n",
    "            h6.append(e[i][1])\n",
    "            key6.append(i)\n",
    "        if data1[6][i]==1:\n",
    "            h7.append(e[i][1])\n",
    "            key7.append(i)\n",
    "        if data1[7][i]==1:            \n",
    "            h8.append(e[i][1])\n",
    "            key8.append(i)\n",
    "#print(hc)\n",
    "#     print(key1)\n",
    "    \n",
    "    for i in range(1,len(h1)):\n",
    "        for j in range(i,0,-1):\n",
    "            var=0\n",
    "            var2=0\n",
    "            if h1[j-1]<h1[j]:\n",
    "                index=j\n",
    "                var=h1[j]\n",
    "                h1[j]=h1[j-1]\n",
    "                h1[j-1]=var\n",
    "\n",
    "                var2=key1[j]\n",
    "                key1[j]=key1[j-1]\n",
    "                key1[j-1]=var2\n",
    "            else:\n",
    "                break\n",
    "    \n",
    "\n",
    "    for i in range(1,len(h2)):\n",
    "        for j in range(i,0,-1):\n",
    "            var=0\n",
    "            var2=0\n",
    "            if h2[j-1]<h2[j]:\n",
    "                index=j\n",
    "                var=h2[j]\n",
    "                h2[j]=h2[j-1]\n",
    "                h2[j-1]=var\n",
    "\n",
    "                var2=key2[j]\n",
    "                key2[j]=key2[j-1]\n",
    "                key2[j-1]=var2\n",
    "            else:\n",
    "                break\n",
    "    for i in range(1,len(h3)):\n",
    "        for j in range(i,0,-1):\n",
    "            var=0\n",
    "            var2=0\n",
    "            if h3[j-1]<h3[j]:\n",
    "                index=j\n",
    "                var=h3[j]\n",
    "                h3[j]=h3[j-1]\n",
    "                h3[j-1]=var\n",
    "\n",
    "                var2=key3[j]\n",
    "                key3[j]=key3[j-1]\n",
    "                key3[j-1]=var2\n",
    "            else:\n",
    "                break\n",
    "    for i in range(1,len(h4)):\n",
    "        for j in range(i,0,-1):\n",
    "            var=0\n",
    "            var2=0\n",
    "            if h4[j-1]<h4[j]:\n",
    "                index=j\n",
    "                var=h4[j]\n",
    "                h4[j]=h4[j-1]\n",
    "                h4[j-1]=var\n",
    "\n",
    "                var2=key4[j]\n",
    "                key4[j]=key4[j-1]\n",
    "                key4[j-1]=var2\n",
    "            else:\n",
    "                break\n",
    "    for i in range(1,len(h5)):\n",
    "        for j in range(i,0,-1):\n",
    "            var=0\n",
    "            var2=0\n",
    "            if h5[j-1]<h5[j]:\n",
    "                index=j\n",
    "                var=h5[j]\n",
    "                h5[j]=h5[j-1]\n",
    "                h5[j-1]=var\n",
    "\n",
    "                var2=key5[j]\n",
    "                key5[j]=key5[j-1]\n",
    "                key5[j-1]=var2\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "                \n",
    "                \n",
    "    for i in range(1,len(h6)):\n",
    "        for j in range(i,0,-1):\n",
    "            var=0\n",
    "            var2=0\n",
    "            if h6[j-1]<h6[j]:\n",
    "                index=j\n",
    "                var=h6[j]\n",
    "                h6[j]=h6[j-1]\n",
    "                h6[j-1]=var\n",
    "\n",
    "                var2=key6[j]\n",
    "                key6[j]=key6[j-1]\n",
    "                key6[j-1]=var2\n",
    "            else:\n",
    "                break        \n",
    "                \n",
    "\n",
    "    for i in range(1,len(h7)):\n",
    "        for j in range(i,0,-1):\n",
    "            var=0\n",
    "            var2=0\n",
    "            if h7[j-1]<h7[j]:\n",
    "                index=j\n",
    "                var=h7[j]\n",
    "                h7[j]=h7[j-1]\n",
    "                h7[j-1]=var\n",
    "\n",
    "                var2=key7[j]\n",
    "                key7[j]=key7[j-1]\n",
    "                key7[j-1]=var2\n",
    "            else:\n",
    "                break \n",
    "    ############################################            \n",
    "    for i in range(1,len(h8)):\n",
    "        for j in range(i,0,-1):\n",
    "            var=0\n",
    "            var2=0\n",
    "            if h8[j-1]<h8[j]:\n",
    "                index=j\n",
    "                var=h8[j]\n",
    "                h8[j]=h8[j-1]\n",
    "                h8[j-1]=var\n",
    "\n",
    "                var2=key8[j]\n",
    "                key8[j]=key8[j-1]\n",
    "                key8[j-1]=var2\n",
    "            else:\n",
    "                break\n",
    "          \n",
    "                \n",
    "\n",
    "    \n",
    "    '''            \n",
    "    \n",
    "    for j in range(len(key1)):    \n",
    "        if h1[j]==h1[j-1] and j>=1:\n",
    "            data2[0][key1[j]]=data2[0][key1[j-1]]\n",
    "        else:    \n",
    "            data2[0][key1[j]]=j+1\n",
    "    for j in range(len(key2)):\n",
    "        if h2[j]==h2[j-1] and j>=1:\n",
    "            data2[1][key2[j]]=data2[0][key2[j-1]]\n",
    "        else:    \n",
    "            data2[1][key2[j]]=j+1\n",
    "    for j in range(len(key3)):\n",
    "        if h3[j]==h3[j-1] and j>=1:\n",
    "            data2[2][key3[j]]=data2[2][key3[j-1]]\n",
    "        else:    \n",
    "            data2[2][key3[j]]=j+1\n",
    "    for j in range(len(key4)):\n",
    "        if h4[j]==h4[j-1] and j>=1:\n",
    "            data2[3][key4[j]]=data2[3][key4[j-1]]\n",
    "        else:    \n",
    "            data2[3][key4[j]]=j+1\n",
    "    for j in range(len(key5)):\n",
    "        if h5[j]==h5[j-1] and j>=1:\n",
    "            data2[4][key5[j]]=data2[4][key5[j-1]]\n",
    "        else:    \n",
    "            data2[4][key5[j]]=j+1\n",
    "    for j in range(len(key6)):\n",
    "        if h6[j]==h6[j-1] and j>=1:\n",
    "            data2[5][key6[j]]=data2[5][key6[j-1]]\n",
    "        else:    \n",
    "            data2[5][key6[j]]=j+1\n",
    "    for j in range(len(key7)):\n",
    "        if h7[j]==h7[j-1] and j>=1:\n",
    "            data2[6][key7[j]]=data2[6][key7[j-1]]\n",
    "        else:    \n",
    "            data2[6][key7[j]]=j+1 \n",
    "    \n",
    "  ###############################1#################################  \n",
    "    #2nd approach\n",
    "    for j in range(len(key1)):    \n",
    "        #data2[0][key1[j]]=((j+1)/((beta[0]*len(key1))*((beta[0]*len(key1))+1)/2)*alpha[0])\n",
    "         \n",
    "        data2[0][key1[j]]=(j+1)*alpha[0]\n",
    "    for j in range(len(key2)):\n",
    "        data2[1][key2[j]]=(j+1)*alpha[1]\n",
    "    for j in range(len(key3)):\n",
    "        if data1[2][key3[j]]==1 and data1[0][key3[j]]==1: \n",
    "            data2[2][key3[j]]=(j+1)*(len(key1)/len(key3))*alpha[2]\n",
    "        else:\n",
    "            data2[2][key3[j]]=(j+1)*(len(key2)/len(key3))*alpha[2]                  \n",
    "        \n",
    "    for j in range(len(key4)):\n",
    "        if data1[3][key4[j]]==1 and data1[0][key4[j]]==1:                   \n",
    "            data2[3][key4[j]]=(j+1)*(len(key1)/len(key4))*alpha[3]\n",
    "        else :                     \n",
    "            data2[3][key4[j]]=(j+1)*(len(key2)/len(key4))*alpha[3]\n",
    "                             \n",
    "    for j in range(len(key5)):\n",
    "        if data1[4][key5[j]]==1 and data1[0][key5[j]]==1:                  \n",
    "            data2[4][key5[j]]=(j+1)*(len(key1)/len(key5))*alpha[4]\n",
    "        else:      \n",
    "            data2[4][key5[j]]=(j+1)*(len(key2)/len(key5))*alpha[4]\n",
    "    for j in range(len(key6)):\n",
    "        if data1[5][key6[j]]==1 and data1[0][key6[j]]==1:                    \n",
    "            data2[5][key6[j]]=(j+1)*(len(key1)/len(key6))*alpha[5]\n",
    "        else:                    \n",
    "             data2[5][key6[j]]=(j+1)*(len(key2)/len(key6))*alpha[5]               \n",
    "    for j in range(len(key7)):\n",
    "        if data1[6][key7[j]]==1 and data1[0][key7[j]]==1:\n",
    "            data2[6][key7[j]]=(j+1)*(len(key1)/len(key7))*alpha[6]\n",
    "        else:\n",
    "             data2[6][key7[j]]=(j+1)*(len(key2)/len(key7))*alpha[6]\n",
    "    '''\n",
    "    '''\n",
    "    \n",
    "    #1st approach\n",
    "    for j in range(len(key1)):    \n",
    "        #data2[0][key1[j]]=((j+1)/((beta[0]*len(key1))*((beta[0]*len(key1))+1)/2)*alpha[0])\n",
    "         \n",
    "        data2[0][key1[j]]=(j+1)*alpha[0]\n",
    "    for j in range(len(key2)):\n",
    "        data2[1][key2[j]]=(j+1)*alpha[1]\n",
    "    for j in range(len(key3)):\n",
    "        data2[2][key3[j]]=(j+1)*alpha[2]              \n",
    "        \n",
    "    for j in range(len(key4)):\n",
    "        data2[3][key4[j]]=(j+1)*alpha[3]\n",
    "        \n",
    "                             \n",
    "    for j in range(len(key5)):               \n",
    "        data2[4][key5[j]]=(j+1)*alpha[4]\n",
    "       \n",
    "    for j in range(len(key6)):\n",
    "        data2[5][key6[j]]=(j+1)*alpha[5]\n",
    "                    \n",
    "    for j in range(len(key7)):\n",
    "        data2[6][key7[j]]=(j+1)*alpha[6]\n",
    "    '''   \n",
    "    '''\n",
    "    for j in range(len(key1)):    \n",
    "        #data2[0][key1[j]]=((j+1)/((beta[0]*len(key1))*((beta[0]*len(key1))+1)/2)*alpha[0])\n",
    "         \n",
    "        data2[0][key1[j]]=(j+1)*alpha[0]\n",
    "    for j in range(len(key2)):\n",
    "        data2[1][key2[j]]=(j+1)*((beta[0]*len(key1))/(beta[1]*len(key2)))*alpha[1]\n",
    "    for j in range(len(key3)):\n",
    "        data2[2][key3[j]]=(j+1)*((beta[2]*len(key3))/(beta[2]*len(key3)))*alpha[2]\n",
    "                         \n",
    "        \n",
    "    for j in range(len(key4)):           \n",
    "        #data2[3][key4[j]]=(j+1)*((beta[2]*len(key3))/(beta[3]*len(key4)))*alpha[3]\n",
    "        if data1[3][key4[j]]==1 and data1[0][key4[j]]==1:                   \n",
    "            data2[3][key4[j]]=(j+1)*((beta[2]*len(key3))/(beta[3]*len(key4)))*alpha[3]\n",
    "        else :                     \n",
    "            data2[3][key4[j]]=(j+1)*((beta[2]*len(key3))/(beta[3]*len(key4)))*alpha[3]\n",
    "                             \n",
    "    for j in range(len(key5)):\n",
    "        data2[4][key5[j]]=(j+1)*((beta[2]*len(key3))/(beta[4]*len(key5)))*alpha[4]      \n",
    "    for j in range(len(key6)):                 \n",
    "        data2[5][key6[j]]=(j+1)*((beta[2]*len(key3))/(beta[5]*len(key6)))*alpha[5]  \n",
    "    for j in range(len(key7)):                 \n",
    "        data2[6][key7[j]]=(j+1)*((beta[2]*len(key3))/(beta[6]*len(key7)))*alpha[6] \n",
    "    '''\n",
    "    #######################################################################    \n",
    "    \n",
    "    ####################################################################### \n",
    "   \n",
    "    for j in range(len(key1)):    \n",
    "        data2[0][key1[j]]=(j+1)*((beta[0]*len(key1))/(beta[6]*len(key7)))\n",
    "    for j in range(len(key2)):\n",
    "        data2[1][key2[j]]=(j+1)*((beta[1]*len(key2))/(beta[6]*len(key7)))\n",
    "    \n",
    "    for j in range(len(key3)):\n",
    "        data2[2][key3[j]]=(j+1)*((beta[2]*len(key3))/(beta[6]*len(key7)))\n",
    "                         \n",
    "        \n",
    "    for j in range(len(key4)):           \n",
    "        data2[3][key4[j]]=(j+1)*((beta[3]*len(key4))/(beta[6]*len(key7)))\n",
    "                             \n",
    "    for j in range(len(key5)):\n",
    "        data2[4][key5[j]]=(j+1)*((beta[4]*len(key5))/(beta[6]*len(key7)))\n",
    "       \n",
    "           \n",
    "    for j in range(len(key6)):                 \n",
    "        data2[5][key6[j]]=(j+1)*((beta[5]*len(key6))/(beta[6]*len(key7)))\n",
    "    \n",
    "    for j in range(len(key7)):                 \n",
    "        data2[6][key7[j]]=(j+1)  *((beta[6]*len(key7))/(beta[6]*len(key7)))\n",
    "       \n",
    "    for j in range(len(key8)):    \n",
    "        data2[7][key8[j]]=(j+1)*((beta[7]*len(key8))/(beta[6]*len(key7)))\n",
    "    \n",
    "        \n",
    "    '''    \n",
    "    for j in range(len(key1)):    \n",
    "        #data2[0][key1[j]]=((j+1)/((beta[0]*len(key1))*((beta[0]*len(key1))+1)/2)*alpha[0])\n",
    "         \n",
    "        data2[0][key1[j]]=(j+1)*((beta[1]*len(key2))/(beta[0]*len(key1)))*alpha[0]\n",
    "    for j in range(len(key2)):\n",
    "        data2[1][key2[j]]=(j+1)*alpha[1]\n",
    "    for j in range(len(key3)):\n",
    "        data2[2][key3[j]]=(j+1)*alpha[2]\n",
    "                         \n",
    "        \n",
    "    for j in range(len(key4)):           \n",
    "        #data2[3][key4[j]]=(j+1)*((beta[2]*len(key3))/(beta[3]*len(key4)))*alpha[3]\n",
    "        if data1[3][key4[j]]==1 and data1[0][key4[j]]==1:                   \n",
    "            data2[3][key4[j]]=(j+1)*((beta[2]*len(key3))/(beta[3]*len(key4)))*1\n",
    "        else :                     \n",
    "            data2[3][key4[j]]=(j+1)*((beta[2]*len(key3))/(beta[3]*len(key4)))*alpha[3]\n",
    "   \n",
    "       \n",
    "    '''\n",
    "    for j in range(n):\n",
    "        sum=0\n",
    "        for i in range(m):\n",
    "       \n",
    "            sum=sum+data2[i][j] \n",
    "        cost[j]=sum\n",
    "        \n",
    "        \n",
    "    ################\n",
    "    Lp_prob = p.LpProblem('Problem', p.LpMinimize)  \n",
    "   \n",
    "    \n",
    "#     X=np.zeros(n+1,dtype=p.LpVariable)\n",
    "    X=np.zeros(n+m+1,dtype=p.LpVariable)\n",
    "    sizes=np.zeros(m,dtype=int)\n",
    "#     report_index(index,data1,e):  \n",
    "    max_size=0\n",
    "    for i in range(m):\n",
    "        count=0\n",
    "        for j in range(n):\n",
    "            if data1[i][j]==1:\n",
    "                count=count+1 \n",
    "        if count>max_size:\n",
    "            max_size=count\n",
    "        sizes[i]=count\n",
    "        \n",
    "    #############################33\n",
    "    ''' \n",
    "        #basic1\n",
    "    for j in range(len(key1)):    \n",
    "        #data2[0][key1[j]]=((j+1)/((beta[0]*len(key1))*((beta[0]*len(key1))+1)/2)*alpha[0])\n",
    "         \n",
    "        data2[0][key1[j]]=(j+1)*alpha[0]\n",
    "    for j in range(len(key2)):\n",
    "        data2[1][key2[j]]=(j+1)*alpha[1]\n",
    "    for j in range(len(key3)):\n",
    "        data2[2][key3[j]]=(j+1)*alpha[2]              \n",
    "        \n",
    "    for j in range(len(key4)):\n",
    "        data2[3][key4[j]]=(j+1)*alpha[3]\n",
    "        \n",
    "        \n",
    "    #basic2\n",
    "            \n",
    "    '''\n",
    "    \n",
    "            \n",
    "    \n",
    "    \n",
    "    \n",
    "    ###############################\n",
    "        \n",
    "        \n",
    "        \n",
    "  \n",
    "    select_sizes=np.zeros(m,dtype=int)\n",
    "   \n",
    "    size_final=np.zeros(m,dtype=int)\n",
    "\n",
    "    \n",
    "    for i in range(n):\n",
    "        var1=str(i)       \n",
    "        X[i]=p.LpVariable(var1,lowBound=0,upBound=1,cat='Integer')\n",
    "   \n",
    "    X[n]=p.LpVariable(str(n),lowBound=0,upBound=1,cat='Continuous')  \n",
    "   \n",
    "    #########objective function#####################\n",
    "    \n",
    "#     Lp_prob += 2*X[n+1]+10*X[n+2]+9*X[n+3]+3*X[n+4]\n",
    "    Lp_prob+= p.lpSum([(X[j])*cost[j] for j in range(n)]) \n",
    "  \n",
    "    \n",
    "\n",
    "    ##############constraint#################\n",
    "    for i in range(2*m):\n",
    "        if i<m:\n",
    "\n",
    "            Lp_prob += p.lpSum([(X[j])*(data1[i][j]) for j in range(n)]) >= math.floor(beta[i]*sizes[i])\n",
    "            Lp_prob += p.lpSum([(X[j])*(data1[i][j]) for j in range(n)]) <= math.ceil((beta[i]+eps)*sizes[i])\n",
    "          \n",
    "        \n",
    "    #####################################\n",
    "    status = Lp_prob.solve()   # Solver \n",
    "    print(p.LpStatus[status]) \n",
    "    print(\"objective is:\")        \n",
    "    print(p.value(Lp_prob.objective))\n",
    "    print(\"discripency is:\") \n",
    "    print(p.value(X[n]))\n",
    "    x=np.zeros(n,dtype=float)\n",
    "\n",
    "   # The solution status \n",
    "    Synth1={}\n",
    "    Synth2={}\n",
    "    # # Printing the final solution \n",
    "    for i in range(n):\n",
    "        if(p.value(X[i])==1):\n",
    "            Synth1[i]=1 \n",
    "            Synth2[i]=-1\n",
    "#             if(data1[2][i]==1):\n",
    "#                 print(\"no\")\n",
    "        else:\n",
    "            Synth1[i]=-1\n",
    "            Synth2[i]=1\n",
    "    Synthu1=Synth1  \n",
    "    Synthu2=Synth2  \n",
    "    \n",
    "              \n",
    "    return Synthu1,Synthu2   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    3175\n",
      "0    2103\n",
      "Name: African_American, dtype: int64\n",
      "0    4247\n",
      "1    1031\n",
      "Name: Female, dtype: int64\n",
      "   Number_of_Priors  score_factor  Age_Above_FourtyFive  Age_Below_TwentyFive  \\\n",
      "0                 0             0                     0                     0   \n",
      "1                 4             0                     0                     1   \n",
      "2                14             1                     0                     0   \n",
      "3                 0             0                     0                     0   \n",
      "4                 0             0                     0                     0   \n",
      "\n",
      "   African_American  Female  Misdemeanor  \n",
      "0                 1       0            0  \n",
      "1                 1       0            0  \n",
      "2                 0       0            0  \n",
      "3                 0       1            1  \n",
      "4                 0       0            0  \n",
      "There are 3694 samples in the training set and 1584 samples in the test set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/subham/anaconda3/envs/tensorflow/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the SVM classifier on training data is 0.70\n",
      "The accuracy of the SVM classifier on test data is 0.68\n",
      "####Train prediction Label###############################################\n",
      "####Actual Train Label###############################################\n",
      "####Change to colors###############################################\n",
      "[[0.27769831 0.72230169]\n",
      " [0.71624585 0.28375415]\n",
      " [0.70399223 0.29600777]\n",
      " ...\n",
      " [0.27880346 0.72119654]\n",
      " [0.47620161 0.52379839]\n",
      " [0.42266393 0.57733607]]\n",
      "[1 0 0 ... 1 1 1]\n",
      "      Number_of_Priors  score_factor  Age_Above_FourtyFive  \\\n",
      "0                    0             1                     0   \n",
      "1                    2             0                     1   \n",
      "2                    5             0                     0   \n",
      "3                    2             1                     0   \n",
      "4                    1             0                     0   \n",
      "...                ...           ...                   ...   \n",
      "1579                25             1                     0   \n",
      "1580                 0             1                     0   \n",
      "1581                20             1                     1   \n",
      "1582                 2             1                     0   \n",
      "1583                 3             1                     0   \n",
      "\n",
      "      Age_Below_TwentyFive  African_American  Female  Misdemeanor  \n",
      "0                        1                 1       0            1  \n",
      "1                        0                 1       0            1  \n",
      "2                        0                 0       0            0  \n",
      "3                        1                 1       0            0  \n",
      "4                        0                 1       0            0  \n",
      "...                    ...               ...     ...          ...  \n",
      "1579                     0                 1       0            1  \n",
      "1580                     1                 1       1            0  \n",
      "1581                     0                 1       0            0  \n",
      "1582                     0                 1       0            0  \n",
      "1583                     0                 1       1            1  \n",
      "\n",
      "[1584 rows x 7 columns]\n",
      "[1 0 0 ... 1 1 1]\n",
      "      Two_yr_Recidivism\n",
      "0                     1\n",
      "1                     0\n",
      "2                     1\n",
      "3                     1\n",
      "4                     1\n",
      "...                 ...\n",
      "1579                  1\n",
      "1580                  0\n",
      "1581                  0\n",
      "1582                  0\n",
      "1583                  0\n",
      "\n",
      "[1584 rows x 1 columns]\n",
      "      African_American  Female\n",
      "0                    1       0\n",
      "1                    1       0\n",
      "2                    0       0\n",
      "3                    1       0\n",
      "4                    1       0\n",
      "...                ...     ...\n",
      "1579                 1       0\n",
      "1580                 1       1\n",
      "1581                 1       0\n",
      "1582                 1       0\n",
      "1583                 1       1\n",
      "\n",
      "[1584 rows x 2 columns]\n",
      "                    0     1     2     3     4     5     6     7     8     \\\n",
      "african_american_0     0     0     1     0     0     1     0     1     0   \n",
      "african_american_1     1     1     0     1     1     0     1     0     1   \n",
      "female_0               1     1     1     1     1     0     1     0     1   \n",
      "female_1               0     0     0     0     0     1     0     1     0   \n",
      "\n",
      "                    9     ...  1574  1575  1576  1577  1578  1579  1580  1581  \\\n",
      "african_american_0     1  ...     1     0     1     1     0     0     0     0   \n",
      "african_american_1     0  ...     0     1     0     0     1     1     1     1   \n",
      "female_0               1  ...     0     1     0     1     0     1     0     1   \n",
      "female_1               0  ...     1     0     1     0     1     0     1     0   \n",
      "\n",
      "                    1582  1583  \n",
      "african_american_0     0     0  \n",
      "african_american_1     1     1  \n",
      "female_0               1     0  \n",
      "female_1               0     1  \n",
      "\n",
      "[4 rows x 1584 columns]\n"
     ]
    }
   ],
   "source": [
    "# without accuracy\n",
    "import time\n",
    "# import pulp as p \n",
    "# from random import *\n",
    "data= pd.read_csv('data/propublica/compass.csv', skipinitialspace=True)\n",
    "# data = data1[data1[\"race\"].isin([\"african-american\", \"caucasian\"])]\n",
    "\n",
    "print(data['African_American'].value_counts())\n",
    "print(data['Female'].value_counts())\n",
    "# print(data.shape[0],data.shape[1])\n",
    "data=data.drop(columns=['id'])\n",
    "# print(data.head())\n",
    "# Age_Above_FourtyFive,Age_Below_TwentyFive, African_American,Female,  Two_yr_Recidivism  \n",
    "\n",
    "\n",
    "\n",
    "data_c = data.drop(columns=[ 'Two_yr_Recidivism' ])\n",
    "# print(sens)\n",
    "print(data_c.head())\n",
    "r=data[['Two_yr_Recidivism']]\n",
    "\n",
    "X_test,Y_test_pred,Y_test,e = Propublica_svm(data_c , r)\n",
    "\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "# Y_test_pred.reset_index()\n",
    "\n",
    "Y_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(X_test)\n",
    "print(Y_test_pred)\n",
    "print(Y_test)\n",
    "sens=X_test[['African_American','Female']]\n",
    "print(sens)\n",
    "p=sens.shape[0]\n",
    "\n",
    "\n",
    "sens1 = pd.get_dummies(sens, columns=['African_American','Female'], prefix =['african_american','female'])\n",
    "sensitive = sens1.T\n",
    "print(sensitive)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Yang gerry results\n",
    "\n",
    "sensitive2=sens1.values\n",
    "\n",
    "sensitive1 = np.zeros((sensitive2.shape[0],4),dtype=float)\n",
    "#sensitive2 = np.zeros((sens.shape[0],7),dtype=int)\n",
    "sensitive3 = np.zeros((sensitive2.shape[0],8),dtype=float)\n",
    "\n",
    "   #print(sensitive1.shape[0])\n",
    "   #print(sensitive1.shape[1])\n",
    "\n",
    "'''\n",
    "for k in range(sens.shape[0]):\n",
    "    for i in range(7):\n",
    "        sensitive2[k][i] = sens.iloc[k,i]\n",
    "'''\n",
    "for k in range(sensitive2.shape[0]):\n",
    "    count = 0\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            if sensitive2[k][i]==1 and sensitive2[k][2+j]==1:\n",
    "                sensitive1[k][count] = 1\n",
    "                count = count+1\n",
    "            else:\n",
    "                sensitive1[k][count] = 0\n",
    "                count = count+1\n",
    "                \n",
    "selected = []          \n",
    "counts = []\n",
    "sensitive3 = np.concatenate((sensitive1,sensitive2), axis = 1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.] [1101  483]\n",
      "[0. 1.] [1443  141]\n",
      "[0. 1.] [807 777]\n",
      "[0. 1.] [1401  183]\n",
      "[0. 1.] [960 624]\n",
      "[0. 1.] [624 960]\n",
      "[0. 1.] [ 324 1260]\n",
      "[0. 1.] [1260  324]\n",
      "[0 1] [1101  483]\n",
      "[0 1] [1443  141]\n",
      "[0 1] [807 777]\n",
      "[0 1] [1401  183]\n",
      "[0 1] [960 624]\n",
      "[0 1] [624 960]\n",
      "[0 1] [ 324 1260]\n",
      "[0 1] [1260  324]\n"
     ]
    }
   ],
   "source": [
    "for i in range(8):\n",
    "    a,b=np.unique(sensitive3[:,i],return_counts=True)\n",
    "    print(a,b)\n",
    "    \n",
    "\n",
    "sensitive1=np.transpose(sensitive3)\n",
    "\n",
    "sensitiven=np.zeros((8,sensitive1.shape[1]),dtype=int)\n",
    "\n",
    "\n",
    "\n",
    "k=[0, 1,2,3,4, 5, 6, 7]\n",
    "for i in range(8):\n",
    "    sensitiven[i,:]=sensitive1[k[i],:]    \n",
    "    \n",
    "for i in range(8):\n",
    "    a,b=np.unique(sensitiven[i,:],return_counts=True)\n",
    "    print(a,b)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensitive attribute  1\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "483\n",
      "186\n",
      "0.38509316770186336\n",
      "sensitive attribute  2\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "141\n",
      "52\n",
      "0.36879432624113473\n",
      "sensitive attribute  3\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "777\n",
      "413\n",
      "0.5315315315315315\n",
      "sensitive attribute  4\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "183\n",
      "71\n",
      "0.3879781420765027\n",
      "sensitive attribute  5\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "624\n",
      "238\n",
      "0.3814102564102564\n",
      "sensitive attribute  6\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "960\n",
      "484\n",
      "0.5041666666666667\n",
      "sensitive attribute  7\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1260\n",
      "599\n",
      "0.4753968253968254\n",
      "sensitive attribute  8\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "324\n",
      "123\n",
      "0.37962962962962965\n",
      "data acceptance rates\n",
      "[0.38509316770186336, 0.36879432624113473, 0.5315315315315315, 0.3879781420765027, 0.3814102564102564, 0.5041666666666667, 0.4753968253968254, 0.37962962962962965]\n",
      "data DP\n",
      "0.1627372052903968\n",
      "sensitive attribute  1\n",
      "prec reca accuracy for each sens\n",
      "0.6131386861313869 0.45161290322580644 0.6790890269151139\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "483\n",
      "137\n",
      "0.2836438923395445\n",
      "sensitive attribute  2\n",
      "prec reca accuracy for each sens\n",
      "0.75 0.34615384615384615 0.7163120567375887\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "141\n",
      "24\n",
      "0.1702127659574468\n",
      "sensitive attribute  3\n",
      "prec reca accuracy for each sens\n",
      "0.6680942184154176 0.7554479418886199 0.6705276705276705\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "777\n",
      "467\n",
      "0.601029601029601\n",
      "sensitive attribute  4\n",
      "prec reca accuracy for each sens\n",
      "0.6538461538461539 0.4788732394366197 0.6994535519125683\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "183\n",
      "52\n",
      "0.28415300546448086\n",
      "sensitive attribute  5\n",
      "prec reca accuracy for each sens\n",
      "0.6335403726708074 0.42857142857142855 0.6875\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "624\n",
      "161\n",
      "0.25801282051282054\n",
      "sensitive attribute  6\n",
      "prec reca accuracy for each sens\n",
      "0.6666666666666666 0.7148760330578512 0.6760416666666667\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "960\n",
      "519\n",
      "0.540625\n",
      "sensitive attribute  7\n",
      "prec reca accuracy for each sens\n",
      "0.6556291390728477 0.66110183639399 0.6738095238095239\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1260\n",
      "604\n",
      "0.4793650793650794\n",
      "sensitive attribute  8\n",
      "prec reca accuracy for each sens\n",
      "0.6842105263157895 0.42276422764227645 0.7067901234567902\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "324\n",
      "76\n",
      "0.2345679012345679\n",
      "data acceptance rates\n",
      "[0.2836438923395445, 0.1702127659574468, 0.601029601029601, 0.28415300546448086, 0.25801282051282054, 0.540625, 0.4793650793650794, 0.2345679012345679]\n",
      "data DP\n",
      "0.4308168350721542\n",
      "SVM accuracy--------------------------\n",
      "0.6588235294117647 0.6204986149584487 0.6805555555555556\n",
      "dimension of data\n",
      "8 1584\n",
      "Optimal\n",
      "objective is:\n",
      "333256.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.4451345755693582, 0.41134751773049644, 0.462033462033462, 0.46994535519125685, 0.4375, 0.4635416666666667, 0.45555555555555555, 0.4444444444444444] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 0\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "sensitive attribute  7\n",
      "sensitive attribute  8\n",
      "individual acceptance rates\n",
      "[0.4451345755693582, 0.41134751773049644, 0.462033462033462, 0.46994535519125685, 0.4375, 0.4635416666666667, 0.45555555555555555, 0.4444444444444444]\n",
      "individul precision\n",
      "[0.5348837209302325, 0.46551724137931033, 0.7019498607242339, 0.5581395348837209, 0.5201465201465202, 0.6741573033707865, 0.6393728222996515, 0.5208333333333334]\n",
      "individual recall\n",
      "[0.6182795698924731, 0.5192307692307693, 0.6101694915254238, 0.676056338028169, 0.5966386554621849, 0.6198347107438017, 0.6126878130217028, 0.6097560975609756]\n",
      "DP all\n",
      "0.058597837460760416\n",
      "precision all 0.6155988857938719\n",
      "recall all 0.6121883656509696\n",
      "accuracy all 0.648989898989899\n",
      "TP,FP,TN,FN\n",
      "442 276 586 280\n",
      "dimension of data\n",
      "8 1584\n",
      "Optimal\n",
      "objective is:\n",
      "344341.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.4451345755693582, 0.41134751773049644, 0.47619047619047616, 0.46994535519125685, 0.4375, 0.475, 0.4642857142857143, 0.4444444444444444] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 0\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "sensitive attribute  7\n",
      "sensitive attribute  8\n",
      "individual acceptance rates\n",
      "[0.4451345755693582, 0.41134751773049644, 0.47619047619047616, 0.46994535519125685, 0.4375, 0.475, 0.4642857142857143, 0.4444444444444444]\n",
      "individul precision\n",
      "[0.5348837209302325, 0.46551724137931033, 0.6972972972972973, 0.5581395348837209, 0.5201465201465202, 0.6710526315789473, 0.6376068376068376, 0.5208333333333334]\n",
      "individual recall\n",
      "[0.6182795698924731, 0.5192307692307693, 0.6246973365617433, 0.676056338028169, 0.5966386554621849, 0.6322314049586777, 0.6227045075125208, 0.6097560975609756]\n",
      "DP all\n",
      "0.06484295845997973\n",
      "precision all 0.6145404663923183\n",
      "recall all 0.6204986149584487\n",
      "accuracy all 0.6496212121212122\n",
      "TP,FP,TN,FN\n",
      "448 281 581 274\n",
      "dimension of data\n",
      "8 1584\n",
      "Optimal\n",
      "objective is:\n",
      "339812.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.4306418219461698, 0.41134751773049644, 0.4774774774774775, 0.46994535519125685, 0.42628205128205127, 0.47604166666666664, 0.4595238095238095, 0.4444444444444444] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 0\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "sensitive attribute  7\n",
      "sensitive attribute  8\n",
      "individual acceptance rates\n",
      "[0.4306418219461698, 0.41134751773049644, 0.4774774774774775, 0.46994535519125685, 0.42628205128205127, 0.47604166666666664, 0.4595238095238095, 0.4444444444444444]\n",
      "individul precision\n",
      "[0.5432692307692307, 0.46551724137931033, 0.6981132075471698, 0.5581395348837209, 0.5263157894736842, 0.6717724288840262, 0.6424870466321243, 0.5208333333333334]\n",
      "individual recall\n",
      "[0.6075268817204301, 0.5192307692307693, 0.6271186440677966, 0.676056338028169, 0.5882352941176471, 0.6342975206611571, 0.6210350584307178, 0.6097560975609756]\n",
      "DP all\n",
      "0.06612995974698105\n",
      "precision all 0.6182572614107884\n",
      "recall all 0.6191135734072022\n",
      "accuracy all 0.6521464646464646\n",
      "TP,FP,TN,FN\n",
      "447 276 586 275\n",
      "dimension of data\n",
      "8 1584\n",
      "Optimal\n",
      "objective is:\n",
      "347657.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.4451345755693582, 0.41134751773049644, 0.47619047619047616, 0.4918032786885246, 0.4375, 0.4791666666666667, 0.4642857142857143, 0.4567901234567901] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 0\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "sensitive attribute  7\n",
      "sensitive attribute  8\n",
      "individual acceptance rates\n",
      "[0.4451345755693582, 0.41134751773049644, 0.47619047619047616, 0.4918032786885246, 0.4375, 0.4791666666666667, 0.4642857142857143, 0.4567901234567901]\n",
      "individul precision\n",
      "[0.5348837209302325, 0.46551724137931033, 0.6972972972972973, 0.5444444444444444, 0.5201465201465202, 0.6673913043478261, 0.6376068376068376, 0.5135135135135135]\n",
      "individual recall\n",
      "[0.6182795698924731, 0.5192307692307693, 0.6246973365617433, 0.6901408450704225, 0.5966386554621849, 0.6342975206611571, 0.6227045075125208, 0.6178861788617886]\n",
      "DP all\n",
      "0.08045576095802814\n",
      "precision all 0.6125511596180082\n",
      "recall all 0.6218836565096952\n",
      "accuracy all 0.6483585858585859\n",
      "TP,FP,TN,FN\n",
      "449 284 578 273\n"
     ]
    }
   ],
   "source": [
    "#gerry Yang\n",
    "accu,DP_all,acceptance_rate,alpha_weight = main(sensitiven, Y_test, Y_test_pred,e )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensitive attribute  1\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "483\n",
      "186\n",
      "0.38509316770186336\n",
      "sensitive attribute  2\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "141\n",
      "52\n",
      "0.36879432624113473\n",
      "sensitive attribute  3\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "777\n",
      "413\n",
      "0.5315315315315315\n",
      "sensitive attribute  4\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "183\n",
      "71\n",
      "0.3879781420765027\n",
      "sensitive attribute  5\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "624\n",
      "238\n",
      "0.3814102564102564\n",
      "sensitive attribute  6\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "960\n",
      "484\n",
      "0.5041666666666667\n",
      "sensitive attribute  7\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1260\n",
      "599\n",
      "0.4753968253968254\n",
      "sensitive attribute  8\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "324\n",
      "123\n",
      "0.37962962962962965\n",
      "data acceptance rates\n",
      "[0.38509316770186336, 0.36879432624113473, 0.5315315315315315, 0.3879781420765027, 0.3814102564102564, 0.5041666666666667, 0.4753968253968254, 0.37962962962962965]\n",
      "data DP\n",
      "0.1627372052903968\n",
      "sensitive attribute  1\n",
      "prec reca accuracy for each sens\n",
      "0.6131386861313869 0.45161290322580644 0.6790890269151139\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "483\n",
      "137\n",
      "0.2836438923395445\n",
      "sensitive attribute  2\n",
      "prec reca accuracy for each sens\n",
      "0.75 0.34615384615384615 0.7163120567375887\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "141\n",
      "24\n",
      "0.1702127659574468\n",
      "sensitive attribute  3\n",
      "prec reca accuracy for each sens\n",
      "0.6680942184154176 0.7554479418886199 0.6705276705276705\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "777\n",
      "467\n",
      "0.601029601029601\n",
      "sensitive attribute  4\n",
      "prec reca accuracy for each sens\n",
      "0.6538461538461539 0.4788732394366197 0.6994535519125683\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "183\n",
      "52\n",
      "0.28415300546448086\n",
      "sensitive attribute  5\n",
      "prec reca accuracy for each sens\n",
      "0.6335403726708074 0.42857142857142855 0.6875\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "624\n",
      "161\n",
      "0.25801282051282054\n",
      "sensitive attribute  6\n",
      "prec reca accuracy for each sens\n",
      "0.6666666666666666 0.7148760330578512 0.6760416666666667\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "960\n",
      "519\n",
      "0.540625\n",
      "sensitive attribute  7\n",
      "prec reca accuracy for each sens\n",
      "0.6556291390728477 0.66110183639399 0.6738095238095239\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1260\n",
      "604\n",
      "0.4793650793650794\n",
      "sensitive attribute  8\n",
      "prec reca accuracy for each sens\n",
      "0.6842105263157895 0.42276422764227645 0.7067901234567902\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "324\n",
      "76\n",
      "0.2345679012345679\n",
      "data acceptance rates\n",
      "[0.2836438923395445, 0.1702127659574468, 0.601029601029601, 0.28415300546448086, 0.25801282051282054, 0.540625, 0.4793650793650794, 0.2345679012345679]\n",
      "data DP\n",
      "0.4308168350721542\n",
      "SVM accuracy--------------------------\n",
      "0.6588235294117647 0.6204986149584487 0.6805555555555556\n",
      "dimension of data\n",
      "8 1584\n",
      "Optimal\n",
      "objective is:\n",
      "429759.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.4451345755693582, 0.41134751773049644, 0.462033462033462, 0.46994535519125685, 0.4375, 0.4635416666666667, 0.45555555555555555, 0.4444444444444444] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 0\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "sensitive attribute  7\n",
      "sensitive attribute  8\n",
      "individual acceptance rates\n",
      "[0.4451345755693582, 0.41134751773049644, 0.462033462033462, 0.46994535519125685, 0.4375, 0.4635416666666667, 0.45555555555555555, 0.4444444444444444]\n",
      "individul precision\n",
      "[0.5348837209302325, 0.46551724137931033, 0.7019498607242339, 0.5581395348837209, 0.5201465201465202, 0.6741573033707865, 0.6393728222996515, 0.5208333333333334]\n",
      "individual recall\n",
      "[0.6182795698924731, 0.5192307692307693, 0.6101694915254238, 0.676056338028169, 0.5966386554621849, 0.6198347107438017, 0.6126878130217028, 0.6097560975609756]\n",
      "DP all\n",
      "0.058597837460760416\n",
      "precision all 0.6155988857938719\n",
      "recall all 0.6121883656509696\n",
      "accuracy all 0.648989898989899\n",
      "TP,FP,TN,FN\n",
      "442 276 586 280\n",
      "dimension of data\n",
      "8 1584\n",
      "Optimal\n",
      "objective is:\n",
      "442911.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.4451345755693582, 0.41134751773049644, 0.47619047619047616, 0.46994535519125685, 0.4375, 0.475, 0.4642857142857143, 0.4444444444444444] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 0\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "sensitive attribute  7\n",
      "sensitive attribute  8\n",
      "individual acceptance rates\n",
      "[0.4451345755693582, 0.41134751773049644, 0.47619047619047616, 0.46994535519125685, 0.4375, 0.475, 0.4642857142857143, 0.4444444444444444]\n",
      "individul precision\n",
      "[0.5348837209302325, 0.46551724137931033, 0.6972972972972973, 0.5581395348837209, 0.5201465201465202, 0.6710526315789473, 0.6376068376068376, 0.5208333333333334]\n",
      "individual recall\n",
      "[0.6182795698924731, 0.5192307692307693, 0.6246973365617433, 0.676056338028169, 0.5966386554621849, 0.6322314049586777, 0.6227045075125208, 0.6097560975609756]\n",
      "DP all\n",
      "0.06484295845997973\n",
      "precision all 0.6145404663923183\n",
      "recall all 0.6204986149584487\n",
      "accuracy all 0.6496212121212122\n",
      "TP,FP,TN,FN\n",
      "448 281 581 274\n",
      "dimension of data\n",
      "8 1584\n",
      "Optimal\n",
      "objective is:\n",
      "435494.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.4306418219461698, 0.41134751773049644, 0.4774774774774775, 0.46994535519125685, 0.42628205128205127, 0.47604166666666664, 0.4595238095238095, 0.4444444444444444] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 0\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "sensitive attribute  7\n",
      "sensitive attribute  8\n",
      "individual acceptance rates\n",
      "[0.4306418219461698, 0.41134751773049644, 0.4774774774774775, 0.46994535519125685, 0.42628205128205127, 0.47604166666666664, 0.4595238095238095, 0.4444444444444444]\n",
      "individul precision\n",
      "[0.5432692307692307, 0.46551724137931033, 0.6981132075471698, 0.5581395348837209, 0.5263157894736842, 0.6717724288840262, 0.6424870466321243, 0.5208333333333334]\n",
      "individual recall\n",
      "[0.6075268817204301, 0.5192307692307693, 0.6271186440677966, 0.676056338028169, 0.5882352941176471, 0.6342975206611571, 0.6210350584307178, 0.6097560975609756]\n",
      "DP all\n",
      "0.06612995974698105\n",
      "precision all 0.6182572614107884\n",
      "recall all 0.6191135734072022\n",
      "accuracy all 0.6521464646464646\n",
      "TP,FP,TN,FN\n",
      "447 276 586 275\n",
      "dimension of data\n",
      "8 1584\n",
      "Optimal\n",
      "objective is:\n",
      "446665.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.4451345755693582, 0.41134751773049644, 0.47619047619047616, 0.4918032786885246, 0.4375, 0.4791666666666667, 0.4642857142857143, 0.4567901234567901] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 0\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "sensitive attribute  7\n",
      "sensitive attribute  8\n",
      "individual acceptance rates\n",
      "[0.4451345755693582, 0.41134751773049644, 0.47619047619047616, 0.4918032786885246, 0.4375, 0.4791666666666667, 0.4642857142857143, 0.4567901234567901]\n",
      "individul precision\n",
      "[0.5348837209302325, 0.46551724137931033, 0.6972972972972973, 0.5444444444444444, 0.5201465201465202, 0.6673913043478261, 0.6376068376068376, 0.5135135135135135]\n",
      "individual recall\n",
      "[0.6182795698924731, 0.5192307692307693, 0.6246973365617433, 0.6901408450704225, 0.5966386554621849, 0.6342975206611571, 0.6227045075125208, 0.6178861788617886]\n",
      "DP all\n",
      "0.08045576095802814\n",
      "precision all 0.6125511596180082\n",
      "recall all 0.6218836565096952\n",
      "accuracy all 0.6483585858585859\n",
      "TP,FP,TN,FN\n",
      "449 284 578 273\n"
     ]
    }
   ],
   "source": [
    "#gerry Yang\n",
    "accu,DP_all,acceptance_rate,alpha_weight = main(sensitiven, Y_test, Y_test_pred,e )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.style.use('seaborn-white')\n",
    "\n",
    "labels = ['0.0585','0.0648', '0.0661','0.0804']\n",
    "LP = [.6489,0.6496,0.6521,0.6483]\n",
    "YA = [ 0.6540,0.6521, 0.6534, 0.6521]\n",
    "\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, LP, width, color = 'g',label='LPCA ')\n",
    "rects2 = ax.bar(x + width/2, YA, width, color = 'blue',label='Yang ')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Test Accuracy')\n",
    "ax.set_xlabel('Demographic Disparity')\n",
    "ax.set_title('COMPAS')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "#ax.legend()\n",
    "ax.set_ylim(.5,.7 )\n",
    "#ax.bar_label(rects1, padding=3)\n",
    "#ax.bar_label(rects2, padding=3)\n",
    "\n",
    "ax.legend(loc='lower center', bbox_to_anchor=(0.5, -0.30), shadow=True, ncol=10)\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "# plt.savefig('a5.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensitive attribute  1\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "624\n",
      "238\n",
      "0.3814102564102564\n",
      "sensitive attribute  2\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "960\n",
      "484\n",
      "0.5041666666666667\n",
      "sensitive attribute  3\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1260\n",
      "599\n",
      "0.4753968253968254\n",
      "sensitive attribute  4\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "324\n",
      "123\n",
      "0.37962962962962965\n",
      "data acceptance rates\n",
      "[0.3814102564102564, 0.5041666666666667, 0.4753968253968254, 0.37962962962962965]\n",
      "data DP\n",
      "0.124537037037037\n",
      "sensitive attribute  1\n",
      "prec reca accuracy for each sens\n",
      "0.6335403726708074 0.42857142857142855 0.6875\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "624\n",
      "161\n",
      "0.25801282051282054\n",
      "sensitive attribute  2\n",
      "prec reca accuracy for each sens\n",
      "0.6666666666666666 0.7148760330578512 0.6760416666666667\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "960\n",
      "519\n",
      "0.540625\n",
      "sensitive attribute  3\n",
      "prec reca accuracy for each sens\n",
      "0.6556291390728477 0.66110183639399 0.6738095238095239\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1260\n",
      "604\n",
      "0.4793650793650794\n",
      "sensitive attribute  4\n",
      "prec reca accuracy for each sens\n",
      "0.6842105263157895 0.42276422764227645 0.7067901234567902\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "324\n",
      "76\n",
      "0.2345679012345679\n",
      "data acceptance rates\n",
      "[0.25801282051282054, 0.540625, 0.4793650793650794, 0.2345679012345679]\n",
      "data DP\n",
      "0.30605709876543213\n",
      "SVM accuracy--------------------------\n",
      "0.6588235294117647 0.6204986149584487 0.6805555555555556\n",
      "dimension of data\n",
      "4 1584\n",
      "Optimal\n",
      "objective is:\n",
      "349077.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.25961538461538464, 0.51875, 0.430952380952381, 0.3611111111111111] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 0\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "individual acceptance rates\n",
      "[0.25961538461538464, 0.51875, 0.430952380952381, 0.3611111111111111]\n",
      "individul precision\n",
      "[0.6172839506172839, 0.6606425702811245, 0.6666666666666666, 0.5726495726495726]\n",
      "individual recall\n",
      "[0.42016806722689076, 0.6797520661157025, 0.6043405676126878, 0.5447154471544715]\n",
      "DP all\n",
      "0.2591346153846154\n",
      "precision all 0.65\n",
      "recall all 0.5941828254847645\n",
      "accuracy all 0.6691919191919192\n",
      "TP,FP,TN,FN\n",
      "429 231 631 293\n",
      "dimension of data\n",
      "4 1584\n",
      "Optimal\n",
      "objective is:\n",
      "496773.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.25961538461538464, 0.51875, 0.430952380952381, 0.3611111111111111] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 1\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "individual acceptance rates\n",
      "[0.25961538461538464, 0.51875, 0.430952380952381, 0.3611111111111111]\n",
      "individul precision\n",
      "[0.6172839506172839, 0.6606425702811245, 0.6666666666666666, 0.5726495726495726]\n",
      "individual recall\n",
      "[0.42016806722689076, 0.6797520661157025, 0.6043405676126878, 0.5447154471544715]\n",
      "DP all\n",
      "0.2591346153846154\n",
      "precision all 0.65\n",
      "recall all 0.5941828254847645\n",
      "accuracy all 0.6691919191919192\n",
      "TP,FP,TN,FN\n",
      "429 231 631 293\n",
      "dimension of data\n",
      "4 1584\n",
      "Optimal\n",
      "objective is:\n",
      "792165.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.25961538461538464, 0.51875, 0.430952380952381, 0.3611111111111111] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 2\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "individual acceptance rates\n",
      "[0.25961538461538464, 0.51875, 0.430952380952381, 0.3611111111111111]\n",
      "individul precision\n",
      "[0.6172839506172839, 0.6606425702811245, 0.6666666666666666, 0.5726495726495726]\n",
      "individual recall\n",
      "[0.42016806722689076, 0.6797520661157025, 0.6043405676126878, 0.5447154471544715]\n",
      "DP all\n",
      "0.2591346153846154\n",
      "precision all 0.65\n",
      "recall all 0.5941828254847645\n",
      "accuracy all 0.6691919191919192\n",
      "TP,FP,TN,FN\n",
      "429 231 631 293\n",
      "dimension of data\n",
      "4 1584\n",
      "Optimal\n",
      "objective is:\n",
      "1087557.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.25961538461538464, 0.51875, 0.430952380952381, 0.3611111111111111] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 3\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "individual acceptance rates\n",
      "[0.25961538461538464, 0.51875, 0.430952380952381, 0.3611111111111111]\n",
      "individul precision\n",
      "[0.6172839506172839, 0.6606425702811245, 0.6666666666666666, 0.5726495726495726]\n",
      "individual recall\n",
      "[0.42016806722689076, 0.6797520661157025, 0.6043405676126878, 0.5447154471544715]\n",
      "DP all\n",
      "0.2591346153846154\n",
      "precision all 0.65\n",
      "recall all 0.5941828254847645\n",
      "accuracy all 0.6691919191919192\n",
      "TP,FP,TN,FN\n",
      "429 231 631 293\n",
      "dimension of data\n",
      "4 1584\n",
      "Optimal\n",
      "objective is:\n",
      "1382949.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.25961538461538464, 0.51875, 0.430952380952381, 0.3611111111111111] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 4\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "individual acceptance rates\n",
      "[0.25961538461538464, 0.51875, 0.430952380952381, 0.3611111111111111]\n",
      "individul precision\n",
      "[0.6172839506172839, 0.6606425702811245, 0.6666666666666666, 0.5726495726495726]\n",
      "individual recall\n",
      "[0.42016806722689076, 0.6797520661157025, 0.6043405676126878, 0.5447154471544715]\n",
      "DP all\n",
      "0.2591346153846154\n",
      "precision all 0.65\n",
      "recall all 0.5941828254847645\n",
      "accuracy all 0.6691919191919192\n",
      "TP,FP,TN,FN\n",
      "429 231 631 293\n",
      "dimension of data\n",
      "4 1584\n",
      "Optimal\n",
      "objective is:\n",
      "1678341.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.25961538461538464, 0.51875, 0.430952380952381, 0.3611111111111111] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 5\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "individual acceptance rates\n",
      "[0.25961538461538464, 0.51875, 0.430952380952381, 0.3611111111111111]\n",
      "individul precision\n",
      "[0.6172839506172839, 0.6606425702811245, 0.6666666666666666, 0.5726495726495726]\n",
      "individual recall\n",
      "[0.42016806722689076, 0.6797520661157025, 0.6043405676126878, 0.5447154471544715]\n",
      "DP all\n",
      "0.2591346153846154\n",
      "precision all 0.65\n",
      "recall all 0.5941828254847645\n",
      "accuracy all 0.6691919191919192\n",
      "TP,FP,TN,FN\n",
      "429 231 631 293\n",
      "dimension of data\n",
      "4 1584\n",
      "Optimal\n",
      "objective is:\n",
      "2416821.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.25961538461538464, 0.51875, 0.430952380952381, 0.3611111111111111] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 6\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "individual acceptance rates\n",
      "[0.25961538461538464, 0.51875, 0.430952380952381, 0.3611111111111111]\n",
      "individul precision\n",
      "[0.6172839506172839, 0.6606425702811245, 0.6666666666666666, 0.5726495726495726]\n",
      "individual recall\n",
      "[0.42016806722689076, 0.6797520661157025, 0.6043405676126878, 0.5447154471544715]\n",
      "DP all\n",
      "0.2591346153846154\n",
      "precision all 0.65\n",
      "recall all 0.5941828254847645\n",
      "accuracy all 0.6691919191919192\n",
      "TP,FP,TN,FN\n",
      "429 231 631 293\n",
      "dimension of data\n",
      "4 1584\n",
      "Optimal\n",
      "objective is:\n",
      "230703.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.25961538461538464, 0.51875, 0.430952380952381, 0.3611111111111111] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 7\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "individual acceptance rates\n",
      "[0.25961538461538464, 0.51875, 0.430952380952381, 0.3611111111111111]\n",
      "individul precision\n",
      "[0.6172839506172839, 0.6606425702811245, 0.6666666666666666, 0.5726495726495726]\n",
      "individual recall\n",
      "[0.42016806722689076, 0.6797520661157025, 0.6043405676126878, 0.5447154471544715]\n",
      "DP all\n",
      "0.2591346153846154\n",
      "precision all 0.65\n",
      "recall all 0.5941828254847645\n",
      "accuracy all 0.6691919191919192\n",
      "TP,FP,TN,FN\n",
      "429 231 631 293\n",
      "dimension of data\n",
      "4 1584\n",
      "Optimal\n",
      "objective is:\n",
      "3893781.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.25961538461538464, 0.51875, 0.430952380952381, 0.3611111111111111] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 8\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "individual acceptance rates\n",
      "[0.25961538461538464, 0.51875, 0.430952380952381, 0.3611111111111111]\n",
      "individul precision\n",
      "[0.6172839506172839, 0.6606425702811245, 0.6666666666666666, 0.5726495726495726]\n",
      "individual recall\n",
      "[0.42016806722689076, 0.6797520661157025, 0.6043405676126878, 0.5447154471544715]\n",
      "DP all\n",
      "0.2591346153846154\n",
      "precision all 0.65\n",
      "recall all 0.5941828254847645\n",
      "accuracy all 0.6691919191919192\n",
      "TP,FP,TN,FN\n",
      "429 231 631 293\n",
      "dimension of data\n",
      "4 1584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal\n",
      "objective is:\n",
      "4632261.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.25961538461538464, 0.51875, 0.430952380952381, 0.3611111111111111] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 9\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "individual acceptance rates\n",
      "[0.25961538461538464, 0.51875, 0.430952380952381, 0.3611111111111111]\n",
      "individul precision\n",
      "[0.6172839506172839, 0.6606425702811245, 0.6666666666666666, 0.5726495726495726]\n",
      "individual recall\n",
      "[0.42016806722689076, 0.6797520661157025, 0.6043405676126878, 0.5447154471544715]\n",
      "DP all\n",
      "0.2591346153846154\n",
      "precision all 0.65\n",
      "recall all 0.5941828254847645\n",
      "accuracy all 0.6691919191919192\n",
      "TP,FP,TN,FN\n",
      "429 231 631 293\n",
      "dimension of data\n",
      "4 1584\n",
      "Optimal\n",
      "objective is:\n",
      "215907.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.25961538461538464, 0.51875, 0.430952380952381, 0.3611111111111111] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 10\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "individual acceptance rates\n",
      "[0.25961538461538464, 0.51875, 0.430952380952381, 0.3611111111111111]\n",
      "individul precision\n",
      "[0.6172839506172839, 0.6606425702811245, 0.6666666666666666, 0.5726495726495726]\n",
      "individual recall\n",
      "[0.42016806722689076, 0.6797520661157025, 0.6043405676126878, 0.5447154471544715]\n",
      "DP all\n",
      "0.2591346153846154\n",
      "precision all 0.65\n",
      "recall all 0.5941828254847645\n",
      "accuracy all 0.6691919191919192\n",
      "TP,FP,TN,FN\n",
      "429 231 631 293\n",
      "dimension of data\n",
      "4 1584\n",
      "Optimal\n",
      "objective is:\n",
      "308934.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.34615384615384615, 0.43125, 0.403968253968254, 0.3734567901234568] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 0\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "individual acceptance rates\n",
      "[0.34615384615384615, 0.43125, 0.403968253968254, 0.3734567901234568]\n",
      "individul precision\n",
      "[0.5416666666666666, 0.678743961352657, 0.656188605108055, 0.5289256198347108]\n",
      "individual recall\n",
      "[0.49159663865546216, 0.5805785123966942, 0.5575959933222037, 0.5203252032520326]\n",
      "DP all\n",
      "0.08509615384615388\n",
      "precision all 0.6317460317460317\n",
      "recall all 0.5512465373961218\n",
      "accuracy all 0.648989898989899\n",
      "TP,FP,TN,FN\n",
      "398 232 630 324\n",
      "dimension of data\n",
      "4 1584\n",
      "Optimal\n",
      "objective is:\n",
      "444489.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.34615384615384615, 0.43125, 0.403968253968254, 0.3734567901234568] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 1\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "individual acceptance rates\n",
      "[0.34615384615384615, 0.43125, 0.403968253968254, 0.3734567901234568]\n",
      "individul precision\n",
      "[0.5462962962962963, 0.6811594202898551, 0.6620825147347741, 0.5206611570247934]\n",
      "individual recall\n",
      "[0.4957983193277311, 0.5826446280991735, 0.5626043405676127, 0.5121951219512195]\n",
      "DP all\n",
      "0.08509615384615388\n",
      "precision all 0.6349206349206349\n",
      "recall all 0.554016620498615\n",
      "accuracy all 0.6515151515151515\n",
      "TP,FP,TN,FN\n",
      "400 230 632 322\n",
      "dimension of data\n",
      "4 1584\n",
      "Optimal\n",
      "objective is:\n",
      "711981.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.34615384615384615, 0.43125, 0.403968253968254, 0.3734567901234568] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 2\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "individual acceptance rates\n",
      "[0.34615384615384615, 0.43125, 0.403968253968254, 0.3734567901234568]\n",
      "individul precision\n",
      "[0.5462962962962963, 0.6980676328502415, 0.6699410609037328, 0.5454545454545454]\n",
      "individual recall\n",
      "[0.4957983193277311, 0.5971074380165289, 0.5692821368948247, 0.5365853658536586]\n",
      "DP all\n",
      "0.08509615384615388\n",
      "precision all 0.6460317460317461\n",
      "recall all 0.5637119113573407\n",
      "accuracy all 0.6603535353535354\n",
      "TP,FP,TN,FN\n",
      "407 223 639 315\n",
      "dimension of data\n",
      "4 1584\n",
      "Optimal\n",
      "objective is:\n",
      "976374.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.34615384615384615, 0.43125, 0.403968253968254, 0.3734567901234568] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 3\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "individual acceptance rates\n",
      "[0.34615384615384615, 0.43125, 0.403968253968254, 0.3734567901234568]\n",
      "individul precision\n",
      "[0.5509259259259259, 0.6980676328502415, 0.6738703339882122, 0.5371900826446281]\n",
      "individual recall\n",
      "[0.5, 0.5971074380165289, 0.5726210350584308, 0.5284552845528455]\n",
      "DP all\n",
      "0.08509615384615388\n",
      "precision all 0.6476190476190476\n",
      "recall all 0.5650969529085873\n",
      "accuracy all 0.6616161616161617\n",
      "TP,FP,TN,FN\n",
      "408 222 640 314\n",
      "dimension of data\n",
      "4 1584\n",
      "Optimal\n",
      "objective is:\n",
      "1238697.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.34615384615384615, 0.43125, 0.403968253968254, 0.3734567901234568] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 4\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "individual acceptance rates\n",
      "[0.34615384615384615, 0.43125, 0.403968253968254, 0.3734567901234568]\n",
      "individul precision\n",
      "[0.5509259259259259, 0.6908212560386473, 0.6699410609037328, 0.5289256198347108]\n",
      "individual recall\n",
      "[0.5, 0.5909090909090909, 0.5692821368948247, 0.5203252032520326]\n",
      "DP all\n",
      "0.08509615384615388\n",
      "precision all 0.6428571428571429\n",
      "recall all 0.5609418282548476\n",
      "accuracy all 0.6578282828282829\n",
      "TP,FP,TN,FN\n",
      "405 225 637 317\n",
      "dimension of data\n",
      "4 1584\n",
      "Optimal\n",
      "objective is:\n",
      "1500387.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.34615384615384615, 0.43125, 0.403968253968254, 0.3734567901234568] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 5\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "individual acceptance rates\n",
      "[0.34615384615384615, 0.43125, 0.403968253968254, 0.3734567901234568]\n",
      "individul precision\n",
      "[0.5462962962962963, 0.6884057971014492, 0.6679764243614931, 0.5206611570247934]\n",
      "individual recall\n",
      "[0.4957983193277311, 0.5888429752066116, 0.5676126878130217, 0.5121951219512195]\n",
      "DP all\n",
      "0.08509615384615388\n",
      "precision all 0.6396825396825396\n",
      "recall all 0.5581717451523546\n",
      "accuracy all 0.6553030303030303\n",
      "TP,FP,TN,FN\n",
      "403 227 635 319\n",
      "dimension of data\n",
      "4 1584\n",
      "Optimal\n",
      "objective is:\n",
      "2153688.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.34615384615384615, 0.43125, 0.403968253968254, 0.3734567901234568] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 6\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "individual acceptance rates\n",
      "[0.34615384615384615, 0.43125, 0.403968253968254, 0.3734567901234568]\n",
      "individul precision\n",
      "[0.5462962962962963, 0.6884057971014492, 0.6679764243614931, 0.5206611570247934]\n",
      "individual recall\n",
      "[0.4957983193277311, 0.5888429752066116, 0.5676126878130217, 0.5121951219512195]\n",
      "DP all\n",
      "0.08509615384615388\n",
      "precision all 0.6396825396825396\n",
      "recall all 0.5581717451523546\n",
      "accuracy all 0.6553030303030303\n",
      "TP,FP,TN,FN\n",
      "403 227 635 319\n",
      "dimension of data\n",
      "4 1584\n",
      "Optimal\n",
      "objective is:\n",
      "198434.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.34615384615384615, 0.43125, 0.403968253968254, 0.3734567901234568] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 7\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "individual acceptance rates\n",
      "[0.34615384615384615, 0.43125, 0.403968253968254, 0.3734567901234568]\n",
      "individul precision\n",
      "[0.5601851851851852, 0.6642512077294686, 0.6444007858546169, 0.5619834710743802]\n",
      "individual recall\n",
      "[0.5084033613445378, 0.5681818181818182, 0.5475792988313857, 0.5528455284552846]\n",
      "DP all\n",
      "0.08509615384615388\n",
      "precision all 0.6285714285714286\n",
      "recall all 0.5484764542936288\n",
      "accuracy all 0.6464646464646465\n",
      "TP,FP,TN,FN\n",
      "396 234 628 326\n",
      "dimension of data\n",
      "4 1584\n",
      "Optimal\n",
      "objective is:\n",
      "3458324.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.34615384615384615, 0.43125, 0.403968253968254, 0.3734567901234568] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 8\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "individual acceptance rates\n",
      "[0.34615384615384615, 0.43125, 0.403968253968254, 0.3734567901234568]\n",
      "individul precision\n",
      "[0.5416666666666666, 0.6884057971014492, 0.6719056974459725, 0.49586776859504134]\n",
      "individual recall\n",
      "[0.49159663865546216, 0.5888429752066116, 0.5709515859766278, 0.4878048780487805]\n",
      "DP all\n",
      "0.08509615384615388\n",
      "precision all 0.638095238095238\n",
      "recall all 0.556786703601108\n",
      "accuracy all 0.6540404040404041\n",
      "TP,FP,TN,FN\n",
      "402 228 634 320\n",
      "dimension of data\n",
      "4 1584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal\n",
      "objective is:\n",
      "4109874.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.34615384615384615, 0.43125, 0.403968253968254, 0.3734567901234568] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 9\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "individual acceptance rates\n",
      "[0.34615384615384615, 0.43125, 0.403968253968254, 0.3734567901234568]\n",
      "individul precision\n",
      "[0.5416666666666666, 0.6859903381642513, 0.6699410609037328, 0.49586776859504134]\n",
      "individual recall\n",
      "[0.49159663865546216, 0.5867768595041323, 0.5692821368948247, 0.4878048780487805]\n",
      "DP all\n",
      "0.08509615384615388\n",
      "precision all 0.6365079365079365\n",
      "recall all 0.5554016620498615\n",
      "accuracy all 0.6527777777777778\n",
      "TP,FP,TN,FN\n",
      "401 229 633 321\n",
      "dimension of data\n",
      "4 1584\n",
      "Optimal\n",
      "objective is:\n",
      "184447.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.34615384615384615, 0.43125, 0.403968253968254, 0.3734567901234568] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 10\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "individual acceptance rates\n",
      "[0.34615384615384615, 0.43125, 0.403968253968254, 0.3734567901234568]\n",
      "individul precision\n",
      "[0.5601851851851852, 0.6642512077294686, 0.6444007858546169, 0.5619834710743802]\n",
      "individual recall\n",
      "[0.5084033613445378, 0.5681818181818182, 0.5475792988313857, 0.5528455284552846]\n",
      "DP all\n",
      "0.08509615384615388\n",
      "precision all 0.6285714285714286\n",
      "recall all 0.5484764542936288\n",
      "accuracy all 0.6464646464646465\n",
      "TP,FP,TN,FN\n",
      "396 234 628 326\n",
      "dimension of data\n",
      "4 1584\n",
      "Optimal\n",
      "objective is:\n",
      "113938.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.27884615384615385, 0.20416666666666666, 0.23015873015873015, 0.24691358024691357] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 0\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "individual acceptance rates\n",
      "[0.27884615384615385, 0.20416666666666666, 0.23015873015873015, 0.24691358024691357]\n",
      "individul precision\n",
      "[0.5862068965517241, 0.7551020408163265, 0.7068965517241379, 0.5625]\n",
      "individual recall\n",
      "[0.42857142857142855, 0.30578512396694213, 0.34223706176961605, 0.36585365853658536]\n",
      "DP all\n",
      "0.07467948717948719\n",
      "precision all 0.6756756756756757\n",
      "recall all 0.3462603878116344\n",
      "accuracy all 0.6262626262626263\n",
      "TP,FP,TN,FN\n",
      "250 120 742 472\n",
      "dimension of data\n",
      "4 1584\n",
      "Optimal\n",
      "objective is:\n",
      "165974.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.27884615384615385, 0.20416666666666666, 0.23015873015873015, 0.24691358024691357] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 1\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "individual acceptance rates\n",
      "[0.27884615384615385, 0.20416666666666666, 0.23015873015873015, 0.24691358024691357]\n",
      "individul precision\n",
      "[0.5747126436781609, 0.7448979591836735, 0.7034482758620689, 0.525]\n",
      "individual recall\n",
      "[0.42016806722689076, 0.30165289256198347, 0.34056761268781305, 0.34146341463414637]\n",
      "DP all\n",
      "0.07467948717948719\n",
      "precision all 0.6648648648648648\n",
      "recall all 0.3407202216066482\n",
      "accuracy all 0.6212121212121212\n",
      "TP,FP,TN,FN\n",
      "246 124 738 476\n",
      "dimension of data\n",
      "4 1584\n",
      "Optimal\n",
      "objective is:\n",
      "264071.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.27884615384615385, 0.20416666666666666, 0.23015873015873015, 0.24691358024691357] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 2\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "individual acceptance rates\n",
      "[0.27884615384615385, 0.20416666666666666, 0.23015873015873015, 0.24691358024691357]\n",
      "individul precision\n",
      "[0.5689655172413793, 0.7551020408163265, 0.7172413793103448, 0.4875]\n",
      "individual recall\n",
      "[0.41596638655462187, 0.30578512396694213, 0.34724540901502504, 0.3170731707317073]\n",
      "DP all\n",
      "0.07467948717948719\n",
      "precision all 0.6675675675675675\n",
      "recall all 0.34210526315789475\n",
      "accuracy all 0.6224747474747475\n",
      "TP,FP,TN,FN\n",
      "247 123 739 475\n",
      "dimension of data\n",
      "4 1584\n",
      "Optimal\n",
      "objective is:\n",
      "357672.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.27884615384615385, 0.20416666666666666, 0.23015873015873015, 0.24691358024691357] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 3\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "individual acceptance rates\n",
      "[0.27884615384615385, 0.20625, 0.23015873015873015, 0.25308641975308643]\n",
      "individul precision\n",
      "[0.5574712643678161, 0.7525252525252525, 0.7206896551724138, 0.45121951219512196]\n",
      "individual recall\n",
      "[0.40756302521008403, 0.30785123966942146, 0.34891485809682804, 0.3008130081300813]\n",
      "DP all\n",
      "0.07259615384615387\n",
      "precision all 0.6612903225806451\n",
      "recall all 0.3407202216066482\n",
      "accuracy all 0.6199494949494949\n",
      "TP,FP,TN,FN\n",
      "246 126 736 476\n",
      "dimension of data\n",
      "4 1584\n",
      "Optimal\n",
      "objective is:\n",
      "447813.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.27884615384615385, 0.20416666666666666, 0.23015873015873015, 0.24691358024691357] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 4\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "individual acceptance rates\n",
      "[0.27884615384615385, 0.20833333333333334, 0.23015873015873015, 0.25925925925925924]\n",
      "individul precision\n",
      "[0.5574712643678161, 0.75, 0.7206896551724138, 0.4523809523809524]\n",
      "individual recall\n",
      "[0.40756302521008403, 0.30991735537190085, 0.34891485809682804, 0.3089430894308943]\n",
      "DP all\n",
      "0.07051282051282051\n",
      "precision all 0.660427807486631\n",
      "recall all 0.34210526315789475\n",
      "accuracy all 0.6199494949494949\n",
      "TP,FP,TN,FN\n",
      "247 127 735 475\n",
      "dimension of data\n",
      "4 1584\n",
      "Optimal\n",
      "objective is:\n",
      "537873.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.27884615384615385, 0.20416666666666666, 0.23015873015873015, 0.24691358024691357] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 5\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "individual acceptance rates\n",
      "[0.27884615384615385, 0.20833333333333334, 0.23015873015873015, 0.25925925925925924]\n",
      "individul precision\n",
      "[0.5574712643678161, 0.75, 0.7206896551724138, 0.4523809523809524]\n",
      "individual recall\n",
      "[0.40756302521008403, 0.30991735537190085, 0.34891485809682804, 0.3089430894308943]\n",
      "DP all\n",
      "0.07051282051282051\n",
      "precision all 0.660427807486631\n",
      "recall all 0.34210526315789475\n",
      "accuracy all 0.6199494949494949\n",
      "TP,FP,TN,FN\n",
      "247 127 735 475\n",
      "dimension of data\n",
      "4 1584\n",
      "Optimal\n",
      "objective is:\n",
      "763023.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.27884615384615385, 0.20416666666666666, 0.23015873015873015, 0.24691358024691357] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 6\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "individual acceptance rates\n",
      "[0.27884615384615385, 0.20833333333333334, 0.23015873015873015, 0.25925925925925924]\n",
      "individul precision\n",
      "[0.5574712643678161, 0.75, 0.7206896551724138, 0.4523809523809524]\n",
      "individual recall\n",
      "[0.40756302521008403, 0.30991735537190085, 0.34891485809682804, 0.3089430894308943]\n",
      "DP all\n",
      "0.07051282051282051\n",
      "precision all 0.660427807486631\n",
      "recall all 0.34210526315789475\n",
      "accuracy all 0.6199494949494949\n",
      "TP,FP,TN,FN\n",
      "247 127 735 475\n",
      "dimension of data\n",
      "4 1584\n",
      "Optimal\n",
      "objective is:\n",
      "68271.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.27884615384615385, 0.20416666666666666, 0.23015873015873015, 0.24691358024691357] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 7\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "individual acceptance rates\n",
      "[0.27884615384615385, 0.20416666666666666, 0.23015873015873015, 0.24691358024691357]\n",
      "individul precision\n",
      "[0.6091954022988506, 0.75, 0.6931034482758621, 0.65]\n",
      "individual recall\n",
      "[0.44537815126050423, 0.3037190082644628, 0.335559265442404, 0.42276422764227645]\n",
      "DP all\n",
      "0.07467948717948719\n",
      "precision all 0.6837837837837838\n",
      "recall all 0.35041551246537395\n",
      "accuracy all 0.6300505050505051\n",
      "TP,FP,TN,FN\n",
      "253 117 745 469\n",
      "dimension of data\n",
      "4 1584\n",
      "Optimal\n",
      "objective is:\n",
      "1213323.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.27884615384615385, 0.20416666666666666, 0.23015873015873015, 0.24691358024691357] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 8\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "individual acceptance rates\n",
      "[0.27884615384615385, 0.20833333333333334, 0.23015873015873015, 0.25925925925925924]\n",
      "individul precision\n",
      "[0.5574712643678161, 0.75, 0.7206896551724138, 0.4523809523809524]\n",
      "individual recall\n",
      "[0.40756302521008403, 0.30991735537190085, 0.34891485809682804, 0.3089430894308943]\n",
      "DP all\n",
      "0.07051282051282051\n",
      "precision all 0.660427807486631\n",
      "recall all 0.34210526315789475\n",
      "accuracy all 0.6199494949494949\n",
      "TP,FP,TN,FN\n",
      "247 127 735 475\n",
      "dimension of data\n",
      "4 1584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal\n",
      "objective is:\n",
      "1438473.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.27884615384615385, 0.20416666666666666, 0.23015873015873015, 0.24691358024691357] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 9\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "individual acceptance rates\n",
      "[0.27884615384615385, 0.20833333333333334, 0.23015873015873015, 0.25925925925925924]\n",
      "individul precision\n",
      "[0.5574712643678161, 0.75, 0.7206896551724138, 0.4523809523809524]\n",
      "individual recall\n",
      "[0.40756302521008403, 0.30991735537190085, 0.34891485809682804, 0.3089430894308943]\n",
      "DP all\n",
      "0.07051282051282051\n",
      "precision all 0.660427807486631\n",
      "recall all 0.34210526315789475\n",
      "accuracy all 0.6199494949494949\n",
      "TP,FP,TN,FN\n",
      "247 127 735 475\n",
      "dimension of data\n",
      "4 1584\n",
      "Optimal\n",
      "objective is:\n",
      "62424.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.27884615384615385, 0.20416666666666666, 0.23015873015873015, 0.24691358024691357] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 10\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "individual acceptance rates\n",
      "[0.27884615384615385, 0.20416666666666666, 0.23015873015873015, 0.24691358024691357]\n",
      "individul precision\n",
      "[0.6091954022988506, 0.75, 0.6931034482758621, 0.65]\n",
      "individual recall\n",
      "[0.44537815126050423, 0.3037190082644628, 0.335559265442404, 0.42276422764227645]\n",
      "DP all\n",
      "0.07467948717948719\n",
      "precision all 0.6837837837837838\n",
      "recall all 0.35041551246537395\n",
      "accuracy all 0.6300505050505051\n",
      "TP,FP,TN,FN\n",
      "253 117 745 469\n",
      "dimension of data\n",
      "4 1584\n",
      "Optimal\n",
      "objective is:\n",
      "159458.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.2948717948717949, 0.2708333333333333, 0.2857142857142857, 0.25925925925925924] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 0\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "individual acceptance rates\n",
      "[0.2948717948717949, 0.2708333333333333, 0.2857142857142857, 0.25925925925925924]\n",
      "individul precision\n",
      "[0.592391304347826, 0.7230769230769231, 0.6777777777777778, 0.6309523809523809]\n",
      "individual recall\n",
      "[0.4579831932773109, 0.3884297520661157, 0.4073455759599332, 0.43089430894308944]\n",
      "DP all\n",
      "0.035612535612535634\n",
      "precision all 0.668918918918919\n",
      "recall all 0.4113573407202216\n",
      "accuracy all 0.6388888888888888\n",
      "TP,FP,TN,FN\n",
      "297 147 715 425\n",
      "dimension of data\n",
      "4 1584\n",
      "Optimal\n",
      "objective is:\n",
      "234933.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.2948717948717949, 0.2708333333333333, 0.2857142857142857, 0.25925925925925924] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 1\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "individual acceptance rates\n",
      "[0.2948717948717949, 0.2708333333333333, 0.2857142857142857, 0.25925925925925924]\n",
      "individul precision\n",
      "[0.5652173913043478, 0.7192307692307692, 0.6833333333333333, 0.5357142857142857]\n",
      "individual recall\n",
      "[0.4369747899159664, 0.38636363636363635, 0.41068447412353926, 0.36585365853658536]\n",
      "DP all\n",
      "0.035612535612535634\n",
      "precision all 0.6554054054054054\n",
      "recall all 0.40304709141274236\n",
      "accuracy all 0.6313131313131313\n",
      "TP,FP,TN,FN\n",
      "291 153 709 431\n",
      "dimension of data\n",
      "4 1584\n",
      "Optimal\n",
      "objective is:\n",
      "378638.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.2948717948717949, 0.2708333333333333, 0.2857142857142857, 0.25925925925925924] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 2\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "individual acceptance rates\n",
      "[0.2948717948717949, 0.2708333333333333, 0.2857142857142857, 0.25925925925925924]\n",
      "individul precision\n",
      "[0.5760869565217391, 0.7192307692307692, 0.6944444444444444, 0.5119047619047619]\n",
      "individual recall\n",
      "[0.44537815126050423, 0.38636363636363635, 0.41736227045075125, 0.34959349593495936]\n",
      "DP all\n",
      "0.035612535612535634\n",
      "precision all 0.6599099099099099\n",
      "recall all 0.40581717451523547\n",
      "accuracy all 0.6338383838383839\n",
      "TP,FP,TN,FN\n",
      "293 151 711 429\n",
      "dimension of data\n",
      "4 1584\n",
      "Optimal\n",
      "objective is:\n",
      "518410.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.2948717948717949, 0.2708333333333333, 0.2857142857142857, 0.25925925925925924] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 3\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "individual acceptance rates\n",
      "[0.2948717948717949, 0.2708333333333333, 0.2857142857142857, 0.25925925925925924]\n",
      "individul precision\n",
      "[0.5652173913043478, 0.7192307692307692, 0.6916666666666667, 0.5]\n",
      "individual recall\n",
      "[0.4369747899159664, 0.38636363636363635, 0.41569282136894825, 0.34146341463414637]\n",
      "DP all\n",
      "0.035612535612535634\n",
      "precision all 0.6554054054054054\n",
      "recall all 0.40304709141274236\n",
      "accuracy all 0.6313131313131313\n",
      "TP,FP,TN,FN\n",
      "291 153 709 431\n",
      "dimension of data\n",
      "4 1584\n",
      "Optimal\n",
      "objective is:\n",
      "655763.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.2948717948717949, 0.2708333333333333, 0.2857142857142857, 0.25925925925925924] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 4\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "individual acceptance rates\n",
      "[0.2948717948717949, 0.2708333333333333, 0.2857142857142857, 0.25925925925925924]\n",
      "individul precision\n",
      "[0.5597826086956522, 0.7192307692307692, 0.6944444444444444, 0.47619047619047616]\n",
      "individual recall\n",
      "[0.4327731092436975, 0.38636363636363635, 0.41736227045075125, 0.3252032520325203]\n",
      "DP all\n",
      "0.035612535612535634\n",
      "precision all 0.6531531531531531\n",
      "recall all 0.40166204986149584\n",
      "accuracy all 0.6300505050505051\n",
      "TP,FP,TN,FN\n",
      "290 154 708 432\n",
      "dimension of data\n",
      "4 1584\n",
      "Optimal\n",
      "objective is:\n",
      "791737.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.2948717948717949, 0.2708333333333333, 0.2857142857142857, 0.25925925925925924] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 5\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "individual acceptance rates\n",
      "[0.2948717948717949, 0.2708333333333333, 0.2857142857142857, 0.25925925925925924]\n",
      "individul precision\n",
      "[0.5543478260869565, 0.7153846153846154, 0.6944444444444444, 0.4523809523809524]\n",
      "individual recall\n",
      "[0.42857142857142855, 0.384297520661157, 0.41736227045075125, 0.3089430894308943]\n",
      "DP all\n",
      "0.035612535612535634\n",
      "precision all 0.6486486486486487\n",
      "recall all 0.3988919667590028\n",
      "accuracy all 0.6275252525252525\n",
      "TP,FP,TN,FN\n",
      "288 156 706 434\n",
      "dimension of data\n",
      "4 1584\n",
      "Optimal\n",
      "objective is:\n",
      "1126751.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.2948717948717949, 0.2708333333333333, 0.2857142857142857, 0.25925925925925924] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 6\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "individual acceptance rates\n",
      "[0.2948717948717949, 0.275, 0.2857142857142857, 0.2716049382716049]\n",
      "individul precision\n",
      "[0.5489130434782609, 0.7121212121212122, 0.6944444444444444, 0.4431818181818182]\n",
      "individual recall\n",
      "[0.42436974789915966, 0.3884297520661157, 0.41736227045075125, 0.3170731707317073]\n",
      "DP all\n",
      "0.023266856600189956\n",
      "precision all 0.6450892857142857\n",
      "recall all 0.4002770083102493\n",
      "accuracy all 0.6262626262626263\n",
      "TP,FP,TN,FN\n",
      "289 159 703 433\n",
      "dimension of data\n",
      "4 1584\n",
      "Optimal\n",
      "objective is:\n",
      "96297.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.2948717948717949, 0.2708333333333333, 0.2857142857142857, 0.25925925925925924] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 7\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "individual acceptance rates\n",
      "[0.2948717948717949, 0.2708333333333333, 0.2857142857142857, 0.25925925925925924]\n",
      "individul precision\n",
      "[0.592391304347826, 0.7269230769230769, 0.675, 0.6547619047619048]\n",
      "individual recall\n",
      "[0.4579831932773109, 0.390495867768595, 0.4056761268781302, 0.44715447154471544]\n",
      "DP all\n",
      "0.035612535612535634\n",
      "precision all 0.6711711711711712\n",
      "recall all 0.41274238227146814\n",
      "accuracy all 0.6401515151515151\n",
      "TP,FP,TN,FN\n",
      "298 146 716 424\n",
      "dimension of data\n",
      "4 1584\n",
      "Optimal\n",
      "objective is:\n",
      "1795031.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.2948717948717949, 0.2708333333333333, 0.2857142857142857, 0.25925925925925924] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 8\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "individual acceptance rates\n",
      "[0.2948717948717949, 0.275, 0.2857142857142857, 0.2716049382716049]\n",
      "individul precision\n",
      "[0.5489130434782609, 0.7121212121212122, 0.6944444444444444, 0.4431818181818182]\n",
      "individual recall\n",
      "[0.42436974789915966, 0.3884297520661157, 0.41736227045075125, 0.3170731707317073]\n",
      "DP all\n",
      "0.023266856600189956\n",
      "precision all 0.6450892857142857\n",
      "recall all 0.4002770083102493\n",
      "accuracy all 0.6262626262626263\n",
      "TP,FP,TN,FN\n",
      "289 159 703 433\n",
      "dimension of data\n",
      "4 1584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal\n",
      "objective is:\n",
      "2129171.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.2948717948717949, 0.2708333333333333, 0.2857142857142857, 0.25925925925925924] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 9\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "individual acceptance rates\n",
      "[0.2948717948717949, 0.275, 0.2857142857142857, 0.2716049382716049]\n",
      "individul precision\n",
      "[0.5489130434782609, 0.7121212121212122, 0.6944444444444444, 0.4431818181818182]\n",
      "individual recall\n",
      "[0.42436974789915966, 0.3884297520661157, 0.41736227045075125, 0.3170731707317073]\n",
      "DP all\n",
      "0.023266856600189956\n",
      "precision all 0.6450892857142857\n",
      "recall all 0.4002770083102493\n",
      "accuracy all 0.6262626262626263\n",
      "TP,FP,TN,FN\n",
      "289 159 703 433\n",
      "dimension of data\n",
      "4 1584\n",
      "Optimal\n",
      "objective is:\n",
      "88341.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.2948717948717949, 0.2708333333333333, 0.2857142857142857, 0.25925925925925924] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 10\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "individual acceptance rates\n",
      "[0.2948717948717949, 0.2708333333333333, 0.2857142857142857, 0.25925925925925924]\n",
      "individul precision\n",
      "[0.5978260869565217, 0.7230769230769231, 0.675, 0.6547619047619048]\n",
      "individual recall\n",
      "[0.46218487394957986, 0.3884297520661157, 0.4056761268781302, 0.44715447154471544]\n",
      "DP all\n",
      "0.035612535612535634\n",
      "precision all 0.6711711711711712\n",
      "recall all 0.41274238227146814\n",
      "accuracy all 0.6401515151515151\n",
      "TP,FP,TN,FN\n",
      "298 146 716 424\n"
     ]
    }
   ],
   "source": [
    "#Overlapping\n",
    "accu_all,DP_all,acceptance_rate,alpha_weight = main(sensitive, Y_test, Y_test_pred,e )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
