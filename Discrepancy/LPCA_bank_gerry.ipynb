{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7.73645546 26.09922918  9.33333333 29.7029703   9.42622951  6.08695652\n",
      " 21.27932865]\n",
      "29.7029703\n",
      "11\n",
      "[[0.0773645546, 0.2609922918]]\n",
      "0.2609922918\n",
      "0.0933333333\n",
      "0.0933333333\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "gamma2=[[ 7.73645546,  8.12672176,  8.99908173, 10.30762167, 11.98347107, 13.08539945,\n",
    "     13.86593205, 14.09550046, 14.02662994, 15.22038567, 15.6795225 ]\n",
    "    ,[26.09922918, 25.51297362 ,24.15590055, 22.42970362 ,20.56237108 ,19.45499946\n",
    "     ,17.92422104 ,16.13288459, 14.07013354, 12.202801,   10.98686353]\n",
    "    ,[ 9.33333333,  9.33333333  ,8.66666667,  8.       ,   9.33333333 , 9.33333333\n",
    "      ,9.33333333 , 9.33333333 ,13.33333333 ,11.33333333 ,14.        ]\n",
    "    ,[29.7029703  ,29.45544554 ,27.97029703 ,27.47524752 ,27.22772277 ,25.24752475\n",
    "     ,24.25742574 ,24.25742574 ,21.78217822 ,19.30693069, 18.56435644]\n",
    "    ,[ 9.42622951 , 9.50819672 ,10.08196721 ,10.24590164  ,9.91803279, 10.08196721\n",
    "     ,10.40983607 ,10.98360656 ,11.39344262, 11.39344262 ,11.14754098]\n",
    "    ,[6.08695652 ,4.34782609 ,4.34782609 ,5.2173913,  6.08695652 ,7.82608696\n",
    "     ,8.69565217 ,6.08695652 ,5.2173913  ,6.95652174 ,7.82608696]\n",
    "    ,[21.27932865 ,20.9796198  ,20.23462922 ,19.36119198 ,18.5305703,  18.1024148\n",
    "     ,17.1775989 , 15.81606439, 14.16338414, 13.23000514 ,12.45076212]]\n",
    "gamma1=np.transpose(gamma2)\n",
    "print(gamma1[0])\n",
    "k=gamma1[0]\n",
    "print(k[3])\n",
    "print(gamma1.shape[0])\n",
    "l=[]\n",
    "l=[[k[0]/100 , k[1]/100]]\n",
    "print(l)\n",
    "t=[k[0]/100 , k[1]/100]\n",
    "print(t[1])\n",
    "t=[k[2]/100 , k[3]/100]\n",
    "print(t[0])\n",
    "#t.append(k[0] , k[1])\n",
    "print(t[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries necessary for this project\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "from time import time\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Import 'GridSearchCV', 'make_scorer', and any other necessary libraries\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Import the three supervised learning models from sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA    \n",
    "\n",
    "# Pretty display for notebooks\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #without accuracy ---> 1\n",
    "# def main(datax,rx):\n",
    "     \n",
    "    \n",
    "#     n=datax.shape[1]\n",
    "#     s=datax.shape[0]\n",
    "        \n",
    "#     r = np.zeros(n, dtype = int) \n",
    "#     data = np.zeros((s, n), dtype = int)\n",
    "#     for i in range(n):\n",
    "#         if int(rx.iloc[i,0])==1 :\n",
    "#             r[i]=1\n",
    "#         else :\n",
    "#             r[i]= 0   \n",
    "#     ar=[]\n",
    "    \n",
    "    \n",
    "#     for j in range(s):\n",
    "#         print(\"sensitive attribute \",(j+1)) \n",
    "#         a=0\n",
    "#         b=0\n",
    "#         acc1=0\n",
    "#         acc2=0\n",
    "#         for i in range(n):\n",
    "                \n",
    "#                 data[j][i]= datax.iloc[j,i]\n",
    "#                 if data[j][i]== 1 :\n",
    "#                     a=a+1\n",
    "#                     if r[i]==1:\n",
    "#                          acc1=acc1+1 \n",
    "\n",
    "#         print(\"total ,fair accepted, aceeptance rate:\")             \n",
    "#         a1=float(acc1/a)\n",
    "\n",
    "#         print(a)\n",
    "#         print(acc1)\n",
    "#         print(a1)\n",
    "#         ar.append(a1)\n",
    "        \n",
    "#     maxi=max(ar)\n",
    "#     mini= min(ar)\n",
    "#     DP=float(maxi-mini)\n",
    "#     print(\"data acceptance rates\")\n",
    "#     print(ar)\n",
    "#     print(\"data DP\")\n",
    "#     print(DP)                       \n",
    "                    \n",
    "#     gama=[.1,.15,.2,.25,.3,.4,.5]\n",
    "#     epsilon=[.0001,.0002,.0003,.0004,.0008,.0009,.001]\n",
    "    \n",
    "#     for gamma in gama:\n",
    "#         for eps in epsilon:\n",
    "#             u1,u2=min_max_lp_all(data,gamma,eps,r)\n",
    "#             #######################Disp_impact#######################  \n",
    "#             print(\"gamma-epsilon\",gamma,eps)\n",
    "#             ar=[]\n",
    "#             for j in range(s):\n",
    "#                 print(\"sensitive attribute \",(j+1)) \n",
    "#                 a=0\n",
    "#                 b=0\n",
    "#                 acc1=0\n",
    "#                 acc2=0\n",
    "#                 for i in range(n):\n",
    "#                         if data[j][i]== 1 :\n",
    "#                             a=a+1\n",
    "#                             if u1[i]==1:\n",
    "#                                  acc1=acc1+1 \n",
    "\n",
    "#                 print(\"total ,fair accepted, aceeptance rate:\")             \n",
    "#                 a1=float(acc1/a)\n",
    "                \n",
    "#                 print(a)\n",
    "#                 print(acc1)\n",
    "#                 print(a1)\n",
    "#                 ar.append(a1)\n",
    "             \n",
    "#             maxi=max(ar)\n",
    "#             mini= min(ar)\n",
    "#             DP=float(maxi-mini)\n",
    "#             print(\"acceptance rates\")\n",
    "#             print(ar)\n",
    "#             print(\"DP\")\n",
    "#             print(DP)\n",
    "\n",
    "#     return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##  (without accuracy)  ## ---> 1\n",
    "# # import time\n",
    "# # import pulp as p \n",
    "# def min_max_lp_all(data1,gamma,eps,r):\n",
    "#     import time\n",
    "#     import pulp as p \n",
    "#     m=data1.shape[0]\n",
    "#     n=data1.shape[1]\n",
    "#     print('dimension of data')\n",
    "#     print(m,n)\n",
    "#     Lp_prob = p.LpProblem('Problem', p.LpMinimize)  \n",
    "   \n",
    "    \n",
    "#     X=np.zeros(n+1,dtype=p.LpVariable)\n",
    "#     sizes=np.zeros(m,dtype=int)\n",
    "#     for i in range(m):\n",
    "#         count=0\n",
    "#         for j in range(n):\n",
    "#             if data1[i][j]==1:\n",
    "#                 count=count+1\n",
    "                \n",
    "#         sizes[i]=count\n",
    "  \n",
    "\n",
    "#     for i in range(n):\n",
    "#         var1=str(i)\n",
    "        \n",
    "#         X[i]=p.LpVariable(var1,lowBound=0,upBound=1,cat='Integer')\n",
    "       \n",
    "        \n",
    "# #     X[n] =  p.LpVariable(\"z1\",lowBound=0)\n",
    "\n",
    "\n",
    "#     #########objective function#####################\n",
    "# #     Lp_prob += X[n]\n",
    "#       Lp_prob+=1\n",
    "\n",
    "#     ##############constraint#################\n",
    "#     for i in range(2*m):\n",
    "#         if i<m:\n",
    "# #             Lp_prob += X[n] >= p.lpSum([2*(X[j]-0.5)*data1[i][j] for j in range(n)])\n",
    "#             Lp_prob += p.lpSum([2*(X[j]-0.5)*data1[i][j] for j in range(n)]) >= (2*gamma-1)*sizes[i]\n",
    "#             Lp_prob += p.lpSum([2*(X[j]-0.5)*data1[i][j] for j in range(n)]) <= ((2*gamma-1)+eps)*sizes[i]\n",
    "            \n",
    "# #         else:        \n",
    "# #             Lp_prob += X[n] >= p.lpSum([-1*2*(X[j]-0.5)*data1[i-m][j] for j in range(n)])\n",
    "            \n",
    "        \n",
    "#     #n is the number of elements in sensitive attribute \n",
    "         \n",
    "# #     Lp_prob += X[n] <= 42000\n",
    "    \n",
    "#     #####################################\n",
    "#     status = Lp_prob.solve()   # Solver \n",
    "#     print(p.LpStatus[status]) \n",
    "#     print(\"discripency is:\")        \n",
    "#     print(p.value(Lp_prob.objective))\n",
    "#     x=np.zeros(n,dtype=float)\n",
    "\n",
    "#    # The solution status \n",
    "#     Synth1={}\n",
    "#     Synth2={}\n",
    "#     # # Printing the final solution \n",
    "#     for i in range(n):\n",
    "#         if(p.value(X[i])==1):\n",
    "#             Synth1[i]=1 \n",
    "#             Synth2[i]=-1\n",
    "#         else:\n",
    "#             Synth1[i]=-1\n",
    "#             Synth2[i]=1\n",
    "#     Synthu1=Synth1  \n",
    "#     Synthu2=Synth2  \n",
    "    \n",
    "              \n",
    "#     return Synthu1,Synthu2   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # without accuracy\n",
    "# import time\n",
    "# import pulp as p \n",
    "# data= pd.read_csv('data/bank_train.csv',skipinitialspace=True)\n",
    "\n",
    "# # print(data.head())\n",
    "# # print(data.shape[0],data.shape[1])\n",
    "\n",
    "# #sensitive columns name 0='age',2='marital'\n",
    "\n",
    "# # print(sens)\n",
    "# r=data[['y']]\n",
    "\n",
    "# sens=data[['age','marital']]\n",
    "# print(sens)\n",
    "# p=sens.shape[0]\n",
    "\n",
    "# # for i in range(0,p):  \n",
    "# #     \n",
    "            \n",
    "# for i in range(0,p):\n",
    "#     if sens.loc[i,'age'] > 60 or sens.loc[i,'age'] < 25 :\n",
    "#                sens.loc[i,'age'] = 1 \n",
    "#     else :\n",
    "#                sens.loc[i,'age'] = 2  \n",
    "#     if r.loc[i,'y'] == 1 :\n",
    "#                r.loc[i,\"y\"] = 1 \n",
    "#     else: \n",
    "#                r.loc[i,\"y\"] = 0 \n",
    "            \n",
    "# sens1 = pd.get_dummies(sens, columns=['age','marital'], prefix =['a','m'])\n",
    "# sensitive = sens1.T\n",
    "# print(sensitive)\n",
    "\n",
    "# # (unique, counts) = numpy.unique(Y_test_pred, return_counts=True)\n",
    "# # frequencies = numpy.asarray((unique, counts)).T\n",
    "# # print(frequencies)\n",
    "# # print(r['y'].value_counts())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# x=main(sensitive, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/subham/anaconda3/envs/tensorflow/lib/python3.6/site-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['time']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "#SVM \n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline\n",
    "from random import *\n",
    "from subprocess import check_output\n",
    "def Bank_svm(X,Y):\n",
    "    #Split data into training and test datasets (training will be based on 70% of data)\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0,shuffle=True) \n",
    "    #test_size: if integer, number of examples into test dataset; if between 0.0 and 1.0, means proportion\n",
    "    print('There are {} samples in the training set and {} samples in the test set'.format(X_train.shape[0], X_test.shape[0]))\n",
    "\n",
    "    \n",
    "    #Scaling data\n",
    "    #from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    from sklearn.model_selection import cross_val_score\n",
    "\n",
    "    #sc = StandardScaler(with_mean=False)\n",
    "    \n",
    "    \n",
    "    #sc.fit(X_train)\n",
    "    #X_train_std = sc.transform(X_train)\n",
    "    #X_test_std = sc.transform(X_test)\n",
    "\n",
    "    #X_train_std and X_test_std are the scaled datasets to be used in algorithms\n",
    "\n",
    "    #Applying SVC (Support Vector Classification)\n",
    "    from sklearn.svm import SVC\n",
    "    svm = SVC(kernel='rbf', random_state=0, gamma=.001, C=1.0,probability=True)\n",
    "    svm.fit(X_train, Y_train)\n",
    "    print('The accuracy of the SVM classifier on training data is {:.2f}'.format(svm.score(X_train, Y_train)))\n",
    "    print('The accuracy of the SVM classifier on test data is {:.2f}'.format(svm.score(X_test, Y_test)))\n",
    "    print('####Train prediction Label###############################################')\n",
    "    Y_train_pred=svm.predict(X_train)\n",
    "    #print(y_1)\n",
    "    Y_test_pred=svm.predict(X_test)\n",
    "\n",
    "    print('####Actual Train Label###############################################')\n",
    "    e=svm.predict_proba(X_test)\n",
    "\n",
    "    print(e)\n",
    "    print(Y_test_pred)\n",
    "    print('####Change to colors###############################################')\n",
    "        \n",
    "    \n",
    "    return X_test,Y_test_pred,Y_test,e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # without accuracy ---> 2\n",
    "# import time\n",
    "# import pulp as p \n",
    "# def min_max_lp_all2(data1,gamma,eps,r,delta):\n",
    "#     import pulp as p \n",
    "    \n",
    "#     m=data1.shape[0]\n",
    "#     n=data1.shape[1]\n",
    "#     print('dimension of data')\n",
    "#     print(m,n)\n",
    "#     Lp_prob = p.LpProblem('Problem', p.LpMinimize)  \n",
    "   \n",
    "    \n",
    "# #     X=np.zeros(n+1,dtype=p.LpVariable)\n",
    "#     X=np.zeros(n,dtype=p.LpVariable)\n",
    "#     sizes=np.zeros(m,dtype=int)\n",
    "#     for i in range(m):\n",
    "#         count=0\n",
    "#         for j in range(n):\n",
    "#             if data1[i][j]==1:\n",
    "#                 count=count+1               \n",
    "#         sizes[i]=count\n",
    "  \n",
    "\n",
    "#     for i in range(n):\n",
    "#         var1=str(i)       \n",
    "#         X[i]=p.LpVariable(var1,lowBound=0,upBound=1,cat='Integer')\n",
    "       \n",
    "        \n",
    "# #     X[n]=  p.LpVariable(\"z1\",lowBound=0)\n",
    "#     #X[n+1]=  p.LpVariable(\"z2\",lowBound=0)\n",
    "\n",
    "\n",
    "#     #########objective function#####################\n",
    "# #     Lp_prob += X[n] \n",
    "#     Lp_prob += 1 \n",
    "\n",
    "\n",
    "#     ##############constraint#################\n",
    "#     for i in range(2*m):\n",
    "#         if i<m:\n",
    "# #             Lp_prob += X[n] >= p.lpSum([2*(X[j]-0.5)*data1[i][j] for j in range(n)])\n",
    "#             Lp_prob += p.lpSum([2*(X[j]-0.5)*data1[i][j] for j in range(n)]) >= (2*gamma-1)*sizes[i]\n",
    "#             Lp_prob += p.lpSum([2*(X[j]-0.5)*data1[i][j] for j in range(n)]) <= ((2*gamma-1)+eps)*sizes[i]\n",
    "            \n",
    "# #         else:        \n",
    "# #             Lp_prob += X[n] >= p.lpSum([-1*2*(X[j]-0.5)*data1[i-m][j] for j in range(n)])\n",
    "#             #Lp_prob += X[n+1] >= p.lpSum([-1*2*(X[j]-0.5)+r[j] for j in range(n)]) \n",
    "# #     Lp_prob += X[n+1] >= p.lpSum([2*(X[j]-0.5)-r[j] for j in range(n)])\n",
    "# #     Lp_prob += X[n+1] >= p.lpSum([-1*2*(X[j]-0.5)+r[j] for j in range(n)])       \n",
    "         \n",
    "#     Lp_prob += p.lpSum([2*(X[i]-0.5)*r[i] for i in range(n)])>=delta*n\n",
    "#     #n is the number of elements in sensitive attribute \n",
    "                 \n",
    "# #     Lp_prob += X[n] <= 42000\n",
    "    \n",
    "#     #####################################\n",
    "#     status = Lp_prob.solve()   # Solver \n",
    "#     print(p.LpStatus[status]) \n",
    "#     print(\"discripency is:\")        \n",
    "#     print(p.value(Lp_prob.objective))\n",
    "\n",
    "#     x=np.zeros(n,dtype=float)\n",
    "#    # The solution status \n",
    "#     Synth1={}\n",
    "#     Synth2={}\n",
    "#     # # Printing the final solution \n",
    "#     for i in range(n):\n",
    "#         if(p.value(X[i])==1):\n",
    "#             Synth1[i]=1 \n",
    "#             Synth2[i]=-1\n",
    "#         else:\n",
    "#             Synth1[i]=-1\n",
    "#             Synth2[i]=1\n",
    "#     Synthu1=Synth1  \n",
    "#     Synthu2=Synth2               \n",
    "#     return Synthu1,Synthu2   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#without accuracy ---> 2\n",
    "def main(datax, y_test, y_test_pred,e): \n",
    "        \n",
    "    n=datax.shape[1]\n",
    "    s=datax.shape[0]    \n",
    "    data = datax\n",
    "    \n",
    "    r = np.zeros(n, dtype = int) \n",
    "    \n",
    "    for i in range(n):\n",
    "        if int(y_test.iloc[i])==1 :\n",
    "            r[i]=1\n",
    "        else :\n",
    "            r[i]= -1  \n",
    "    \n",
    "    r2 = np.zeros(n, dtype = int) \n",
    "    for i in range(n):\n",
    "        if int(y_test_pred[i])==1 :\n",
    "            r2[i]=1\n",
    "        else :\n",
    "            r2[i]= -1          \n",
    "    ar=[]\n",
    "    \n",
    "    for j in range(s):\n",
    "        print(\"sensitive attribute \",(j+1)) \n",
    "        a=0\n",
    "        b=0\n",
    "        acc1=0\n",
    "        acc2=0\n",
    "        for i in range(n):\n",
    "                if data[j][i]== 1 :\n",
    "                    a=a+1\n",
    "                    if r[i]==1:\n",
    "                         acc1=acc1+1 \n",
    "\n",
    "        print(\"ACTUAL----------total ,accepted, aceeptance rate:\")             \n",
    "        a1=float(acc1/a)\n",
    "        print(a)\n",
    "        \n",
    "        print(acc1)\n",
    "        print(a1)\n",
    "        ar.append(a1)\n",
    "        \n",
    "    maxi= max(ar)\n",
    "    mini= min(ar)\n",
    "    DP=float(maxi-mini)\n",
    "    print(\"data acceptance rates\")\n",
    "    print(ar)\n",
    "    print(\"data DP\")\n",
    "    print(DP)\n",
    "    \n",
    "    ar=[]\n",
    "    \n",
    "    for j in range(s):\n",
    "        print(\"sensitive attribute \",(j+1)) \n",
    "        a=0\n",
    "        b=0\n",
    "        acc1=0\n",
    "        acc2=0\n",
    "        prec=0\n",
    "        reca=0\n",
    "        accur=0\n",
    "        FP=0\n",
    "        FN=0\n",
    "        TP=0\n",
    "        TN=0\n",
    "        for i in range(n):\n",
    "             if data[j][i]== 1 :\n",
    "                    a=a+1\n",
    "                    if r2[i]==1:\n",
    "                        acc1=acc1+1 \n",
    "                        if r[i]==1:\n",
    "                            TP=TP+1\n",
    "                        else:\n",
    "                             FP=FP+1                \n",
    "                    else:\n",
    "                        if r[i]==1:\n",
    "                            FN=FN+1\n",
    "                        else:\n",
    "                            TN=TN+1    \n",
    "        \n",
    "        print(\"prec reca accuracy for each sens\") \n",
    "        prec= float(TP/(TP+FP))\n",
    "        reca= float(TP/(TP+FN))\n",
    "        accur= float((TP+TN)/a)\n",
    "        print(prec,reca,accur)\n",
    "        \n",
    "        \n",
    "        print(\"SVM----------total , accepted, aceeptance rate:\")             \n",
    "        \n",
    "        a1=float(acc1/a)\n",
    "        print(a)\n",
    "        \n",
    "        print(acc1)\n",
    "        print(a1)\n",
    "        ar.append(a1)\n",
    "        \n",
    "    maxi= max(ar)\n",
    "    mini= min(ar)\n",
    "    DP=float(maxi-mini)\n",
    "    print(\"data acceptance rates\")\n",
    "    print(ar)\n",
    "    print(\"data DP\")\n",
    "    print(DP) \n",
    "    \n",
    "    print(\"SVM accuracy--------------------------\")\n",
    "    prec=0\n",
    "    reca=0\n",
    "    accur=0\n",
    "    FP=0\n",
    "    FN=0\n",
    "    TP=0\n",
    "    TN=0\n",
    "    for i in range(n):\n",
    "            if r2[i]==1:\n",
    "                acc1=acc1+1 \n",
    "                if r[i]==1:\n",
    "                    TP=TP+1\n",
    "                else:\n",
    "                     FP=FP+1                \n",
    "            else:\n",
    "                if r[i]==1:\n",
    "                     FN=FN+1\n",
    "                else:\n",
    "                     TN=TN+1    \n",
    "        \n",
    "    prec= float(TP/(TP+FP))\n",
    "    reca= float(TP/(TP+FN))\n",
    "    accur= float((TP+TN)/n)\n",
    "    print(prec,reca,accur)\n",
    "    \n",
    "\n",
    "    \n",
    "#   delta=1\n",
    "#     gama=[.05,.1,.15,.2,.25]\n",
    "#     epsilon=[.01,.02,.05,.1,.15,.20,.25,.30,.35,.40,.50]\n",
    "\n",
    "#ADULT ZAFAR =? epsilon=[0.088 ,0.1656, 0.168,  0.211, 0.251 ] \n",
    " \n",
    "#agarwal=> epsilon=[ 0.071, 0.1271, 0.2437, 0.27 ]\n",
    " \n",
    "\n",
    "    #gama=[0.0869, 0.0521,0.0782, 0.0608,0.0434, 0.1,0.069,0.0434,0.034]\n",
    "    epsilon=[.01]\n",
    "    fi= np.zeros(n,dtype=int) \n",
    "#     for delta in delta1:\n",
    "    #4 gamma=[0.175442,    0.142103, 0.166039,    0.164754,  0.153465,    0.14,  0.104348   ]\n",
    "\n",
    "    #1 gamma=[0.259147,   0.0730028, 0.210139, 0.0893443, 0.306931, 0.0933333,  0.0347826]\n",
    "    #gamma=[0.196178,0.126722,   0.179654, 0.140164,     0.153465,   0.133333,  0.0695652]\n",
    "   \n",
    "    \n",
    "    #agar\n",
    "    # gamma2=[[0.175442], [0.142103],[0.166039],[0.164754],[0.153465],[0.14],[0.104348]]\n",
    "    \n",
    "    #gamma2=[[0.175442], [0.142103],[0.166039],[0.164754],[0.153465],[0.14],[0.104348]]\n",
    "    #bilal 1 15 19\n",
    "    gamma=[ [0.08403361344537816, 0.04844290657439446, 0.05546448087431694, 0.061053984575835475, 0.03700906344410876, 0.07081174438687392, 0.054763117677024964, 0.05636411749139984, 0.05998235812996178, 0.04072727272727273],\n",
    "[0.1092436974789916, 0.06228373702422145, 0.055327868852459015, 0.06298200514138817, 0.03323262839879154, 0.08808290155440414, 0.054763117677024964, 0.05702566816618153, 0.06292266980299911, 0.03709090909090909],\n",
    "[0.12605042016806722, 0.06920415224913495, 0.05491803278688524, 0.06491002570694088, 0.0324773413897281, 0.09844559585492228, 0.05493292579385295, 0.05715797830113787, 0.065274919141429, 0.03636363636363636],\n",
    "[0.18067226890756302, 0.12110726643598616, 0.05259562841530055, 0.06908740359897173, 0.030211480362537766, 0.14853195164075994, 0.054338597384955, 0.05662873776131252, 0.07350779182593355, 0.03490909090909091]]\n",
    "    \n",
    "    alpha=[[1,1,1,1,1,1,1,1,1,1]]\n",
    "    '''\n",
    "    alpha=[[1,.1,1,1,1,1],[1,1,1,1,.01,1],[1,1,1,1,1,.001]] \n",
    "    gamma2=[[0.2970639033,0.2158894646,.127806563,.0967184801,.1070811744],\n",
    "[0.05815928,0.0556970623,0.0562913907,0.0560366786,.0578196638],\n",
    "[0.0631119344,0.0599364911,0.0571579783,0.0553056364,.0571579783],\n",
    "[0.0911496619,0.0782122905,0.072037636,0.0682152308,.0743898853],\n",
    "[0.0501818182,.0443636364 ,.0429090909,.0472727273,.0414545455],\n",
    "[0.0434782609,0.0434782609,0.0434782609,.0434782609,.0434782609]\n",
    "]\n",
    "    gamma=np.transpose(gamma2) \n",
    "    '''\n",
    "    '''\n",
    "    #LP5 case beta .074\n",
    "    \n",
    "    alpha=[[1,1,1,1,1,1],[1,1,1,2,1,1],[1,1,1,4,1,1],[1,1,1,6,1,1],[1,1,1,8,1,1],[1,1,1,10,1,1]]\n",
    "    gamma=[[0.2970639032815199, 0.06707420614705382, 0.07105054247155332, 0.10202881505439576, 0.056, 0.043478260869565216],\n",
    "[0.25734024179620035, 0.06911190354898965, 0.0723736438211167, 0.09732431637753602, 0.06036363636363636, 0.08695652173913043],\n",
    "[0.21243523316062177, 0.07123450500933945, 0.07369674517068008, 0.09232578653337253, 0.06472727272727273, 0.08695652173913043],\n",
    "[0.16753022452504318, 0.07327220241127526, 0.07488753638528711, 0.08732725668920906, 0.06909090909090909, 0.08695652173913043],\n",
    "[0.12435233160621761, 0.07539480387162506, 0.07621063773485048, 0.0826227580123493, 0.07345454545454545, 0.08695652173913043],\n",
    "[0.08635578583765112, 0.07751740533197486, 0.07753373908441387, 0.07880035283740076, 0.07781818181818181, 0.08695652173913043]]\n",
    "    '''\n",
    "    '''\n",
    "    alpha=[[1,1,1,1,1,1], [1,1,1,10,1,1], [1,1,1,100,1,1],[1,1,1,100,10,1],[1,1,1,1000,10,1], [1,1,100,100,10,1],[1,1,10,1000,100,1],[1,1,1,100,10,1], [1,1,1,10,10,1],[1,1,1,100,100,1],\n",
    "           [1,1,1,1,1000,1], [1,1,10,1000,10,1],[1,1,10,10,10,1]   ]\n",
    "    #alpha=[[1,1,1,1,1,1],[1,1,1,10,1,1], [1,1,1,100,1,1],[1,1,1,100,10,1],[1,1,1,1000,10,1]]\n",
    "    #gamm=.12\n",
    "    gamma=[[0.2970639032815199, 0.06707420614705382, 0.07105054247155332, 0.10202881505439576, 0.056, 0.043478260869565216],\n",
    "[0.2659758203799655, 0.07760230939038885, 0.08084149245832231, 0.10585122022934432, 0.06909090909090909, 0.08695652173913043],\n",
    "[0.229706390328152, 0.08821531669213789, 0.09063244244509129, 0.10937959423698912, 0.08218181818181818, 0.08695652173913043],\n",
    "[0.19343696027633853, 0.09882832399388691, 0.10042339243186028, 0.11290796824463394, 0.09454545454545454, 0.13043478260869565],\n",
    "[0.15889464594127806, 0.10944133129563594, 0.11021434241862926, 0.11673037341958248, 0.10763636363636364, 0.13043478260869565],\n",
    "[0.13989637305699482, 0.12005433859738496, 0.12000529240539826, 0.12349309026756836, 0.12, 0.13043478260869565]]\n",
    "    '''\n",
    "    #alpha=[[1,1,1,1,1,1],[.001,.001,1,1,1,1],[.001,.01,1,1,1,1],[.001,.1,1,1,1,1],[.01,.1,1,1,1,1]]\n",
    "    \n",
    "#     gamma=[[0.30355010313755293, 0.10743801652892562, 0.252269224182223, 0.13278688524590163, 0.3118811881188119, 0.1, 0.13043478260869565]\n",
    "# ]\n",
    "    '''\n",
    "    alpha=[[1,1,1,1,1,1],[.1,1,1,1,1,1], [.01,1,1,1,1,1],[.001,1,1,1,1,1],\n",
    "           [1,.1,1,1,1,1], [1,.01,1,1,1,1],[1,.001,1,1,1,1],\n",
    "           [1,1,.1,1,1,1], [1,1,.01,1,1,1],[1,1,.001,1,1,1],\n",
    "           [1,1,1,.1,1,1], [1,1,1,.01,1,1],[1,1,1,.001,1,1],\n",
    "           [1,1,1,1,.1,1], [1,1,1,1,.01,1],[1,1,1,1,.001,1],\n",
    "           [1,1,1,1,1,.1], [1,1,1,1,1,.01],[1,1,1,1,1,.001],\n",
    "        [.1,.1,1,1,1,1],[.01,.01,1,1,1,1], [.001,.001,1,1,1,1],[.001,.01,1,1,1,1],[.001,.1,1,1,1,1],\n",
    "         [.01,.1,1,1,1,1] ]\n",
    "    '''\n",
    "    '''     \n",
    "    alpha=[[1,1,1,1,1,1],[1,1,1,2,1,1],[1,1,1,4,1,1],[1,1,1,6,1,1],[1,1,1,8,1,1],[1,1,1,10,1,1]]\n",
    "    gamma=[ [0.18825561312607944, 0.05306503650874512, 0.055570256681661816, 0.0764481034989709, 0.03854545454545454, 0.043478260869565216],\n",
    "[0.09844559585492228, 0.04202750891492613, 0.04247155332098439, 0.055865921787709494, 0.029818181818181817, 0.001],\n",
    "[0.0690846286701209, 0.0554423501443369, 0.05609949722148717, 0.06233460746839165, 0.04145454545454545, 0.001],\n",
    "[0.025906735751295335, 0.019697741552046188, 0.01905265943371262, 0.022640399882387533, 0.01890909090909091, 0.001]]\n",
    "    '''\n",
    "           \n",
    "           #alpha=[90,40,100,10,4,1,10]\n",
    "    #alpha=[ 9211,  4356, 11678,  1220,   404,   150 ,  115]\n",
    "    a=0\n",
    "    print(alpha)\n",
    "    \n",
    "    '''\n",
    "    #agarwal 1 15 25\n",
    "     alpha=[[1,1,1,1,1,1],[1,1,1,1,.01,1],  [.01,.1,1,1,1,1] ] \n",
    "    gamma2=   [[ 0.074266,0.0725389,0.0794473,0.0777202,0.0673575],\n",
    "[0.051367,0.051367,0.0517915,0.0524707,0.0533197],\n",
    "[0.0497486,0.0497486,0.0493517,0.0501455,0.0504102],\n",
    "[0.0629227,0.0649809,0.0729197,0.0738018,0.0743899],\n",
    "[0.0407273,0.0349091,0.0254545,0.024,0.024],\n",
    "[0.0869565,0.0869565,0,0,0]]\n",
    "    \n",
    "                \n",
    "    gamma=np.transpose(gamma2)\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    #for t in range(gamma.shape[0]):\n",
    "    for new in range(4):\n",
    "        for t in range(1):\n",
    "            for eps in epsilon:\n",
    "                u1,u2=min_sum_lpca_g(data,gamma[new],eps,e,alpha[t])\n",
    "                #######################Disp_impact#######################  \n",
    "                print(\"gamma-epsilon-delta\",gamma[new],eps)\n",
    "                accu_all=[]\n",
    "                DP_all=[]\n",
    "                precision_all=[]\n",
    "                recall_all=[]\n",
    "                ar_all=[]\n",
    "                acceptance_rate=np.zeros((7,28),dtype=float)\n",
    "                count=0\n",
    "                print(\"<--------------------------------------->\")\n",
    "                print(\"iteration t\",t)\n",
    "        #                 for alpha in np.arange(0,1.05,0.05):\n",
    "        #                     print(\"alpha: \",alpha)\n",
    "        #                     for i in range(n):\n",
    "\n",
    "        #                         z=random()\n",
    "        #                         if z < alpha:\n",
    "        #                                fi[i]= u1[i] \n",
    "\n",
    "        #                         else:\n",
    "        #                                fi[i]= r2[i]\n",
    "\n",
    "                for i in range(n):\n",
    "                     fi[i] = u1[i]\n",
    "\n",
    "\n",
    "                for j in range(s):\n",
    "                    print(\"sensitive attribute \",(j+1)) \n",
    "\n",
    "                    TP=0\n",
    "                    FP=0\n",
    "                    FN=0\n",
    "                    TN=0\n",
    "                    precision=0\n",
    "                    recall=0\n",
    "                    for i in range(n):\n",
    "                         if data[j][i]== 1 :                        \n",
    "                            if fi[i]==1 and r[i]==1:\n",
    "                                TP=TP+1\n",
    "                            if fi[i]==1 and r[i]==-1:\n",
    "                                FP=FP+1 \n",
    "                            if fi[i]==-1 and r[i]==1:\n",
    "                                FN=FN+1\n",
    "                            if fi[i]==-1 and r[i]==-1:\n",
    "                                TN=TN+1    \n",
    "                    if TP+FP !=0:\n",
    "                        precision=float(TP/(TP+FP))\n",
    "                    #print(\"precision\",precision)\n",
    "                    if TP+FN !=0:    \n",
    "                        recall=float(TP/(TP+FN))\n",
    "                    #print(\"recall\",recall)\n",
    "\n",
    "                    precision_all.append(precision)\n",
    "                    recall_all.append(recall)\n",
    "                    #print(\"TP,FP,TN,FN\")\n",
    "                    #print(TP,FP,TN,FN)\n",
    "\n",
    "                    a=0\n",
    "                    b=0\n",
    "                    acc1=0\n",
    "                    acc2=0\n",
    "                    for i in range(n):\n",
    "                            if data[j][i]== 1 :\n",
    "                                a=a+1\n",
    "                                if fi[i]==1:\n",
    "                                     acc1=acc1+1 \n",
    "\n",
    "        #                         print(\"total ,fair accepted, aceeptance rate:\")             \n",
    "                    a1=float(acc1/a)\n",
    "\n",
    "\n",
    "\n",
    "        #                         print(a)\n",
    "        #                         print(acc1)\n",
    "        #                         print(a1)\n",
    "                    ar_all.append(a1)\n",
    "\n",
    "                count = count+1\n",
    "                maxi=max(ar_all)\n",
    "                mini= min(ar_all)\n",
    "                DP=float(maxi-mini)\n",
    "                print(\"individual acceptance rates\")\n",
    "                print(ar_all)\n",
    "                print(\"individul precision\")\n",
    "                print(precision_all)\n",
    "                print(\"individual recall\")\n",
    "                print(recall_all)\n",
    "                print(\"DP all\")\n",
    "                print(DP)\n",
    "                f_acc=0\n",
    "                for i in range(n):\n",
    "                     if fi[i] == r[i]:\n",
    "                            f_acc=f_acc+1\n",
    "                f_acc_l=float((f_acc*100)/n) \n",
    "\n",
    "        #######################################################################33   \n",
    "\n",
    "        #                         print(\"sensitive attribute \",(j+1)) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                TP=0\n",
    "                FP=0\n",
    "                FN=0\n",
    "                TN=0\n",
    "                precision=0\n",
    "                recall=0\n",
    "                accu=0\n",
    "                for i in range(n):\n",
    "                        if fi[i]==1 and r[i]==1:\n",
    "                            TP=TP+1\n",
    "                        if fi[i]==1 and r[i]==-1:\n",
    "                            FP=FP+1 \n",
    "                        if fi[i]==-1 and r[i]==1:\n",
    "                            FN=FN+1\n",
    "                        if fi[i]==-1 and r[i]==-1:\n",
    "                            TN=TN+1    \n",
    "\n",
    "                if TP+FP!=0:\n",
    "                    precision=float(TP/(TP+FP))\n",
    "                print(\"precision all\",precision)\n",
    "                if TP+FN!=0:\n",
    "                    recall=float(TP/(TP+FN))\n",
    "\n",
    "\n",
    "                print(\"recall all\",recall)\n",
    "                accu=float((TP+TN)/(TP+FN+TN+FP))\n",
    "\n",
    "\n",
    "                print(\"accuracy all\",accu)\n",
    "\n",
    "\n",
    "\n",
    "                print(\"TP,FP,TN,FN\")\n",
    "                print(TP,FP,TN,FN)\n",
    "        #                         print(\"total ,fair accepted, aceeptance rate:\")             \n",
    "                a1=float(acc1/a)\n",
    "\n",
    "\n",
    "    print(\"<--------------------------------------->\")\n",
    "    alpha_weight=np.arange(0,1.05,.05)        \n",
    "    return accu_all,DP_all,acceptance_rate,alpha_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#NG\n",
    "import time\n",
    "import pulp as p \n",
    "def min_sum_lpca_g(data1,beta,eps,e,alpha):\n",
    "    import pulp as p \n",
    "    import math\n",
    "    \n",
    "    m=data1.shape[0]\n",
    "    n=data1.shape[1]\n",
    "    print('dimension of data')\n",
    "    print(m,n)\n",
    "    \n",
    "    ################ sorted result\n",
    "  \n",
    "    h1=[]\n",
    "    h2=[]\n",
    "    h3=[]\n",
    "    h4=[]\n",
    "    h5=[]\n",
    "    h6=[]\n",
    "    h7=[]\n",
    "    key1=[]\n",
    "    key2=[]\n",
    "    key3=[]\n",
    "    key4=[]\n",
    "    key5=[]\n",
    "    key6=[]\n",
    "    key7=[]\n",
    "    h8=[]\n",
    "    h9=[]\n",
    "    h10=[]\n",
    "    h11=[]\n",
    "    h12=[]\n",
    "    h13=[]\n",
    "    h14=[]\n",
    "    key8=[]\n",
    "    key9=[]\n",
    "    key10=[]\n",
    "    key11=[]\n",
    "    key12=[]\n",
    "    key13=[]\n",
    "    key14=[]\n",
    "    cost=np.zeros(n,dtype=int)\n",
    "    data2=np.zeros((m,n),dtype=int)\n",
    "    for i in range(n):\n",
    "        if data1[0][i]==1:            \n",
    "\n",
    "            h1.append(e[i][1])\n",
    "            key1.append(i)\n",
    "            \n",
    "\n",
    "        if data1[1][i]==1:\n",
    "            h2.append(e[i][1])\n",
    "            key2.append(i)\n",
    "            \n",
    "            \n",
    "        if data1[2][i]==1:\n",
    "            h3.append(e[i][1])\n",
    "            key3.append(i)\n",
    "            \n",
    "        if data1[3][i]==1:\n",
    "            h4.append(e[i][1])\n",
    "            key4.append(i)\n",
    "        if data1[4][i]==1:\n",
    "            h5.append(e[i][1])\n",
    "            key5.append(i)\n",
    "        if data1[5][i]==1:\n",
    "            h6.append(e[i][1])\n",
    "            key6.append(i)\n",
    "        if data1[6][i]==1:\n",
    "            h7.append(e[i][1])\n",
    "            key7.append(i)\n",
    "        if data1[7][i]==1:            \n",
    "            h8.append(e[i][1])\n",
    "            key8.append(i)\n",
    "        if data1[8][i]==1:\n",
    "            h9.append(e[i][1])\n",
    "            key9.append(i)\n",
    "            \n",
    "            \n",
    "        if data1[9][i]==1:\n",
    "            h10.append(e[i][1])\n",
    "            key10.append(i)\n",
    "       \n",
    "        \n",
    "#print(hc)\n",
    "#     print(key1)\n",
    "    \n",
    "    for i in range(1,len(h1)):\n",
    "        for j in range(i,0,-1):\n",
    "            var=0\n",
    "            var2=0\n",
    "            if h1[j-1]<h1[j]:\n",
    "                index=j\n",
    "                var=h1[j]\n",
    "                h1[j]=h1[j-1]\n",
    "                h1[j-1]=var\n",
    "\n",
    "                var2=key1[j]\n",
    "                key1[j]=key1[j-1]\n",
    "                key1[j-1]=var2\n",
    "            else:\n",
    "                break\n",
    "    \n",
    "\n",
    "    for i in range(1,len(h2)):\n",
    "        for j in range(i,0,-1):\n",
    "            var=0\n",
    "            var2=0\n",
    "            if h2[j-1]<h2[j]:\n",
    "                index=j\n",
    "                var=h2[j]\n",
    "                h2[j]=h2[j-1]\n",
    "                h2[j-1]=var\n",
    "\n",
    "                var2=key2[j]\n",
    "                key2[j]=key2[j-1]\n",
    "                key2[j-1]=var2\n",
    "            else:\n",
    "                break\n",
    "    for i in range(1,len(h3)):\n",
    "        for j in range(i,0,-1):\n",
    "            var=0\n",
    "            var2=0\n",
    "            if h3[j-1]<h3[j]:\n",
    "                index=j\n",
    "                var=h3[j]\n",
    "                h3[j]=h3[j-1]\n",
    "                h3[j-1]=var\n",
    "\n",
    "                var2=key3[j]\n",
    "                key3[j]=key3[j-1]\n",
    "                key3[j-1]=var2\n",
    "            else:\n",
    "                break\n",
    "    for i in range(1,len(h4)):\n",
    "        for j in range(i,0,-1):\n",
    "            var=0\n",
    "            var2=0\n",
    "            if h4[j-1]<h4[j]:\n",
    "                index=j\n",
    "                var=h4[j]\n",
    "                h4[j]=h4[j-1]\n",
    "                h4[j-1]=var\n",
    "\n",
    "                var2=key4[j]\n",
    "                key4[j]=key4[j-1]\n",
    "                key4[j-1]=var2\n",
    "            else:\n",
    "                break\n",
    "    for i in range(1,len(h5)):\n",
    "        for j in range(i,0,-1):\n",
    "            var=0\n",
    "            var2=0\n",
    "            if h5[j-1]<h5[j]:\n",
    "                index=j\n",
    "                var=h5[j]\n",
    "                h5[j]=h5[j-1]\n",
    "                h5[j-1]=var\n",
    "\n",
    "                var2=key5[j]\n",
    "                key5[j]=key5[j-1]\n",
    "                key5[j-1]=var2\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "                \n",
    "                \n",
    "    for i in range(1,len(h6)):\n",
    "        for j in range(i,0,-1):\n",
    "            var=0\n",
    "            var2=0\n",
    "            if h6[j-1]<h6[j]:\n",
    "                index=j\n",
    "                var=h6[j]\n",
    "                h6[j]=h6[j-1]\n",
    "                h6[j-1]=var\n",
    "\n",
    "                var2=key6[j]\n",
    "                key6[j]=key6[j-1]\n",
    "                key6[j-1]=var2\n",
    "            else:\n",
    "                break        \n",
    "                \n",
    "\n",
    "    for i in range(1,len(h7)):\n",
    "        for j in range(i,0,-1):\n",
    "            var=0\n",
    "            var2=0\n",
    "            if h7[j-1]<h7[j]:\n",
    "                index=j\n",
    "                var=h7[j]\n",
    "                h7[j]=h7[j-1]\n",
    "                h7[j-1]=var\n",
    "\n",
    "                var2=key7[j]\n",
    "                key7[j]=key7[j-1]\n",
    "                key7[j-1]=var2\n",
    "            else:\n",
    "                break \n",
    "    ############################################            \n",
    "    for i in range(1,len(h8)):\n",
    "        for j in range(i,0,-1):\n",
    "            var=0\n",
    "            var2=0\n",
    "            if h8[j-1]<h8[j]:\n",
    "                index=j\n",
    "                var=h8[j]\n",
    "                h8[j]=h8[j-1]\n",
    "                h8[j-1]=var\n",
    "\n",
    "                var2=key8[j]\n",
    "                key8[j]=key8[j-1]\n",
    "                key8[j-1]=var2\n",
    "            else:\n",
    "                break\n",
    "    \n",
    "\n",
    "    for i in range(1,len(h9)):\n",
    "        for j in range(i,0,-1):\n",
    "            var=0\n",
    "            var2=0\n",
    "            if h9[j-1]<h9[j]:\n",
    "                index=j\n",
    "                var=h9[j]\n",
    "                h9[j]=h9[j-1]\n",
    "                h9[j-1]=var\n",
    "\n",
    "                var2=key9[j]\n",
    "                key9[j]=key9[j-1]\n",
    "                key9[j-1]=var2\n",
    "            else:\n",
    "                break\n",
    "    for i in range(1,len(h10)):\n",
    "        for j in range(i,0,-1):\n",
    "            var=0\n",
    "            var2=0\n",
    "            if h10[j-1]<h10[j]:\n",
    "                index=j\n",
    "                var=h10[j]\n",
    "                h10[j]=h10[j-1]\n",
    "                h10[j-1]=var\n",
    "\n",
    "                var2=key10[j]\n",
    "                key10[j]=key10[j-1]\n",
    "                key10[j-1]=var2\n",
    "            else:\n",
    "                break\n",
    "       \n",
    "                \n",
    "\n",
    "    \n",
    "    '''            \n",
    "    \n",
    "    for j in range(len(key1)):    \n",
    "        if h1[j]==h1[j-1] and j>=1:\n",
    "            data2[0][key1[j]]=data2[0][key1[j-1]]\n",
    "        else:    \n",
    "            data2[0][key1[j]]=j+1\n",
    "    for j in range(len(key2)):\n",
    "        if h2[j]==h2[j-1] and j>=1:\n",
    "            data2[1][key2[j]]=data2[0][key2[j-1]]\n",
    "        else:    \n",
    "            data2[1][key2[j]]=j+1\n",
    "    for j in range(len(key3)):\n",
    "        if h3[j]==h3[j-1] and j>=1:\n",
    "            data2[2][key3[j]]=data2[2][key3[j-1]]\n",
    "        else:    \n",
    "            data2[2][key3[j]]=j+1\n",
    "    for j in range(len(key4)):\n",
    "        if h4[j]==h4[j-1] and j>=1:\n",
    "            data2[3][key4[j]]=data2[3][key4[j-1]]\n",
    "        else:    \n",
    "            data2[3][key4[j]]=j+1\n",
    "    for j in range(len(key5)):\n",
    "        if h5[j]==h5[j-1] and j>=1:\n",
    "            data2[4][key5[j]]=data2[4][key5[j-1]]\n",
    "        else:    \n",
    "            data2[4][key5[j]]=j+1\n",
    "    for j in range(len(key6)):\n",
    "        if h6[j]==h6[j-1] and j>=1:\n",
    "            data2[5][key6[j]]=data2[5][key6[j-1]]\n",
    "        else:    \n",
    "            data2[5][key6[j]]=j+1\n",
    "    for j in range(len(key7)):\n",
    "        if h7[j]==h7[j-1] and j>=1:\n",
    "            data2[6][key7[j]]=data2[6][key7[j-1]]\n",
    "        else:    \n",
    "            data2[6][key7[j]]=j+1 \n",
    "    \n",
    "  ###############################1#################################  \n",
    "    #2nd approach\n",
    "    for j in range(len(key1)):    \n",
    "        #data2[0][key1[j]]=((j+1)/((beta[0]*len(key1))*((beta[0]*len(key1))+1)/2)*alpha[0])\n",
    "         \n",
    "        data2[0][key1[j]]=(j+1)*alpha[0]\n",
    "    for j in range(len(key2)):\n",
    "        data2[1][key2[j]]=(j+1)*alpha[1]\n",
    "    for j in range(len(key3)):\n",
    "        if data1[2][key3[j]]==1 and data1[0][key3[j]]==1: \n",
    "            data2[2][key3[j]]=(j+1)*(len(key1)/len(key3))*alpha[2]\n",
    "        else:\n",
    "            data2[2][key3[j]]=(j+1)*(len(key2)/len(key3))*alpha[2]                  \n",
    "        \n",
    "    for j in range(len(key4)):\n",
    "        if data1[3][key4[j]]==1 and data1[0][key4[j]]==1:                   \n",
    "            data2[3][key4[j]]=(j+1)*(len(key1)/len(key4))*alpha[3]\n",
    "        else :                     \n",
    "            data2[3][key4[j]]=(j+1)*(len(key2)/len(key4))*alpha[3]\n",
    "                             \n",
    "    for j in range(len(key5)):\n",
    "        if data1[4][key5[j]]==1 and data1[0][key5[j]]==1:                  \n",
    "            data2[4][key5[j]]=(j+1)*(len(key1)/len(key5))*alpha[4]\n",
    "        else:      \n",
    "            data2[4][key5[j]]=(j+1)*(len(key2)/len(key5))*alpha[4]\n",
    "    for j in range(len(key6)):\n",
    "        if data1[5][key6[j]]==1 and data1[0][key6[j]]==1:                    \n",
    "            data2[5][key6[j]]=(j+1)*(len(key1)/len(key6))*alpha[5]\n",
    "        else:                    \n",
    "             data2[5][key6[j]]=(j+1)*(len(key2)/len(key6))*alpha[5]               \n",
    "    for j in range(len(key7)):\n",
    "        if data1[6][key7[j]]==1 and data1[0][key7[j]]==1:\n",
    "            data2[6][key7[j]]=(j+1)*(len(key1)/len(key7))*alpha[6]\n",
    "        else:\n",
    "             data2[6][key7[j]]=(j+1)*(len(key2)/len(key7))*alpha[6]\n",
    "    '''\n",
    "    '''\n",
    "    \n",
    "    #1st approach\n",
    "    for j in range(len(key1)):    \n",
    "        #data2[0][key1[j]]=((j+1)/((beta[0]*len(key1))*((beta[0]*len(key1))+1)/2)*alpha[0])\n",
    "         \n",
    "        data2[0][key1[j]]=(j+1)*alpha[0]\n",
    "    for j in range(len(key2)):\n",
    "        data2[1][key2[j]]=(j+1)*alpha[1]\n",
    "    for j in range(len(key3)):\n",
    "        data2[2][key3[j]]=(j+1)*alpha[2]              \n",
    "        \n",
    "    for j in range(len(key4)):\n",
    "        data2[3][key4[j]]=(j+1)*alpha[3]\n",
    "        \n",
    "                             \n",
    "    for j in range(len(key5)):               \n",
    "        data2[4][key5[j]]=(j+1)*alpha[4]\n",
    "       \n",
    "    for j in range(len(key6)):\n",
    "        data2[5][key6[j]]=(j+1)*alpha[5]\n",
    "                    \n",
    "    for j in range(len(key7)):\n",
    "        data2[6][key7[j]]=(j+1)*alpha[6]\n",
    "    '''   \n",
    "    '''\n",
    "    for j in range(len(key1)):    \n",
    "        #data2[0][key1[j]]=((j+1)/((beta[0]*len(key1))*((beta[0]*len(key1))+1)/2)*alpha[0])\n",
    "         \n",
    "        data2[0][key1[j]]=(j+1)*alpha[0]\n",
    "    for j in range(len(key2)):\n",
    "        data2[1][key2[j]]=(j+1)*((beta[0]*len(key1))/(beta[1]*len(key2)))*alpha[1]\n",
    "    for j in range(len(key3)):\n",
    "        data2[2][key3[j]]=(j+1)*((beta[2]*len(key3))/(beta[2]*len(key3)))*alpha[2]\n",
    "                         \n",
    "        \n",
    "    for j in range(len(key4)):           \n",
    "        #data2[3][key4[j]]=(j+1)*((beta[2]*len(key3))/(beta[3]*len(key4)))*alpha[3]\n",
    "        if data1[3][key4[j]]==1 and data1[0][key4[j]]==1:                   \n",
    "            data2[3][key4[j]]=(j+1)*((beta[2]*len(key3))/(beta[3]*len(key4)))*alpha[3]\n",
    "        else :                     \n",
    "            data2[3][key4[j]]=(j+1)*((beta[2]*len(key3))/(beta[3]*len(key4)))*alpha[3]\n",
    "                             \n",
    "    for j in range(len(key5)):\n",
    "        data2[4][key5[j]]=(j+1)*((beta[2]*len(key3))/(beta[4]*len(key5)))*alpha[4]      \n",
    "    for j in range(len(key6)):                 \n",
    "        data2[5][key6[j]]=(j+1)*((beta[2]*len(key3))/(beta[5]*len(key6)))*alpha[5]  \n",
    "    for j in range(len(key7)):                 \n",
    "        data2[6][key7[j]]=(j+1)*((beta[2]*len(key3))/(beta[6]*len(key7)))*alpha[6] \n",
    "    '''\n",
    "    #######################################################################    \n",
    "    \n",
    "    ####################################################################### \n",
    "   \n",
    "    for j in range(len(key1)):    \n",
    "        data2[0][key1[j]]=(j+1)\n",
    "    for j in range(len(key2)):\n",
    "        data2[1][key2[j]]=(j+1)\n",
    "    \n",
    "    for j in range(len(key3)):\n",
    "        data2[2][key3[j]]=(j+1)\n",
    "                         \n",
    "        \n",
    "    for j in range(len(key4)):           \n",
    "        data2[3][key4[j]]=(j+1)\n",
    "                             \n",
    "    for j in range(len(key5)):\n",
    "        data2[4][key5[j]]=(j+1)\n",
    "       \n",
    "           \n",
    "    for j in range(len(key6)):                 \n",
    "        data2[5][key6[j]]=(j+1)\n",
    "    \n",
    "    for j in range(len(key7)):                 \n",
    "        data2[6][key7[j]]=(j+1)  \n",
    "       \n",
    "    for j in range(len(key8)):    \n",
    "        data2[7][key8[j]]=(j+1)\n",
    "    for j in range(len(key9)):\n",
    "        data2[8][key9[j]]=(j+1)\n",
    "    \n",
    "    for j in range(len(key10)):\n",
    "        data2[9][key10[j]]=(j+1)\n",
    "                         \n",
    "        \n",
    "   \n",
    "       \n",
    "    \n",
    "    for j in range(n):\n",
    "        sum=0\n",
    "        for i in range(m):\n",
    "       \n",
    "            sum=sum+data2[i][j] \n",
    "        cost[j]=sum\n",
    "        \n",
    "        \n",
    "    ################\n",
    "    Lp_prob = p.LpProblem('Problem', p.LpMinimize)  \n",
    "   \n",
    "    \n",
    "#     X=np.zeros(n+1,dtype=p.LpVariable)\n",
    "    X=np.zeros(n+m+1,dtype=p.LpVariable)\n",
    "    sizes=np.zeros(m,dtype=int)\n",
    "#     report_index(index,data1,e):  \n",
    "    max_size=0\n",
    "    for i in range(m):\n",
    "        count=0\n",
    "        for j in range(n):\n",
    "            if data1[i][j]==1:\n",
    "                count=count+1 \n",
    "        if count>max_size:\n",
    "            max_size=count\n",
    "        sizes[i]=count\n",
    "        \n",
    "    #############################33\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ###############################\n",
    "        \n",
    "        \n",
    "        \n",
    "  \n",
    "    select_sizes=np.zeros(m,dtype=int)\n",
    "   \n",
    "    size_final=np.zeros(m,dtype=int)\n",
    "\n",
    "    \n",
    "    for i in range(n):\n",
    "        var1=str(i)       \n",
    "        X[i]=p.LpVariable(var1,lowBound=0,upBound=1,cat='Integer')\n",
    "   \n",
    "    X[n]=p.LpVariable(str(n),lowBound=0,upBound=1,cat='Continuous')  \n",
    "#     for i in range(m):\n",
    "#         k=n+i+1\n",
    "#         var1=str(k)     \n",
    "#         alpha=(((sizes[i])*(sizes[i]+1))/2)\n",
    "#         X[i]=p.LpVariable(var1,lowBound=(((beta*sizes[i])*(beta*sizes[i]+1))/2),upBound=alpha,cat='Continuous')\n",
    "    \n",
    "        \n",
    "#     X[n]=  p.LpVariable(\"z1\",lowBound=0)\n",
    "    #X[n+1]=  p.LpVariable(\"z2\",lowBound=0)\n",
    "  \n",
    "\n",
    "    #########objective function#####################\n",
    "    \n",
    "#     Lp_prob += 2*X[n+1]+10*X[n+2]+9*X[n+3]+3*X[n+4]\n",
    "    Lp_prob+= p.lpSum([(X[j])*cost[j] for j in range(n)]) \n",
    "  \n",
    "    \n",
    "\n",
    "    ##############constraint#################\n",
    "    for i in range(2*m):\n",
    "        if i<m:\n",
    "\n",
    "            Lp_prob += p.lpSum([(X[j])*(data1[i][j]) for j in range(n)]) >= math.floor(beta[i]*sizes[i])\n",
    "            Lp_prob += p.lpSum([(X[j])*(data1[i][j]) for j in range(n)]) <= math.ceil((beta[i]+eps)*sizes[i])\n",
    "           # Lp_prob += p.lpSum([(X[j])*(data1[i][j])*(sizes[i]-report_index(j,i,data1,e)+1) for j in range(n)]) <= X[n+i+1]\n",
    "\n",
    "#              Lp_prob += p.lpSum([(X[j])*(data1[i][j])*(sizes[i]-report_index(j,i,data1,e)+1)/1000 for j in range(n)]) >= X[n+i+1]\n",
    "    \n",
    "            \n",
    "#         else:        \n",
    "#             Lp_prob += X[n] >= p.lpSum([-1*2*(X[j]-0.5)*data1[i-m][j] for j in range(n)])   /((sizes[i])*(sizes[i]+1)/2)\n",
    "            #Lp_prob += X[n+1] >= p.lpSum([-1*2*(X[j]-0.5)+r[j] for j in range(n)]) \n",
    "#     Lp_prob += X[n+1] >= p.lpSum([2*(X[j]-0.5)-r[j] for j in range(n)])\n",
    "#     Lp_prob += X[n+1] >= p.lpSum([-1*2*(X[j]-0.5)+r[j] for j in range(n)])       \n",
    "         \n",
    "#     Lp_prob += p.lpSum([2*(X[i]-0.5)*r[i] for i in range(n)]) >= X[n]*n\n",
    "    \n",
    "#     Lp_prob += p.lpSum([2*(X[i]-0.5)*r[i]*(1/pow(2,l1.index(i))) for i in l1]) >= 2*(1-(1/pow(2,math.floor(beta*size_final[0]))))\n",
    "#     Lp_prob += p.lpSum([2*(X[i]-0.5)*r[i]*(1/pow(2,l2.index(i))) for i in l2]) >= 2*(1-(1/pow(2,math.floor(beta*size_final[1]))))\n",
    "#     Lp_prob += p.lpSum([2*(X[i]-0.5)*r[i]*(1/pow(2,l3.index(i))) for i in l3]) >= 2*(1-(1/pow(2,math.floor(beta*size_final[2]))))\n",
    "#     Lp_prob += p.lpSum([2*(X[i]-0.5)*r[i]*(1/pow(2,l4.index(i))) for i in l4]) >= 2*(1-(1/pow(2,math.floor(beta*size_final[3]))))\n",
    "    \n",
    "    #n is the number of elements in sensitive attribute \n",
    "                 \n",
    "#     Lp_prob += X[n] <= 42000\n",
    "        \n",
    "    #####################################\n",
    "    status = Lp_prob.solve()   # Solver \n",
    "    print(p.LpStatus[status]) \n",
    "    print(\"objective is:\")        \n",
    "    print(p.value(Lp_prob.objective))\n",
    "    print(\"discripency is:\") \n",
    "    print(p.value(X[n]))\n",
    "    x=np.zeros(n,dtype=float)\n",
    "\n",
    "   # The solution status \n",
    "    Synth1={}\n",
    "    Synth2={}\n",
    "    # # Printing the final solution \n",
    "    for i in range(n):\n",
    "        if(p.value(X[i])==1):\n",
    "            Synth1[i]=1 \n",
    "            Synth2[i]=-1\n",
    "#             if(data1[2][i]==1):\n",
    "#                 print(\"no\")\n",
    "        else:\n",
    "            Synth1[i]=-1\n",
    "            Synth2[i]=1\n",
    "    Synthu1=Synth1  \n",
    "    Synthu2=Synth2  \n",
    "    \n",
    "              \n",
    "    return Synthu1,Synthu2   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NG\n",
    "# import time\n",
    "# import pulp as p \n",
    "# def min_max_lp_all_ng2(data1,gamma,eps,r,delta):\n",
    "#     import pulp as p \n",
    "    \n",
    "#     m=data1.shape[0]\n",
    "#     n=data1.shape[1]\n",
    "#     print('dimension of data')\n",
    "#     print(m,n)\n",
    "#     Lp_prob = p.LpProblem('Problem', p.LpMaximize)  \n",
    "   \n",
    "    \n",
    "# #     X=np.zeros(n+1,dtype=p.LpVariable)\n",
    "#     X=np.zeros(n+1,dtype=p.LpVariable)\n",
    "#     sizes=np.zeros(m,dtype=int)\n",
    "#     for i in range(m):\n",
    "#         count=0\n",
    "#         for j in range(n):\n",
    "#             if data1[i][j]==1:\n",
    "#                 count=count+1               \n",
    "#         sizes[i]=count\n",
    "  \n",
    "\n",
    "#     for i in range(n):\n",
    "#         var1=str(i)       \n",
    "#         X[i]=p.LpVariable(var1,lowBound=0,upBound=1,cat='Integer')\n",
    "    \n",
    "#     X[n]=p.LpVariable(str(n),lowBound=0,upBound=1,cat='Continuous')   \n",
    "        \n",
    "# #     X[n]=  p.LpVariable(\"z1\",lowBound=0)\n",
    "#     #X[n+1]=  p.LpVariable(\"z2\",lowBound=0)\n",
    "\n",
    "\n",
    "#     #########objective function#####################\n",
    "# #     Lp_prob += X[n] \n",
    "            \n",
    "#     Lp_prob += X[n]\n",
    "\n",
    "\n",
    "#     ##############constraint#################\n",
    "#     for i in range(2*m):\n",
    "#         if i<m:\n",
    "# #             Lp_prob += X[n] >= p.lpSum([2*(X[j]-0.5)*data1[i][j] for j in range(n)])\n",
    "#             Lp_prob += p.lpSum([2*(X[j]-0.5)*data1[i][j] for j in range(n)]) >= (2*gamma-1)*sizes[i]\n",
    "#             Lp_prob += p.lpSum([2*(X[j]-0.5)*data1[i][j] for j in range(n)]) <= ((2*gamma-1)+eps)*sizes[i]\n",
    "            \n",
    "# #         else:        \n",
    "# #             Lp_prob += X[n] >= p.lpSum([-1*2*(X[j]-0.5)*data1[i-m][j] for j in range(n)])\n",
    "#             #Lp_prob += X[n+1] >= p.lpSum([-1*2*(X[j]-0.5)+r[j] for j in range(n)]) \n",
    "# #     Lp_prob += X[n+1] >= p.lpSum([2*(X[j]-0.5)-r[j] for j in range(n)])\n",
    "# #     Lp_prob += X[n+1] >= p.lpSum([-1*2*(X[j]-0.5)+r[j] for j in range(n)])       \n",
    "         \n",
    "#     Lp_prob += p.lpSum([2*(X[i]-0.5)*r[i] for i in range(n)]) >= X[n]*n\n",
    "#     #n is the number of elements in sensitive attribute \n",
    "                 \n",
    "# #     Lp_prob += X[n] <= 42000\n",
    "    \n",
    "#     #####################################\n",
    "#     status = Lp_prob.solve()   # Solver \n",
    "#     print(p.LpStatus[status]) \n",
    "#     print(\"discripency is:\")        \n",
    "#     print(p.value(Lp_prob.objective))\n",
    "    \n",
    "#     x=np.zeros(n,dtype=float)\n",
    "\n",
    "#    # The solution status \n",
    "#     Synth1={}\n",
    "#     Synth2={}\n",
    "#     # # Printing the final solution \n",
    "#     for i in range(n):\n",
    "#         if(p.value(X[i])==1):\n",
    "#             Synth1[i]=1 \n",
    "#             Synth2[i]=-1\n",
    "#         else:\n",
    "#             Synth1[i]=-1\n",
    "#             Synth2[i]=1\n",
    "#     Synthu1=Synth1  \n",
    "#     Synthu2=Synth2  \n",
    "    \n",
    "              \n",
    "#     return Synthu1,Synthu2   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    24928\n",
      "1    11568\n",
      "2     4612\n",
      "3       80\n",
      "Name: marital, dtype: int64\n",
      "There are 28831 samples in the training set and 12357 samples in the test set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/subham/anaconda3/envs/tensorflow/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the SVM classifier on training data is 0.92\n",
      "The accuracy of the SVM classifier on test data is 0.91\n",
      "####Train prediction Label###############################################\n",
      "####Actual Train Label###############################################\n",
      "[[0.93664901 0.06335099]\n",
      " [0.93204958 0.06795042]\n",
      " [0.9399619  0.0600381 ]\n",
      " ...\n",
      " [0.93542819 0.06457181]\n",
      " [0.93341507 0.06658493]\n",
      " [0.79605492 0.20394508]]\n",
      "[0 0 0 ... 0 0 0]\n",
      "####Change to colors###############################################\n",
      "       age  job  marital  education  default  housing  loan  contact  month  \\\n",
      "0       39    3        0          3        0        1     0        1      2   \n",
      "1       55    3        0          0        0        1     0        1      8   \n",
      "2       39    3        0          3        1        0     0        0      1   \n",
      "3       56    8        0          3        0        1     0        1      3   \n",
      "4       49    3        0          3        0        1     0        1      5   \n",
      "...    ...  ...      ...        ...      ...      ...   ...      ...    ...   \n",
      "12352   35    4        0          4        0        0     0        1      3   \n",
      "12353   39    4        1          4        0        0     0        1      2   \n",
      "12354   43    1        0          2        0        1     0        1      0   \n",
      "12355   39    0        0          6        0        0     0        0      0   \n",
      "12356   46    2        0          1        0        0     1        1      3   \n",
      "\n",
      "       day_of_week  duration  campaign  pdays  previous  poutcome  \\\n",
      "0                4       635         3    999         0         0   \n",
      "1                4       248         2    999         0         0   \n",
      "2                3       207         1    999         0         0   \n",
      "3                3       176         7    999         0         0   \n",
      "4                4       271         1    999         0         0   \n",
      "...            ...       ...       ...    ...       ...       ...   \n",
      "12352            3        38         1    999         0         0   \n",
      "12353            0       617         2    999         0         0   \n",
      "12354            4        68         1    999         0         0   \n",
      "12355            2       174         1    999         0         0   \n",
      "12356            2       871         2    999         0         0   \n",
      "\n",
      "       emp.var.rate  cons.price.idx  cons.conf.idx  euribor3m  nr.employed  \n",
      "0               1.4          93.918          -42.7      4.957       5228.1  \n",
      "1              -1.8          93.075          -47.1      1.405       5099.1  \n",
      "2               1.4          94.465          -41.8      4.961       5228.1  \n",
      "3               1.4          93.444          -36.1      4.963       5228.1  \n",
      "4              -0.1          93.200          -42.0      4.021       5195.8  \n",
      "...             ...             ...            ...        ...          ...  \n",
      "12352           1.4          93.444          -36.1      4.964       5228.1  \n",
      "12353           1.4          93.918          -42.7      4.960       5228.1  \n",
      "12354          -1.8          92.893          -46.2      1.250       5099.1  \n",
      "12355           1.1          93.994          -36.4      4.858       5191.0  \n",
      "12356           1.4          93.444          -36.1      4.965       5228.1  \n",
      "\n",
      "[12357 rows x 20 columns]\n",
      "[0 0 0 ... 0 0 0]\n",
      "       y\n",
      "0      1\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "...   ..\n",
      "12352  0\n",
      "12353  1\n",
      "12354  0\n",
      "12355  0\n",
      "12356  1\n",
      "\n",
      "[12357 rows x 1 columns]\n",
      "       age  marital\n",
      "0       39        0\n",
      "1       55        0\n",
      "2       39        0\n",
      "3       56        0\n",
      "4       49        0\n",
      "...    ...      ...\n",
      "12352   35        0\n",
      "12353   39        1\n",
      "12354   43        0\n",
      "12355   39        0\n",
      "12356   46        0\n",
      "\n",
      "[12357 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/subham/anaconda3/envs/tensorflow/lib/python3.6/site-packages/pandas/core/indexing.py:671: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "/home/subham/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/subham/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   a_1  a_2  m_0  m_1  m_2  m_3\n",
      "0    0    1    1    0    0    0\n",
      "1    0    1    1    0    0    0\n",
      "2    0    1    1    0    0    0\n",
      "3    0    1    1    0    0    0\n",
      "4    0    1    1    0    0    0\n",
      "     0      1      2      3      4      5      6      7      8      9      \\\n",
      "a_1      0      0      0      0      0      0      0      0      1      0   \n",
      "a_2      1      1      1      1      1      1      1      1      0      1   \n",
      "m_0      1      1      1      1      1      1      1      0      0      1   \n",
      "m_1      0      0      0      0      0      0      0      1      1      0   \n",
      "m_2      0      0      0      0      0      0      0      0      0      0   \n",
      "m_3      0      0      0      0      0      0      0      0      0      0   \n",
      "\n",
      "     ...  12347  12348  12349  12350  12351  12352  12353  12354  12355  12356  \n",
      "a_1  ...      0      0      0      0      0      0      0      0      0      0  \n",
      "a_2  ...      1      1      1      1      1      1      1      1      1      1  \n",
      "m_0  ...      0      1      0      1      1      1      0      1      1      1  \n",
      "m_1  ...      1      0      1      0      0      0      1      0      0      0  \n",
      "m_2  ...      0      0      0      0      0      0      0      0      0      0  \n",
      "m_3  ...      0      0      0      0      0      0      0      0      0      0  \n",
      "\n",
      "[6 rows x 12357 columns]\n"
     ]
    }
   ],
   "source": [
    "# without accuracy\n",
    "import time\n",
    "# import pulp as p \n",
    "# from random import *\n",
    "data= pd.read_csv('data/bank_train.csv',skipinitialspace=True)\n",
    "\n",
    "print(data['marital'].value_counts())\n",
    "#marital\n",
    "#U=80, M=24928, S=11568, D=4612\n",
    "# m_3, m_0, m_1, m_2\n",
    "#age\n",
    "#>60 and <25= a_1\n",
    "#>=25and <=60 =a_2\n",
    "# print(data.head())\n",
    "# print(data.shape[0],data.shape[1])\n",
    "\n",
    "#sensitive columns name 0='age',2='marital'\n",
    "\n",
    "data_c = data.drop(columns=['age_group','y'])\n",
    "# print(sens)\n",
    "r=data[['y']]\n",
    "\n",
    "X_test,Y_test_pred,Y_test,e = Bank_svm(data_c , r)\n",
    "\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "# Y_test_pred.reset_index()\n",
    "Y_test.reset_index(drop=True, inplace=True)\n",
    "print(X_test)\n",
    "print(Y_test_pred)\n",
    "print(Y_test)\n",
    "sens=X_test[['age','marital']]\n",
    "print(sens)\n",
    "p=sens.shape[0]\n",
    "\n",
    "# for i in range(0,p):  \n",
    "#     if r.loc[i,'y'] == 1 :\n",
    "#                r.loc[i,\"y\"] = 1 \n",
    "#     else: \n",
    "#                r.loc[i,\"y\"] = 0 \n",
    "            \n",
    "for i in range(0,p):\n",
    "    if sens.loc[i,'age'] > 60 or sens.loc[i,'age'] < 25 :\n",
    "               sens.loc[i,'age'] = 1 \n",
    "    else :\n",
    "               sens.loc[i,'age'] = 2  \n",
    "            \n",
    "sens1 = pd.get_dummies(sens, columns=['age','marital'], prefix =['a','m'])\n",
    "print(sens1.head())\n",
    "sensitive = sens1.T\n",
    "\n",
    "print(sensitive)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(sens1['a_1'].value_counts())\n",
    "# print(sens1['a_2'].value_counts())\n",
    "# print(sens1['m_0'].value_counts())\n",
    "# print(sens1['m_1'].value_counts())\n",
    "# print(sens1['m_2'].value_counts())\n",
    "# print(sens1['m_3'].value_counts())\n",
    "\n",
    "# (unique, counts) = numpy.unique(Y_test_pred, return_counts=True)\n",
    "# frequencies = numpy.asarray((unique, counts)).T\n",
    "# print(frequencies)\n",
    "# print(r['y'].value_counts())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#accu_all,DP_all,acceptance_rate,alpha_weight = main(sensitive, Y_test, Y_test_pred,e )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Yang gerry results\n",
    "\n",
    "sensitive2=sens1.values\n",
    "\n",
    "sensitive1 = np.zeros((sensitive2.shape[0],8),dtype=float)\n",
    "#sensitive2 = np.zeros((sens.shape[0],7),dtype=int)\n",
    "sensitive3 = np.zeros((sensitive2.shape[0],14),dtype=float)\n",
    "\n",
    "   #print(sensitive1.shape[0])\n",
    "   #print(sensitive1.shape[1])\n",
    "\n",
    "'''\n",
    "for k in range(sens.shape[0]):\n",
    "    for i in range(7):\n",
    "        sensitive2[k][i] = sens.iloc[k,i]\n",
    "'''\n",
    "for k in range(sensitive2.shape[0]):\n",
    "    count = 0\n",
    "    for i in range(2):\n",
    "        for j in range(4):\n",
    "            if sensitive2[k][i]==1 and sensitive2[k][2+j]==1:\n",
    "                sensitive1[k][count] = 1\n",
    "                count = count+1\n",
    "            else:\n",
    "                sensitive1[k][count] = 0\n",
    "                count = count+1\n",
    "                \n",
    "selected = []          \n",
    "counts = []\n",
    "sensitive3 = np.concatenate((sensitive1,sensitive2), axis = 1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.] [12119   238]\n",
      "[0. 1.] [12068   289]\n",
      "[0. 1.] [12306    51]\n",
      "[0. 1.] [12356     1]\n",
      "[0. 1.] [5037 7320]\n",
      "[0. 1.] [9245 3112]\n",
      "[0. 1.] [11033  1324]\n",
      "[0. 1.] [12335    22]\n",
      "[0. 1.] [11778   579]\n",
      "[0. 1.] [  579 11778]\n",
      "[0. 1.] [4799 7558]\n",
      "[0. 1.] [8956 3401]\n",
      "[0. 1.] [10982  1375]\n",
      "[0. 1.] [12334    23]\n",
      "[0 1] [12119   238]\n",
      "[0 1] [12068   289]\n",
      "[0 1] [5037 7320]\n",
      "[0 1] [9245 3112]\n",
      "[0 1] [11033  1324]\n",
      "[0 1] [11778   579]\n",
      "[0 1] [  579 11778]\n",
      "[0 1] [4799 7558]\n",
      "[0 1] [8956 3401]\n",
      "[0 1] [10982  1375]\n"
     ]
    }
   ],
   "source": [
    "for i in range(14):\n",
    "    a,b=np.unique(sensitive3[:,i],return_counts=True)\n",
    "    print(a,b)\n",
    "    \n",
    "\n",
    "sensitive1=np.transpose(sensitive3)\n",
    "\n",
    "sensitiven=np.zeros((10,sensitive1.shape[1]),dtype=int)\n",
    "\n",
    "\n",
    "\n",
    "k=[0, 1,4, 5, 6, 8,9, 10, 11,12]\n",
    "for i in range(10):\n",
    "    sensitiven[i,:]=sensitive1[k[i],:]    \n",
    "    \n",
    "for i in range(10):\n",
    "    a,b=np.unique(sensitiven[i,:],return_counts=True)\n",
    "    print(a,b)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Yang gerry\n",
    "\n",
    "accu_al,DP_all,acceptance_rate,alpha_weight = main(sensitiven, Y_test, Y_test_pred,e )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensitive attribute  1\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "238\n",
      "105\n",
      "0.4411764705882353\n",
      "sensitive attribute  2\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "289\n",
      "83\n",
      "0.28719723183391005\n",
      "sensitive attribute  3\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "7320\n",
      "643\n",
      "0.0878415300546448\n",
      "sensitive attribute  4\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "3112\n",
      "414\n",
      "0.13303341902313626\n",
      "sensitive attribute  5\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1324\n",
      "115\n",
      "0.08685800604229607\n",
      "sensitive attribute  6\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "579\n",
      "213\n",
      "0.36787564766839376\n",
      "sensitive attribute  7\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "11778\n",
      "1175\n",
      "0.09976226863644082\n",
      "sensitive attribute  8\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "7558\n",
      "748\n",
      "0.09896798094734056\n",
      "sensitive attribute  9\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "3401\n",
      "497\n",
      "0.14613349014995589\n",
      "sensitive attribute  10\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1375\n",
      "140\n",
      "0.10181818181818182\n",
      "data acceptance rates\n",
      "[0.4411764705882353, 0.28719723183391005, 0.0878415300546448, 0.13303341902313626, 0.08685800604229607, 0.36787564766839376, 0.09976226863644082, 0.09896798094734056, 0.14613349014995589, 0.10181818181818182]\n",
      "data DP\n",
      "0.3543184645459392\n",
      "sensitive attribute  1\n",
      "prec reca accuracy for each sens\n",
      "0.6941176470588235 0.5619047619047619 0.6974789915966386\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "238\n",
      "85\n",
      "0.35714285714285715\n",
      "sensitive attribute  2\n",
      "prec reca accuracy for each sens\n",
      "0.7419354838709677 0.5542168674698795 0.8166089965397924\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "289\n",
      "62\n",
      "0.21453287197231835\n",
      "sensitive attribute  3\n",
      "prec reca accuracy for each sens\n",
      "0.6695842450765864 0.4758942457231726 0.9333333333333333\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "7320\n",
      "457\n",
      "0.06243169398907104\n",
      "sensitive attribute  4\n",
      "prec reca accuracy for each sens\n",
      "0.6421052631578947 0.4420289855072464 0.8929948586118251\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "3112\n",
      "285\n",
      "0.09158097686375322\n",
      "sensitive attribute  5\n",
      "prec reca accuracy for each sens\n",
      "0.7192982456140351 0.3565217391304348 0.93202416918429\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1324\n",
      "57\n",
      "0.04305135951661632\n",
      "sensitive attribute  6\n",
      "prec reca accuracy for each sens\n",
      "0.7142857142857143 0.5633802816901409 0.7564766839378239\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "579\n",
      "168\n",
      "0.29015544041450775\n",
      "sensitive attribute  7\n",
      "prec reca accuracy for each sens\n",
      "0.6625 0.451063829787234 0.9223127865511972\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "11778\n",
      "800\n",
      "0.06792324673119375\n",
      "sensitive attribute  8\n",
      "prec reca accuracy for each sens\n",
      "0.6734317343173432 0.4879679144385027 0.9259063244244509\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "7558\n",
      "542\n",
      "0.071712093146335\n",
      "sensitive attribute  9\n",
      "prec reca accuracy for each sens\n",
      "0.659942363112392 0.4607645875251509 0.8865039694207586\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "3401\n",
      "347\n",
      "0.10202881505439576\n",
      "sensitive attribute  10\n",
      "prec reca accuracy for each sens\n",
      "0.717948717948718 0.4 0.9229090909090909\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1375\n",
      "78\n",
      "0.05672727272727273\n",
      "data acceptance rates\n",
      "[0.35714285714285715, 0.21453287197231835, 0.06243169398907104, 0.09158097686375322, 0.04305135951661632, 0.29015544041450775, 0.06792324673119375, 0.071712093146335, 0.10202881505439576, 0.05672727272727273]\n",
      "data DP\n",
      "0.31409149762624083\n",
      "SVM accuracy--------------------------\n",
      "0.6714876033057852 0.46829971181556196 0.9145423646516144\n",
      "[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "dimension of data\n",
      "10 12357\n",
      "Optimal\n",
      "objective is:\n",
      "435911.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.08403361344537816, 0.04844290657439446, 0.05546448087431694, 0.061053984575835475, 0.03700906344410876, 0.07081174438687392, 0.054763117677024964, 0.05636411749139984, 0.05998235812996178, 0.04072727272727273] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 0\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "sensitive attribute  7\n",
      "sensitive attribute  8\n",
      "sensitive attribute  9\n",
      "sensitive attribute  10\n",
      "individual acceptance rates\n",
      "[0.08403361344537816, 0.04844290657439446, 0.05546448087431694, 0.061053984575835475, 0.03625377643504532, 0.07253886010362694, 0.054763117677024964, 0.05636411749139984, 0.05998235812996178, 0.04072727272727273]\n",
      "individul precision\n",
      "[0.65, 0.7142857142857143, 0.6847290640394089, 0.6736842105263158, 0.7083333333333334, 0.6904761904761905, 0.6821705426356589, 0.6830985915492958, 0.6764705882352942, 0.7142857142857143]\n",
      "individual recall\n",
      "[0.12380952380952381, 0.12048192771084337, 0.432348367029549, 0.30917874396135264, 0.2956521739130435, 0.13615023474178403, 0.37446808510638296, 0.3890374331550802, 0.2776659959758551, 0.2857142857142857]\n",
      "DP all\n",
      "0.04777983701033284\n",
      "precision all 0.6826783114992722\n",
      "recall all 0.33789625360230546\n",
      "accuracy all 0.9079873755765963\n",
      "TP,FP,TN,FN\n",
      "469 218 10751 919\n",
      "dimension of data\n",
      "10 12357\n",
      "Optimal\n",
      "objective is:\n",
      "438644.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.1092436974789916, 0.06228373702422145, 0.055327868852459015, 0.06298200514138817, 0.03323262839879154, 0.08808290155440414, 0.054763117677024964, 0.05702566816618153, 0.06292266980299911, 0.03709090909090909] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 0\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "sensitive attribute  7\n",
      "sensitive attribute  8\n",
      "sensitive attribute  9\n",
      "sensitive attribute  10\n",
      "individual acceptance rates\n",
      "[0.1092436974789916, 0.06228373702422145, 0.055327868852459015, 0.06298200514138817, 0.03323262839879154, 0.08808290155440414, 0.054763117677024964, 0.05702566816618153, 0.06292266980299911, 0.03709090909090909]\n",
      "individul precision\n",
      "[0.6538461538461539, 0.7777777777777778, 0.6864197530864198, 0.6836734693877551, 0.6818181818181818, 0.7254901960784313, 0.6852713178294574, 0.6844547563805105, 0.6915887850467289, 0.7058823529411765]\n",
      "individual recall\n",
      "[0.1619047619047619, 0.1686746987951807, 0.432348367029549, 0.32367149758454106, 0.2608695652173913, 0.17370892018779344, 0.37617021276595747, 0.39438502673796794, 0.2977867203219316, 0.2571428571428571]\n",
      "DP all\n",
      "0.07601106908020006\n",
      "precision all 0.6882183908045977\n",
      "recall all 0.3451008645533141\n",
      "accuracy all 0.908877559278142\n",
      "TP,FP,TN,FN\n",
      "479 217 10752 909\n",
      "dimension of data\n",
      "10 12357\n",
      "Optimal\n",
      "objective is:\n",
      "440942.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.12605042016806722, 0.06920415224913495, 0.05491803278688524, 0.06491002570694088, 0.0324773413897281, 0.09844559585492228, 0.05493292579385295, 0.05715797830113787, 0.065274919141429, 0.03636363636363636] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 0\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "sensitive attribute  7\n",
      "sensitive attribute  8\n",
      "sensitive attribute  9\n",
      "sensitive attribute  10\n",
      "individual acceptance rates\n",
      "[0.12605042016806722, 0.06920415224913495, 0.05491803278688524, 0.06491002570694088, 0.0324773413897281, 0.09844559585492228, 0.05493292579385295, 0.05715797830113787, 0.065274919141429, 0.03636363636363636]\n",
      "individul precision\n",
      "[0.6666666666666666, 0.8, 0.6865671641791045, 0.6831683168316832, 0.6976744186046512, 0.7368421052631579, 0.6862442040185471, 0.6851851851851852, 0.6936936936936937, 0.72]\n",
      "individual recall\n",
      "[0.19047619047619047, 0.1927710843373494, 0.42923794712286156, 0.3333333333333333, 0.2608695652173913, 0.19718309859154928, 0.3778723404255319, 0.39572192513368987, 0.30985915492957744, 0.2571428571428571]\n",
      "DP all\n",
      "0.09357307877833912\n",
      "precision all 0.6903409090909091\n",
      "recall all 0.35014409221902015\n",
      "accuracy all 0.9093631140244396\n",
      "TP,FP,TN,FN\n",
      "486 218 10751 902\n",
      "dimension of data\n",
      "10 12357\n",
      "Optimal\n",
      "objective is:\n",
      "435769.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.18067226890756302, 0.12110726643598616, 0.05259562841530055, 0.06908740359897173, 0.030211480362537766, 0.14853195164075994, 0.054338597384955, 0.05662873776131252, 0.07350779182593355, 0.03490909090909091] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 0\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "sensitive attribute  7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensitive attribute  8\n",
      "sensitive attribute  9\n",
      "sensitive attribute  10\n",
      "individual acceptance rates\n",
      "[0.18067226890756302, 0.12110726643598616, 0.05259562841530055, 0.06908740359897173, 0.030211480362537766, 0.14853195164075994, 0.054338597384955, 0.05662873776131252, 0.07350779182593355, 0.03490909090909091]\n",
      "individul precision\n",
      "[0.7209302325581395, 0.8285714285714286, 0.6857142857142857, 0.6651162790697674, 0.675, 0.7674418604651163, 0.678125, 0.6892523364485982, 0.688, 0.6875]\n",
      "individual recall\n",
      "[0.29523809523809524, 0.3493975903614458, 0.4105754276827372, 0.34541062801932365, 0.23478260869565218, 0.30985915492957744, 0.36936170212765956, 0.39438502673796794, 0.3460764587525151, 0.2357142857142857]\n",
      "DP all\n",
      "0.15046078854502526\n",
      "precision all 0.6887052341597796\n",
      "recall all 0.36023054755043227\n",
      "accuracy all 0.9098486687707372\n",
      "TP,FP,TN,FN\n",
      "500 226 10743 888\n",
      "<--------------------------------------->\n"
     ]
    }
   ],
   "source": [
    "#Yang gerry\n",
    "\n",
    "accu_al,DP_all,acceptance_rate,alpha_weight = main(sensitiven, Y_test, Y_test_pred,e )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.style.use('seaborn-white')\n",
    "\n",
    "labels =['0.0470' ,'0.0760' ,'0.0935' ,'0.1504']\n",
    "\n",
    "men_means = [0.9079, 0.9088,0.9093 ,0.9098]\n",
    "women_means = [0.9048,  0.9051, 0.9053, 0.9074]\n",
    "\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, men_means, width, color = 'g',label='LPCA ')\n",
    "rects2 = ax.bar(x + width/2, women_means, width, color = 'blue',label='Yang ')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Test Accuracy')\n",
    "ax.set_xlabel('Demographic Disparity')\n",
    "ax.set_title('BANK')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "#ax.legend()\n",
    "ax.set_ylim(.6,1 )\n",
    "#ax.bar_label(rects1, padding=3)\n",
    "#ax.bar_label(rects2, padding=3)\n",
    "\n",
    "ax.legend(loc='lower center', bbox_to_anchor=(0.5, -0.30), shadow=True, ncol=10)\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "# plt.savefig('a5.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensitive attribute  1\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "579\n",
      "213\n",
      "0.36787564766839376\n",
      "sensitive attribute  2\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "11778\n",
      "1175\n",
      "0.09976226863644082\n",
      "sensitive attribute  3\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "7558\n",
      "748\n",
      "0.09896798094734056\n",
      "sensitive attribute  4\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "3401\n",
      "497\n",
      "0.14613349014995589\n",
      "sensitive attribute  5\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1375\n",
      "140\n",
      "0.10181818181818182\n",
      "sensitive attribute  6\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "23\n",
      "3\n",
      "0.13043478260869565\n",
      "data acceptance rates\n",
      "[0.36787564766839376, 0.09976226863644082, 0.09896798094734056, 0.14613349014995589, 0.10181818181818182, 0.13043478260869565]\n",
      "data DP\n",
      "0.2689076667210532\n",
      "sensitive attribute  1\n",
      "prec reca accuracy for each sens\n",
      "0.7142857142857143 0.5633802816901409 0.7564766839378239\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "579\n",
      "168\n",
      "0.29015544041450775\n",
      "sensitive attribute  2\n",
      "prec reca accuracy for each sens\n",
      "0.6625 0.451063829787234 0.9223127865511972\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "11778\n",
      "800\n",
      "0.06792324673119375\n",
      "sensitive attribute  3\n",
      "prec reca accuracy for each sens\n",
      "0.6734317343173432 0.4879679144385027 0.9259063244244509\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "7558\n",
      "542\n",
      "0.071712093146335\n",
      "sensitive attribute  4\n",
      "prec reca accuracy for each sens\n",
      "0.659942363112392 0.4607645875251509 0.8865039694207586\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "3401\n",
      "347\n",
      "0.10202881505439576\n",
      "sensitive attribute  5\n",
      "prec reca accuracy for each sens\n",
      "0.717948717948718 0.4 0.9229090909090909\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1375\n",
      "78\n",
      "0.05672727272727273\n",
      "sensitive attribute  6\n",
      "prec reca accuracy for each sens\n",
      "0.0 0.0 0.8260869565217391\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "23\n",
      "1\n",
      "0.043478260869565216\n",
      "data acceptance rates\n",
      "[0.29015544041450775, 0.06792324673119375, 0.071712093146335, 0.10202881505439576, 0.05672727272727273, 0.043478260869565216]\n",
      "data DP\n",
      "0.24667717954494253\n",
      "SVM accuracy--------------------------\n",
      "0.6714876033057852 0.46829971181556196 0.9145423646516144\n",
      "[[1, 0.1, 1, 1, 1, 1]]\n",
      "dimension of data\n",
      "6 12357\n",
      "Optimal\n",
      "objective is:\n",
      "111171.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.04, 0.04, 0.04, 0.04, 0.04, 0.04] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 0\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "individual acceptance rates\n",
      "[0.039723661485319514, 0.039989811512990324, 0.03995766075681397, 0.03998823875330785, 0.04, 0.043478260869565216]\n",
      "individul precision\n",
      "[0.6521739130434783, 0.6539278131634819, 0.6887417218543046, 0.7279411764705882, 0.2909090909090909, 0.0]\n",
      "individual recall\n",
      "[0.07042253521126761, 0.2621276595744681, 0.27807486631016043, 0.19919517102615694, 0.11428571428571428, 0.0]\n",
      "DP all\n",
      "0.003754599384245702\n",
      "precision all 0.6538461538461539\n",
      "recall all 0.23270893371757925\n",
      "accuracy all 0.8999757222626851\n",
      "TP,FP,TN,FN\n",
      "323 171 10798 1065\n",
      "dimension of data\n",
      "6 12357\n",
      "Optimal\n",
      "objective is:\n",
      "170064.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.05, 0.05, 0.05, 0.05, 0.05, 0.05] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 0\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "individual acceptance rates\n",
      "[0.04835924006908463, 0.049923586347427405, 0.0498809208785393, 0.049985298441634816, 0.04945454545454545, 0.043478260869565216]\n",
      "individul precision\n",
      "[0.7142857142857143, 0.6360544217687075, 0.6790450928381963, 0.6941176470588235, 0.29411764705882354, 0.0]\n",
      "individual recall\n",
      "[0.09389671361502347, 0.31829787234042556, 0.3422459893048128, 0.23742454728370221, 0.14285714285714285, 0.0]\n",
      "DP all\n",
      "0.0065070375720696\n",
      "precision all 0.6396103896103896\n",
      "recall all 0.2838616714697406\n",
      "accuracy all 0.9015942380836772\n",
      "TP,FP,TN,FN\n",
      "394 222 10747 994\n",
      "dimension of data\n",
      "6 12357\n",
      "Optimal\n",
      "objective is:\n",
      "243687.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.06, 0.06, 0.06, 0.06, 0.06, 0.06] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 0\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "individual acceptance rates\n",
      "[0.05872193436960276, 0.059942265240278486, 0.05993649113522096, 0.05998235812996178, 0.05963636363636363, 0.043478260869565216]\n",
      "individul precision\n",
      "[0.7352941176470589, 0.6260623229461756, 0.673289183222958, 0.6813725490196079, 0.2804878048780488, 0.0]\n",
      "individual recall\n",
      "[0.11737089201877934, 0.37617021276595747, 0.4077540106951872, 0.2796780684104628, 0.16428571428571428, 0.0]\n",
      "DP all\n",
      "0.01650409726039656\n",
      "precision all 0.6310810810810811\n",
      "recall all 0.33645533141210376\n",
      "accuracy all 0.9033746054867686\n",
      "TP,FP,TN,FN\n",
      "467 273 10696 921\n",
      "dimension of data\n",
      "6 12357\n",
      "Optimal\n",
      "objective is:\n",
      "428852.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.08, 0.08, 0.08, 0.08, 0.08, 0.08] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 0\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "individual acceptance rates\n",
      "[0.07944732297063903, 0.07997962302598065, 0.07991532151362794, 0.0799764775066157, 0.08, 0.08695652173913043]\n",
      "individul precision\n",
      "[0.7608695652173914, 0.578556263269639, 0.6225165562913907, 0.6544117647058824, 0.23636363636363636, 0.0]\n",
      "individual recall\n",
      "[0.1643192488262911, 0.46382978723404256, 0.5026737967914439, 0.358148893360161, 0.18571428571428572, 0.0]\n",
      "DP all\n",
      "0.007509198768491404\n",
      "precision all 0.5870445344129555\n",
      "recall all 0.41786743515850144\n",
      "accuracy all 0.9015942380836772\n",
      "TP,FP,TN,FN\n",
      "580 408 10561 808\n",
      "dimension of data\n",
      "6 12357\n",
      "Optimal\n",
      "objective is:\n",
      "1447002.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.15, 0.15, 0.15, 0.15, 0.15, 0.15] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 0\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "individual acceptance rates\n",
      "[0.14853195164075994, 0.1499405671591102, 0.14990738290553057, 0.14995589532490444, 0.14981818181818182, 0.13043478260869565]\n",
      "individul precision\n",
      "[0.7790697674418605, 0.4535673839184598, 0.4651368049426302, 0.5941176470588235, 0.18446601941747573, 0.0]\n",
      "individual recall\n",
      "[0.3145539906103286, 0.6817021276595745, 0.7045454545454546, 0.6096579476861167, 0.2714285714285714, 0.0]\n",
      "DP all\n",
      "0.019521112716208794\n",
      "precision all 0.468682505399568\n",
      "recall all 0.6253602305475504\n",
      "accuracy all 0.8782876102613904\n",
      "TP,FP,TN,FN\n",
      "868 984 9985 520\n",
      "dimension of data\n",
      "6 12357\n",
      "Optimal\n",
      "objective is:\n",
      "2524719.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.2, 0.2, 0.2, 0.2, 0.2, 0.2] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 0\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "individual acceptance rates\n",
      "[0.19861830742659758, 0.1999490575649516, 0.19992061391902619, 0.19994119376653927, 0.2, 0.17391304347826086]\n",
      "individul precision\n",
      "[0.7739130434782608, 0.3842887473460722, 0.3831899404367968, 0.5367647058823529, 0.1781818181818182, 0.25]\n",
      "individual recall\n",
      "[0.41784037558685444, 0.7702127659574468, 0.7740641711229946, 0.7344064386317908, 0.35, 0.3333333333333333]\n",
      "DP all\n",
      "0.026086956521739146\n",
      "precision all 0.40242914979757083\n",
      "recall all 0.7161383285302594\n",
      "accuracy all 0.8486687707372339\n",
      "TP,FP,TN,FN\n",
      "994 1476 9493 394\n",
      "dimension of data\n",
      "6 12357\n",
      "Optimal\n",
      "objective is:\n",
      "4830952.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.28, 0.28, 0.28, 0.28, 0.28, 0.28] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 0\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "individual acceptance rates\n",
      "[0.27979274611398963, 0.27992868059093223, 0.27996824556761046, 0.279917671273155, 0.28, 0.2608695652173913]\n",
      "individul precision\n",
      "[0.7283950617283951, 0.30087958750379135, 0.29820415879017015, 0.43067226890756305, 0.17662337662337663, 0.16666666666666666]\n",
      "individual recall\n",
      "[0.5539906103286385, 0.8442553191489361, 0.8435828877005348, 0.8249496981891348, 0.4857142857142857, 0.3333333333333333]\n",
      "DP all\n",
      "0.01913043478260873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision all 0.32090199479618386\n",
      "recall all 0.7997118155619597\n",
      "accuracy all 0.7874079469126811\n",
      "TP,FP,TN,FN\n",
      "1110 2349 8620 278\n",
      "dimension of data\n",
      "6 12357\n",
      "Optimal\n",
      "objective is:\n",
      "5514968.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.3, 0.3, 0.3, 0.3, 0.3, 0.3] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 0\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "individual acceptance rates\n",
      "[0.2987910189982729, 0.2999660383766344, 0.29994707594601744, 0.2999117906498089, 0.29963636363636365, 0.30434782608695654]\n",
      "individul precision\n",
      "[0.7167630057803468, 0.28474384375884515, 0.28054697838553155, 0.4117647058823529, 0.17475728155339806, 0.2857142857142857]\n",
      "individual recall\n",
      "[0.5821596244131455, 0.8561702127659574, 0.8502673796791443, 0.8450704225352113, 0.5142857142857142, 0.6666666666666666]\n",
      "DP all\n",
      "0.0055568070886836685\n",
      "precision all 0.3049109552077712\n",
      "recall all 0.8141210374639769\n",
      "accuracy all 0.7706563081654123\n",
      "TP,FP,TN,FN\n",
      "1130 2576 8393 258\n",
      "<--------------------------------------->\n"
     ]
    }
   ],
   "source": [
    "#LP-5 same betas result\n",
    "accu_al,DP_all,acceptance_rate,alpha_weight = main(sensitive, Y_test, Y_test_pred,e )\n",
    "\n",
    "precision=[[0.6521739130434783, 0.6539278131634819, 0.6887417218543046, 0.7279411764705882, 0.2909090909090909, 0.0],\n",
    "[0.7142857142857143, 0.6360544217687075, 0.6790450928381963, 0.6941176470588235, 0.29411764705882354, 0.0],\n",
    "[0.7352941176470589, 0.6260623229461756, 0.673289183222958, 0.6813725490196079, 0.2804878048780488, 0.0],\n",
    "[0.7608695652173914, 0.578556263269639, 0.6225165562913907, 0.6544117647058824, 0.23636363636363636, 0.0],\n",
    "[0.7790697674418605, 0.4535673839184598, 0.4651368049426302, 0.5941176470588235, 0.18446601941747573, 0.0],\n",
    "[0.7739130434782608, 0.3842887473460722, 0.3831899404367968, 0.5367647058823529, 0.1781818181818182, 0.25],\n",
    "[0.7283950617283951, 0.30087958750379135, 0.29820415879017015, 0.43067226890756305, 0.17662337662337663, 0.16666666666666666],\n",
    "          [0.7167630057803468, 0.28474384375884515, 0.28054697838553155, 0.4117647058823529, 0.17475728155339806, 0.2857142857142857]\n",
    "]\n",
    "\n",
    "recall=[[0.07042253521126761, 0.2621276595744681, 0.27807486631016043, 0.19919517102615694, 0.11428571428571428, 0.0],\n",
    "[0.09389671361502347, 0.31829787234042556, 0.3422459893048128, 0.23742454728370221, 0.14285714285714285, 0.0],\n",
    "[0.11737089201877934, 0.37617021276595747, 0.4077540106951872, 0.2796780684104628, 0.16428571428571428, 0.0],\n",
    "[0.1643192488262911, 0.46382978723404256, 0.5026737967914439, 0.358148893360161, 0.18571428571428572, 0.0],\n",
    "[0.3145539906103286, 0.6817021276595745, 0.7045454545454546, 0.6096579476861167, 0.2714285714285714, 0.0],\n",
    "[0.41784037558685444, 0.7702127659574468, 0.7740641711229946, 0.7344064386317908, 0.35, 0.3333333333333333],\n",
    "[0.5539906103286385, 0.8442553191489361, 0.8435828877005348, 0.8249496981891348, 0.4857142857142857, 0.3333333333333333],\n",
    "       [0.5821596244131455, 0.8561702127659574, 0.8502673796791443, 0.8450704225352113, 0.5142857142857142, 0.6666666666666666]\n",
    "]\n",
    "\n",
    "accu=[0.8999, 0.9015 , 0.9033 ,0.9015 ,0.8782 ,0.8486, 0.7874, 0.7706]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensitive attribute  1\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "579\n",
      "213\n",
      "0.36787564766839376\n",
      "sensitive attribute  2\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "11778\n",
      "1175\n",
      "0.09976226863644082\n",
      "sensitive attribute  3\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "7558\n",
      "748\n",
      "0.09896798094734056\n",
      "sensitive attribute  4\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "3401\n",
      "497\n",
      "0.14613349014995589\n",
      "sensitive attribute  5\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "1375\n",
      "140\n",
      "0.10181818181818182\n",
      "sensitive attribute  6\n",
      "ACTUAL----------total ,accepted, aceeptance rate:\n",
      "23\n",
      "3\n",
      "0.13043478260869565\n",
      "data acceptance rates\n",
      "[0.36787564766839376, 0.09976226863644082, 0.09896798094734056, 0.14613349014995589, 0.10181818181818182, 0.13043478260869565]\n",
      "data DP\n",
      "0.2689076667210532\n",
      "sensitive attribute  1\n",
      "prec reca accuracy for each sens\n",
      "0.7142857142857143 0.5633802816901409 0.7564766839378239\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "579\n",
      "168\n",
      "0.29015544041450775\n",
      "sensitive attribute  2\n",
      "prec reca accuracy for each sens\n",
      "0.6625 0.451063829787234 0.9223127865511972\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "11778\n",
      "800\n",
      "0.06792324673119375\n",
      "sensitive attribute  3\n",
      "prec reca accuracy for each sens\n",
      "0.6734317343173432 0.4879679144385027 0.9259063244244509\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "7558\n",
      "542\n",
      "0.071712093146335\n",
      "sensitive attribute  4\n",
      "prec reca accuracy for each sens\n",
      "0.659942363112392 0.4607645875251509 0.8865039694207586\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "3401\n",
      "347\n",
      "0.10202881505439576\n",
      "sensitive attribute  5\n",
      "prec reca accuracy for each sens\n",
      "0.717948717948718 0.4 0.9229090909090909\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "1375\n",
      "78\n",
      "0.05672727272727273\n",
      "sensitive attribute  6\n",
      "prec reca accuracy for each sens\n",
      "0.0 0.0 0.8260869565217391\n",
      "SVM----------total , accepted, aceeptance rate:\n",
      "23\n",
      "1\n",
      "0.043478260869565216\n",
      "data acceptance rates\n",
      "[0.29015544041450775, 0.06792324673119375, 0.071712093146335, 0.10202881505439576, 0.05672727272727273, 0.043478260869565216]\n",
      "data DP\n",
      "0.24667717954494253\n",
      "SVM accuracy--------------------------\n",
      "0.6714876033057852 0.46829971181556196 0.9145423646516144\n",
      "[[1, 1, 1, 1, 1, 1], [1, 1, 1, 2, 1, 1], [1, 1, 1, 4, 1, 1], [1, 1, 1, 6, 1, 1], [1, 1, 1, 8, 1, 1], [1, 1, 1, 10, 1, 1]]\n",
      "dimension of data\n",
      "6 12357\n",
      "Optimal\n",
      "objective is:\n",
      "429929.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.18825561312607944, 0.05306503650874512, 0.055570256681661816, 0.0764481034989709, 0.03854545454545454, 0.043478260869565216] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 0\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "individual acceptance rates\n",
      "[0.18825561312607944, 0.05306503650874512, 0.05570256681661815, 0.0764481034989709, 0.03781818181818182, 0.043478260869565216]\n",
      "individul precision\n",
      "[0.7798165137614679, 0.656, 0.6864608076009501, 0.6884615384615385, 0.5192307692307693, 0.0]\n",
      "individual recall\n",
      "[0.39906103286384975, 0.34893617021276596, 0.38636363636363635, 0.36016096579476864, 0.19285714285714287, 0.0]\n",
      "DP all\n",
      "0.1504374313078976\n",
      "precision all 0.6743869209809265\n",
      "recall all 0.35662824207492794\n",
      "accuracy all 0.9083920045318443\n",
      "TP,FP,TN,FN\n",
      "495 239 10730 893\n",
      "dimension of data\n",
      "6 12357\n",
      "Optimal\n",
      "objective is:\n",
      "473928.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.18825561312607944, 0.05306503650874512, 0.055570256681661816, 0.0764481034989709, 0.03854545454545454, 0.043478260869565216] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 1\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "individual acceptance rates\n",
      "[0.18825561312607944, 0.05306503650874512, 0.05570256681661815, 0.0764481034989709, 0.03781818181818182, 0.043478260869565216]\n",
      "individul precision\n",
      "[0.7798165137614679, 0.664, 0.6935866983372921, 0.6961538461538461, 0.5192307692307693, 0.0]\n",
      "individual recall\n",
      "[0.39906103286384975, 0.35319148936170214, 0.39037433155080214, 0.3641851106639839, 0.19285714285714287, 0.0]\n",
      "DP all\n",
      "0.1504374313078976\n",
      "precision all 0.6811989100817438\n",
      "recall all 0.36023054755043227\n",
      "accuracy all 0.9092012624423403\n",
      "TP,FP,TN,FN\n",
      "500 234 10735 888\n",
      "dimension of data\n",
      "6 12357\n",
      "Optimal\n",
      "objective is:\n",
      "551614.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.18825561312607944, 0.05306503650874512, 0.055570256681661816, 0.0764481034989709, 0.03854545454545454, 0.043478260869565216] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 2\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "individual acceptance rates\n",
      "[0.18825561312607944, 0.05306503650874512, 0.05570256681661815, 0.0764481034989709, 0.03781818181818182, 0.043478260869565216]\n",
      "individul precision\n",
      "[0.7339449541284404, 0.6656, 0.6864608076009501, 0.6923076923076923, 0.5192307692307693, 0.0]\n",
      "individual recall\n",
      "[0.3755868544600939, 0.35404255319148936, 0.38636363636363635, 0.36217303822937624, 0.19285714285714287, 0.0]\n",
      "DP all\n",
      "0.1504374313078976\n",
      "precision all 0.6757493188010899\n",
      "recall all 0.3573487031700288\n",
      "accuracy all 0.9085538561139435\n",
      "TP,FP,TN,FN\n",
      "496 238 10731 892\n",
      "dimension of data\n",
      "6 12357\n",
      "Optimal\n",
      "objective is:\n",
      "618954.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.18825561312607944, 0.05306503650874512, 0.055570256681661816, 0.0764481034989709, 0.03854545454545454, 0.043478260869565216] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 3\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "individual acceptance rates\n",
      "[0.18825561312607944, 0.05306503650874512, 0.05570256681661815, 0.0764481034989709, 0.03781818181818182, 0.043478260869565216]\n",
      "individul precision\n",
      "[0.6788990825688074, 0.6672, 0.6888361045130641, 0.6692307692307692, 0.5192307692307693, 0.0]\n",
      "individual recall\n",
      "[0.3474178403755869, 0.3548936170212766, 0.3877005347593583, 0.3501006036217304, 0.19285714285714287, 0.0]\n",
      "DP all\n",
      "0.1504374313078976\n",
      "precision all 0.6689373297002725\n",
      "recall all 0.3537463976945245\n",
      "accuracy all 0.9077445982034474\n",
      "TP,FP,TN,FN\n",
      "491 243 10726 897\n",
      "dimension of data\n",
      "6 12357\n",
      "Optimal\n",
      "objective is:\n",
      "677925.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.18825561312607944, 0.05306503650874512, 0.055570256681661816, 0.0764481034989709, 0.03854545454545454, 0.043478260869565216] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 4\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "individual acceptance rates\n",
      "[0.18825561312607944, 0.05306503650874512, 0.05570256681661815, 0.0764481034989709, 0.03781818181818182, 0.043478260869565216]\n",
      "individul precision\n",
      "[0.6330275229357798, 0.6688, 0.6793349168646081, 0.6692307692307692, 0.5192307692307693, 0.0]\n",
      "individual recall\n",
      "[0.323943661971831, 0.3557446808510638, 0.38235294117647056, 0.3501006036217304, 0.19285714285714287, 0.0]\n",
      "DP all\n",
      "0.1504374313078976\n",
      "precision all 0.6634877384196185\n",
      "recall all 0.35086455331412103\n",
      "accuracy all 0.9070971918750506\n",
      "TP,FP,TN,FN\n",
      "487 247 10722 901\n",
      "dimension of data\n",
      "6 12357\n",
      "Optimal\n",
      "objective is:\n",
      "729785.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.18825561312607944, 0.05306503650874512, 0.055570256681661816, 0.0764481034989709, 0.03854545454545454, 0.043478260869565216] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 5\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "individual acceptance rates\n",
      "[0.18825561312607944, 0.05306503650874512, 0.05570256681661815, 0.0764481034989709, 0.03781818181818182, 0.043478260869565216]\n",
      "individul precision\n",
      "[0.6055045871559633, 0.6672, 0.6793349168646081, 0.6538461538461539, 0.5192307692307693, 0.0]\n",
      "individual recall\n",
      "[0.30985915492957744, 0.3548936170212766, 0.38235294117647056, 0.3420523138832998, 0.19285714285714287, 0.0]\n",
      "DP all\n",
      "0.1504374313078976\n",
      "precision all 0.6580381471389646\n",
      "recall all 0.3479827089337176\n",
      "accuracy all 0.9064497855466537\n",
      "TP,FP,TN,FN\n",
      "483 251 10718 905\n",
      "dimension of data\n",
      "6 12357\n",
      "Optimal\n",
      "objective is:\n",
      "257182.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.09844559585492228, 0.04202750891492613, 0.04247155332098439, 0.055865921787709494, 0.029818181818181817, 0.001] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 0\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "individual acceptance rates\n",
      "[0.09844559585492228, 0.04202750891492613, 0.04247155332098439, 0.055865921787709494, 0.029818181818181817, 0.0]\n",
      "individul precision\n",
      "[0.7368421052631579, 0.6747474747474748, 0.6915887850467289, 0.7052631578947368, 0.4878048780487805, 0]\n",
      "individual recall\n",
      "[0.19718309859154928, 0.28425531914893615, 0.2967914438502674, 0.26961770623742454, 0.14285714285714285, 0.0]\n",
      "DP all\n",
      "0.09844559585492228\n",
      "precision all 0.6811594202898551\n",
      "recall all 0.27089337175792505\n",
      "accuracy all 0.9038601602330663\n",
      "TP,FP,TN,FN\n",
      "376 176 10793 1012\n",
      "dimension of data\n",
      "6 12357\n",
      "Optimal\n",
      "objective is:\n",
      "284394.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.09844559585492228, 0.04202750891492613, 0.04247155332098439, 0.055865921787709494, 0.029818181818181817, 0.001] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 1\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "individual acceptance rates\n",
      "[0.09844559585492228, 0.04202750891492613, 0.04247155332098439, 0.055865921787709494, 0.029818181818181817, 0.0]\n",
      "individul precision\n",
      "[0.7719298245614035, 0.6747474747474748, 0.6978193146417445, 0.7052631578947368, 0.4878048780487805, 0]\n",
      "individual recall\n",
      "[0.20657276995305165, 0.28425531914893615, 0.2994652406417112, 0.26961770623742454, 0.14285714285714285, 0.0]\n",
      "DP all\n",
      "0.09844559585492228\n",
      "precision all 0.6847826086956522\n",
      "recall all 0.2723342939481268\n",
      "accuracy all 0.9041838633972648\n",
      "TP,FP,TN,FN\n",
      "378 174 10795 1010\n",
      "dimension of data\n",
      "6 12357\n",
      "Optimal\n",
      "objective is:\n",
      "335248.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.09844559585492228, 0.04202750891492613, 0.04247155332098439, 0.055865921787709494, 0.029818181818181817, 0.001] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 2\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "individual acceptance rates\n",
      "[0.09844559585492228, 0.04202750891492613, 0.04247155332098439, 0.055865921787709494, 0.029818181818181817, 0.0]\n",
      "individul precision\n",
      "[0.7894736842105263, 0.6787878787878788, 0.6884735202492211, 0.7368421052631579, 0.4878048780487805, 0]\n",
      "individual recall\n",
      "[0.2112676056338028, 0.28595744680851065, 0.29545454545454547, 0.28169014084507044, 0.14285714285714285, 0.0]\n",
      "DP all\n",
      "0.09844559585492228\n",
      "precision all 0.6902173913043478\n",
      "recall all 0.2744956772334294\n",
      "accuracy all 0.9046694181435624\n",
      "TP,FP,TN,FN\n",
      "381 171 10798 1007\n",
      "dimension of data\n",
      "6 12357\n",
      "Optimal\n",
      "objective is:\n",
      "379818.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.09844559585492228, 0.04202750891492613, 0.04247155332098439, 0.055865921787709494, 0.029818181818181817, 0.001] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 3\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "individual acceptance rates\n",
      "[0.09844559585492228, 0.04202750891492613, 0.04247155332098439, 0.055865921787709494, 0.029818181818181817, 0.0]\n",
      "individul precision\n",
      "[0.7894736842105263, 0.6888888888888889, 0.6915887850467289, 0.7578947368421053, 0.4878048780487805, 0]\n",
      "individual recall\n",
      "[0.2112676056338028, 0.2902127659574468, 0.2967914438502674, 0.289738430583501, 0.14285714285714285, 0.0]\n",
      "DP all\n",
      "0.09844559585492228\n",
      "precision all 0.6992753623188406\n",
      "recall all 0.2780979827089337\n",
      "accuracy all 0.9054786760540584\n",
      "TP,FP,TN,FN\n",
      "386 166 10803 1002\n",
      "dimension of data\n",
      "6 12357\n",
      "Optimal\n",
      "objective is:\n",
      "419837.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.09844559585492228, 0.04202750891492613, 0.04247155332098439, 0.055865921787709494, 0.029818181818181817, 0.001] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 4\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "individual acceptance rates\n",
      "[0.1001727115716753, 0.04202750891492613, 0.04260386345594073, 0.055865921787709494, 0.029818181818181817, 0.0]\n",
      "individul precision\n",
      "[0.8103448275862069, 0.6828282828282828, 0.6925465838509317, 0.7473684210526316, 0.4878048780487805, 0]\n",
      "individual recall\n",
      "[0.22065727699530516, 0.2876595744680851, 0.29812834224598933, 0.2857142857142857, 0.14285714285714285, 0.0]\n",
      "DP all\n",
      "0.1001727115716753\n",
      "precision all 0.6962025316455697\n",
      "recall all 0.27737752161383283\n",
      "accuracy all 0.9052358986809096\n",
      "TP,FP,TN,FN\n",
      "385 168 10801 1003\n",
      "dimension of data\n",
      "6 12357\n",
      "Optimal\n",
      "objective is:\n",
      "455315.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.09844559585492228, 0.04202750891492613, 0.04247155332098439, 0.055865921787709494, 0.029818181818181817, 0.001] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 5\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "individual acceptance rates\n",
      "[0.10880829015544041, 0.04202750891492613, 0.043265414130722414, 0.055865921787709494, 0.029818181818181817, 0.0]\n",
      "individul precision\n",
      "[0.7619047619047619, 0.6848484848484848, 0.691131498470948, 0.7421052631578947, 0.4878048780487805, 0]\n",
      "individual recall\n",
      "[0.22535211267605634, 0.2885106382978723, 0.30213903743315507, 0.2837022132796781, 0.14285714285714285, 0.0]\n",
      "DP all\n",
      "0.10880829015544041\n",
      "precision all 0.6935483870967742\n",
      "recall all 0.2788184438040346\n",
      "accuracy all 0.90515497288986\n",
      "TP,FP,TN,FN\n",
      "387 171 10798 1001\n",
      "dimension of data\n",
      "6 12357\n",
      "Optimal\n",
      "objective is:\n",
      "436072.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.0690846286701209, 0.0554423501443369, 0.05609949722148717, 0.06233460746839165, 0.04145454545454545, 0.001] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 0\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "individual acceptance rates\n",
      "[0.0690846286701209, 0.0554423501443369, 0.05609949722148717, 0.06233460746839165, 0.04145454545454545, 0.0]\n",
      "individul precision\n",
      "[0.75, 0.6722817764165391, 0.6863207547169812, 0.6933962264150944, 0.543859649122807, 0]\n",
      "individual recall\n",
      "[0.14084507042253522, 0.37361702127659574, 0.3890374331550802, 0.29577464788732394, 0.22142857142857142, 0.0]\n",
      "DP all\n",
      "0.0690846286701209\n",
      "precision all 0.6767676767676768\n",
      "recall all 0.33789625360230546\n",
      "accuracy all 0.9075018208302986\n",
      "TP,FP,TN,FN\n",
      "469 224 10745 919\n",
      "dimension of data\n",
      "6 12357\n",
      "Optimal\n",
      "objective is:\n",
      "482214.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.0690846286701209, 0.0554423501443369, 0.05609949722148717, 0.06233460746839165, 0.04145454545454545, 0.001] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 1\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "individual acceptance rates\n",
      "[0.0690846286701209, 0.0554423501443369, 0.05609949722148717, 0.06233460746839165, 0.04145454545454545, 0.0]\n",
      "individul precision\n",
      "[0.75, 0.669218989280245, 0.6839622641509434, 0.6886792452830188, 0.543859649122807, 0]\n",
      "individual recall\n",
      "[0.14084507042253522, 0.3719148936170213, 0.3877005347593583, 0.2937625754527163, 0.22142857142857142, 0.0]\n",
      "DP all\n",
      "0.0690846286701209\n",
      "precision all 0.6738816738816739\n",
      "recall all 0.33645533141210376\n",
      "accuracy all 0.9071781176661002\n",
      "TP,FP,TN,FN\n",
      "467 226 10743 921\n",
      "dimension of data\n",
      "6 12357\n",
      "Optimal\n",
      "objective is:\n",
      "567260.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.0690846286701209, 0.0554423501443369, 0.05609949722148717, 0.06233460746839165, 0.04145454545454545, 0.001] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 2\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "individual acceptance rates\n",
      "[0.0690846286701209, 0.0554423501443369, 0.05609949722148717, 0.06233460746839165, 0.04145454545454545, 0.0]\n",
      "individul precision\n",
      "[0.75, 0.667687595712098, 0.6768867924528302, 0.6981132075471698, 0.543859649122807, 0]\n",
      "individual recall\n",
      "[0.14084507042253522, 0.37106382978723407, 0.3836898395721925, 0.2977867203219316, 0.22142857142857142, 0.0]\n",
      "DP all\n",
      "0.0690846286701209\n",
      "precision all 0.6724386724386724\n",
      "recall all 0.3357348703170029\n",
      "accuracy all 0.907016266084001\n",
      "TP,FP,TN,FN\n",
      "466 227 10742 922\n",
      "dimension of data\n",
      "6 12357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal\n",
      "objective is:\n",
      "646471.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.0690846286701209, 0.0554423501443369, 0.05609949722148717, 0.06233460746839165, 0.04145454545454545, 0.001] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 3\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "individual acceptance rates\n",
      "[0.0690846286701209, 0.0554423501443369, 0.05609949722148717, 0.06233460746839165, 0.04145454545454545, 0.0]\n",
      "individul precision\n",
      "[0.825, 0.666156202143951, 0.6792452830188679, 0.7028301886792453, 0.543859649122807, 0]\n",
      "individual recall\n",
      "[0.15492957746478872, 0.3702127659574468, 0.3850267379679144, 0.29979879275653926, 0.22142857142857142, 0.0]\n",
      "DP all\n",
      "0.0690846286701209\n",
      "precision all 0.6753246753246753\n",
      "recall all 0.3371757925072046\n",
      "accuracy all 0.9073399692481994\n",
      "TP,FP,TN,FN\n",
      "468 225 10744 920\n",
      "dimension of data\n",
      "6 12357\n",
      "Optimal\n",
      "objective is:\n",
      "718385.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.0690846286701209, 0.0554423501443369, 0.05609949722148717, 0.06233460746839165, 0.04145454545454545, 0.001] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 4\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "individual acceptance rates\n",
      "[0.07944732297063903, 0.0554423501443369, 0.056893358031225194, 0.06233460746839165, 0.04145454545454545, 0.0]\n",
      "individul precision\n",
      "[0.8260869565217391, 0.666156202143951, 0.6744186046511628, 0.7216981132075472, 0.5263157894736842, 0]\n",
      "individual recall\n",
      "[0.1784037558685446, 0.3702127659574468, 0.3877005347593583, 0.30784708249496984, 0.21428571428571427, 0.0]\n",
      "DP all\n",
      "0.07944732297063903\n",
      "precision all 0.6766809728183119\n",
      "recall all 0.3407780979827089\n",
      "accuracy all 0.9076636724123979\n",
      "TP,FP,TN,FN\n",
      "473 226 10743 915\n",
      "dimension of data\n",
      "6 12357\n",
      "Optimal\n",
      "objective is:\n",
      "785923.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.0690846286701209, 0.0554423501443369, 0.05609949722148717, 0.06233460746839165, 0.04145454545454545, 0.001] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 5\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "individual acceptance rates\n",
      "[0.07944732297063903, 0.0554423501443369, 0.056893358031225194, 0.06233460746839165, 0.04145454545454545, 0.0]\n",
      "individul precision\n",
      "[0.8260869565217391, 0.664624808575804, 0.6744186046511628, 0.7216981132075472, 0.5087719298245614, 0]\n",
      "individual recall\n",
      "[0.1784037558685446, 0.36936170212765956, 0.3877005347593583, 0.30784708249496984, 0.20714285714285716, 0.0]\n",
      "DP all\n",
      "0.07944732297063903\n",
      "precision all 0.6752503576537912\n",
      "recall all 0.3400576368876081\n",
      "accuracy all 0.9075018208302986\n",
      "TP,FP,TN,FN\n",
      "472 227 10742 916\n",
      "dimension of data\n",
      "6 12357\n",
      "Optimal\n",
      "objective is:\n",
      "59566.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.025906735751295335, 0.019697741552046188, 0.01905265943371262, 0.022640399882387533, 0.01890909090909091, 0.001] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 0\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "individual acceptance rates\n",
      "[0.025906735751295335, 0.019697741552046188, 0.01905265943371262, 0.022640399882387533, 0.01890909090909091, 0.0]\n",
      "individul precision\n",
      "[0.6666666666666666, 0.7370689655172413, 0.7291666666666666, 0.7662337662337663, 0.6538461538461539, 0]\n",
      "individual recall\n",
      "[0.046948356807511735, 0.14553191489361703, 0.14037433155080214, 0.11871227364185111, 0.12142857142857143, 0.0]\n",
      "DP all\n",
      "0.025906735751295335\n",
      "precision all 0.7327935222672065\n",
      "recall all 0.1304034582132565\n",
      "accuracy all 0.8969814679938496\n",
      "TP,FP,TN,FN\n",
      "181 66 10903 1207\n",
      "dimension of data\n",
      "6 12357\n",
      "Optimal\n",
      "objective is:\n",
      "65226.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.025906735751295335, 0.019697741552046188, 0.01905265943371262, 0.022640399882387533, 0.01890909090909091, 0.001] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 1\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "individual acceptance rates\n",
      "[0.025906735751295335, 0.019697741552046188, 0.01905265943371262, 0.022640399882387533, 0.01890909090909091, 0.0]\n",
      "individul precision\n",
      "[0.8, 0.7370689655172413, 0.7361111111111112, 0.7792207792207793, 0.6538461538461539, 0]\n",
      "individual recall\n",
      "[0.056338028169014086, 0.14553191489361703, 0.14171122994652408, 0.12072434607645875, 0.12142857142857143, 0.0]\n",
      "DP all\n",
      "0.025906735751295335\n",
      "precision all 0.7408906882591093\n",
      "recall all 0.13184438040345822\n",
      "accuracy all 0.8973051711580481\n",
      "TP,FP,TN,FN\n",
      "183 64 10905 1205\n",
      "dimension of data\n",
      "6 12357\n",
      "Optimal\n",
      "objective is:\n",
      "75755.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.025906735751295335, 0.019697741552046188, 0.01905265943371262, 0.022640399882387533, 0.01890909090909091, 0.001] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 2\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "individual acceptance rates\n",
      "[0.025906735751295335, 0.019697741552046188, 0.01905265943371262, 0.022640399882387533, 0.01890909090909091, 0.0]\n",
      "individul precision\n",
      "[0.7333333333333333, 0.7370689655172413, 0.7361111111111112, 0.7662337662337663, 0.6538461538461539, 0]\n",
      "individual recall\n",
      "[0.051643192488262914, 0.14553191489361703, 0.14171122994652408, 0.11871227364185111, 0.12142857142857143, 0.0]\n",
      "DP all\n",
      "0.025906735751295335\n",
      "precision all 0.7368421052631579\n",
      "recall all 0.13112391930835735\n",
      "accuracy all 0.8971433195759488\n",
      "TP,FP,TN,FN\n",
      "182 65 10904 1206\n",
      "dimension of data\n",
      "6 12357\n",
      "Optimal\n",
      "objective is:\n",
      "85760.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.025906735751295335, 0.019697741552046188, 0.01905265943371262, 0.022640399882387533, 0.01890909090909091, 0.001] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 3\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "individual acceptance rates\n",
      "[0.025906735751295335, 0.019697741552046188, 0.01905265943371262, 0.022640399882387533, 0.01890909090909091, 0.0]\n",
      "individul precision\n",
      "[0.7333333333333333, 0.7327586206896551, 0.7361111111111112, 0.7662337662337663, 0.6153846153846154, 0]\n",
      "individual recall\n",
      "[0.051643192488262914, 0.14468085106382977, 0.14171122994652408, 0.11871227364185111, 0.11428571428571428, 0.0]\n",
      "DP all\n",
      "0.025906735751295335\n",
      "precision all 0.7327935222672065\n",
      "recall all 0.1304034582132565\n",
      "accuracy all 0.8969814679938496\n",
      "TP,FP,TN,FN\n",
      "181 66 10903 1207\n",
      "dimension of data\n",
      "6 12357\n",
      "Optimal\n",
      "objective is:\n",
      "94756.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.025906735751295335, 0.019697741552046188, 0.01905265943371262, 0.022640399882387533, 0.01890909090909091, 0.001] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 4\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "individual acceptance rates\n",
      "[0.03281519861830743, 0.019697741552046188, 0.019581899973537972, 0.022640399882387533, 0.01890909090909091, 0.0]\n",
      "individul precision\n",
      "[0.7894736842105263, 0.7241379310344828, 0.7364864864864865, 0.7532467532467533, 0.6153846153846154, 0]\n",
      "individual recall\n",
      "[0.07042253521126761, 0.14297872340425533, 0.14572192513368984, 0.11670020120724346, 0.11428571428571428, 0.0]\n",
      "DP all\n",
      "0.03281519861830743\n",
      "precision all 0.7290836653386454\n",
      "recall all 0.13184438040345822\n",
      "accuracy all 0.8969814679938496\n",
      "TP,FP,TN,FN\n",
      "183 68 10901 1205\n",
      "dimension of data\n",
      "6 12357\n",
      "Optimal\n",
      "objective is:\n",
      "102568.0\n",
      "discripency is:\n",
      "None\n",
      "gamma-epsilon-delta [0.025906735751295335, 0.019697741552046188, 0.01905265943371262, 0.022640399882387533, 0.01890909090909091, 0.001] 0.01\n",
      "<--------------------------------------->\n",
      "iteration t 5\n",
      "sensitive attribute  1\n",
      "sensitive attribute  2\n",
      "sensitive attribute  3\n",
      "sensitive attribute  4\n",
      "sensitive attribute  5\n",
      "sensitive attribute  6\n",
      "individual acceptance rates\n",
      "[0.03626943005181347, 0.019697741552046188, 0.019846520243450648, 0.022640399882387533, 0.01890909090909091, 0.0]\n",
      "individul precision\n",
      "[0.8095238095238095, 0.7241379310344828, 0.74, 0.7662337662337663, 0.5769230769230769, 0]\n",
      "individual recall\n",
      "[0.07981220657276995, 0.14297872340425533, 0.1483957219251337, 0.11871227364185111, 0.10714285714285714, 0.0]\n",
      "DP all\n",
      "0.03626943005181347\n",
      "precision all 0.7312252964426877\n",
      "recall all 0.13328530259365995\n",
      "accuracy all 0.8971433195759488\n",
      "TP,FP,TN,FN\n",
      "185 68 10901 1203\n",
      "<--------------------------------------->\n"
     ]
    }
   ],
   "source": [
    "#overlap\n",
    "accu_all,DP_all,acceptance_rate,alpha_weight = main(sensitive, Y_test, Y_test_pred,e )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec=[[0.7798165137614679, 0.656, 0.6864608076009501, 0.6884615384615385, 0.5192307692307693, 0.0],\n",
    "[0.7368421052631579, 0.6747474747474748, 0.6915887850467289, 0.7052631578947368, 0.4878048780487805, 0],\n",
    "[0.75, 0.6722817764165391, 0.6863207547169812, 0.6933962264150944, 0.543859649122807, 0],\n",
    "[0.6666666666666666, 0.7370689655172413, 0.7291666666666666, 0.7662337662337663, 0.6538461538461539, 0]]\n",
    "\n",
    "\n",
    "rec=[[0.39906103286384975, 0.34893617021276596, 0.38636363636363635, 0.36016096579476864, 0.19285714285714287, 0.0],\n",
    "[0.19718309859154928, 0.28425531914893615, 0.2967914438502674, 0.26961770623742454, 0.14285714285714285, 0.0],\n",
    "[0.14084507042253522, 0.37361702127659574, 0.3890374331550802, 0.29577464788732394, 0.22142857142857142, 0.0],\n",
    "[0.046948356807511735, 0.14553191489361703, 0.14037433155080214, 0.11871227364185111, 0.12142857142857143, 0.0]]\n",
    " \n",
    "    \n",
    "accu=[0.9083,0.9038,0.9075,0.8969]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
